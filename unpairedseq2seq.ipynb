{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from torchtext import vocab\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                id  \\\n",
       "0           0             0  0000997932d777bf   \n",
       "1           1             1  000103f0d9cfb60f   \n",
       "2           2             2  0002bcb3da6cb337   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0           0.0   \n",
       "1  D'aww! He matches this background colour I'm s...      0           0.0   \n",
       "2       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1           1.0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0      0.0     0.0     0.0            0.0  \n",
       "1      0.0     0.0     0.0            0.0  \n",
       "2      1.0     0.0     1.0            0.0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/kaggle_toxicity/train_small.csv', error_bad_lines=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenize(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s).decode('utf8'))]\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "txt_field = data.Field(sequential=True, \n",
    "                       tokenize=tokenize, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "label_field = data.Field(sequential=False,\n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)\n",
    "train_fields = [\n",
    "    ('item', None),\n",
    "    ('id', None),\n",
    "    ('id1', None),\n",
    "    ('comment_text', txt_field), \n",
    "    ('toxic', label_field),\n",
    "]\n",
    "trains = data.TabularDataset.splits(path='./data/kaggle_toxicity/', \n",
    "                                            format='csv', \n",
    "                                            train='train_small.csv',\n",
    "                                            fields=train_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 100])\n"
     ]
    }
   ],
   "source": [
    "# vec = vocab.Vectors('model.txt', './data/embeddings/2017_wiki_dump/')\n",
    "txt_field.build_vocab(trains[0], vectors=\"glove.6B.100d\")\n",
    "label_field.build_vocab(trains[0])\n",
    "print(txt_field.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = data.BucketIterator.splits(datasets=(trains[0],trains[0]),\n",
    "                                            batch_sizes=(3,3),\n",
    "                                            sort_key=lambda x: len(x.comment_text),\n",
    "                                            device=-1,\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((Variable containing:\n",
      "    9     7    11\n",
      "    4     6    17\n",
      "   12    26    21\n",
      "   16    19    10\n",
      "   22     3    15\n",
      "    5    18    23\n",
      "    8     2     2\n",
      "   13    25    24\n",
      "   14     1     1\n",
      "   20     1     1\n",
      "[torch.LongTensor of size 10x3]\n",
      ", \n",
      " 10\n",
      "  8\n",
      "  8\n",
      "[torch.LongTensor of size 3]\n",
      "), Variable containing:\n",
      " 0\n",
      " 1\n",
      " 0\n",
      "[torch.LongTensor of size 3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    "            \n",
    "train_batch_it = BatchGenerator(traindl[0], 'comment_text', 'toxic')\n",
    "print(next(iter(train_batch_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'explanation', u'nwhy', u'the', u'edits', u'made', u'under', u'my', u'usern'], u'0')\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_batch_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class G12(nn.Module):\n",
    "    \"\"\"Generator for transfering from mnist to svhn\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G12, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(1, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 3, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 3, 32, 32)\n",
    "        return out\n",
    "    \n",
    "class G21(nn.Module):\n",
    "    \"\"\"Generator for transfering from svhn to mnist\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G21, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(3, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 1, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 1, 32, 32)\n",
    "        return out\n",
    "    \n",
    "class D1(nn.Module):\n",
    "    \"\"\"Discriminator for mnist.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D1, self).__init__()\n",
    "        self.conv1 = conv(1, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out\n",
    "\n",
    "class D2(nn.Module):\n",
    "    \"\"\"Discriminator for svhn.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D2, self).__init__()\n",
    "        self.conv1 = conv(3, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loader(config):\n",
    "    \"\"\"Builds and returns Dataloader for MNIST and SVHN dataset.\"\"\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale(config.image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    svhn = datasets.SVHN(root=config.svhn_path, download=True, transform=transform)\n",
    "    mnist = datasets.MNIST(root=config.mnist_path, download=True, transform=transform)\n",
    "\n",
    "    svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\n",
    "                                              batch_size=config.batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=config.num_workers)\n",
    "\n",
    "    mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                               batch_size=config.batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=config.num_workers)\n",
    "    return svhn_loader, mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f4b22bbc27bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mG12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mD1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named model"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from model import G12, G21\n",
    "from model import D1, D2\n",
    "\n",
    "\n",
    "class Solver(object):\n",
    "    def __init__(self, config, svhn_loader, mnist_loader):\n",
    "        self.svhn_loader = svhn_loader\n",
    "        self.mnist_loader = mnist_loader\n",
    "        self.g12 = None\n",
    "        self.g21 = None\n",
    "        self.d1 = None\n",
    "        self.d2 = None\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "        self.use_reconst_loss = config.use_reconst_loss\n",
    "        self.use_labels = config.use_labels\n",
    "        self.num_classes = config.num_classes\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.train_iters = config.train_iters\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lr = config.lr\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.sample_path = config.sample_path\n",
    "        self.model_path = config.model_path\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds a generator and a discriminator.\"\"\"\n",
    "        self.g12 = G12(conv_dim=self.g_conv_dim)\n",
    "        self.g21 = G21(conv_dim=self.g_conv_dim)\n",
    "        self.d1 = D1(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        self.d2 = D2(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        \n",
    "        g_params = list(self.g12.parameters()) + list(self.g21.parameters())\n",
    "        d_params = list(self.d1.parameters()) + list(self.d2.parameters())\n",
    "        \n",
    "        self.g_optimizer = optim.Adam(g_params, self.lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = optim.Adam(d_params, self.lr, [self.beta1, self.beta2])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.g12.cuda()\n",
    "            self.g21.cuda()\n",
    "            self.d1.cuda()\n",
    "            self.d2.cuda()\n",
    "    \n",
    "    def merge_images(self, sources, targets, k=10):\n",
    "        _, _, h, w = sources.shape\n",
    "        row = int(np.sqrt(self.batch_size))\n",
    "        merged = np.zeros([3, row*h, row*w*2])\n",
    "        for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "            i = idx // row\n",
    "            j = idx % row\n",
    "            merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "            merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "        return merged.transpose(1, 2, 0)\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        \"\"\"Converts numpy to variable.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "    \n",
    "    def to_data(self, x):\n",
    "        \"\"\"Converts variable to numpy.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cpu()\n",
    "        return x.data.numpy()\n",
    "    \n",
    "    def reset_grad(self):\n",
    "        \"\"\"Zeros the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        svhn_iter = iter(self.svhn_loader)\n",
    "        mnist_iter = iter(self.mnist_loader)\n",
    "        iter_per_epoch = min(len(svhn_iter), len(mnist_iter))\n",
    "        \n",
    "        # fixed mnist and svhn for sampling\n",
    "        fixed_svhn = self.to_var(svhn_iter.next()[0])\n",
    "        fixed_mnist = self.to_var(mnist_iter.next()[0])\n",
    "        \n",
    "        # loss if use_labels = True\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for step in range(self.train_iters+1):\n",
    "            # reset data_iter for each epoch\n",
    "            if (step+1) % iter_per_epoch == 0:\n",
    "                mnist_iter = iter(self.mnist_loader)\n",
    "                svhn_iter = iter(self.svhn_loader)\n",
    "            \n",
    "            # load svhn and mnist dataset\n",
    "            svhn, s_labels = svhn_iter.next() \n",
    "            svhn, s_labels = self.to_var(svhn), self.to_var(s_labels).long().squeeze()\n",
    "            mnist, m_labels = mnist_iter.next() \n",
    "            mnist, m_labels = self.to_var(mnist), self.to_var(m_labels)\n",
    "\n",
    "            if self.use_labels:\n",
    "                mnist_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*svhn.size(0)).long())\n",
    "                svhn_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*mnist.size(0)).long())\n",
    "            \n",
    "            #============ train D ============#\n",
    "            \n",
    "            # train with real images\n",
    "            self.reset_grad()\n",
    "            out = self.d1(mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, m_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            out = self.d2(svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, s_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            d_mnist_loss = d1_loss\n",
    "            d_svhn_loss = d2_loss\n",
    "            d_real_loss = d1_loss + d2_loss\n",
    "            d_real_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            # train with fake images\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, svhn_fake_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean(out**2)\n",
    "            \n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, mnist_fake_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean(out**2)\n",
    "            \n",
    "            d_fake_loss = d1_loss + d2_loss\n",
    "            d_fake_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            #============ train G ============#\n",
    "            \n",
    "            # train mnist-svhn-mnist cycle\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            reconst_mnist = self.g21(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, m_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((mnist - reconst_mnist)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # train svhn-mnist-svhn cycle\n",
    "            self.reset_grad()\n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            reconst_svhn = self.g12(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, s_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((svhn - reconst_svhn)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "            \n",
    "            # print the log info\n",
    "            if (step+1) % self.log_step == 0:\n",
    "                print('Step [%d/%d], d_real_loss: %.4f, d_mnist_loss: %.4f, d_svhn_loss: %.4f, '\n",
    "                      'd_fake_loss: %.4f, g_loss: %.4f' \n",
    "                      %(step+1, self.train_iters, d_real_loss.data[0], d_mnist_loss.data[0], \n",
    "                        d_svhn_loss.data[0], d_fake_loss.data[0], g_loss.data[0]))\n",
    "\n",
    "            # save the sampled images\n",
    "            if (step+1) % self.sample_step == 0:\n",
    "                fake_svhn = self.g12(fixed_mnist)\n",
    "                fake_mnist = self.g21(fixed_svhn)\n",
    "                \n",
    "                mnist, fake_mnist = self.to_data(fixed_mnist), self.to_data(fake_mnist)\n",
    "                svhn , fake_svhn = self.to_data(fixed_svhn), self.to_data(fake_svhn)\n",
    "                \n",
    "                merged = self.merge_images(mnist, fake_svhn)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-m-s.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "                \n",
    "                merged = self.merge_images(svhn, fake_mnist)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-s-m.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "            \n",
    "            if (step+1) % 5000 == 0:\n",
    "                # save the model parameters for each epoch\n",
    "                g12_path = os.path.join(self.model_path, 'g12-%d.pkl' %(step+1))\n",
    "                g21_path = os.path.join(self.model_path, 'g21-%d.pkl' %(step+1))\n",
    "                d1_path = os.path.join(self.model_path, 'd1-%d.pkl' %(step+1))\n",
    "                d2_path = os.path.join(self.model_path, 'd2-%d.pkl' %(step+1))\n",
    "                torch.save(self.g12.state_dict(), g12_path)\n",
    "                torch.save(self.g21.state_dict(), g21_path)\n",
    "                torch.save(self.d1.state_dict(), d1_path)\n",
    "                torch.save(self.d2.state_dict(), d2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--image_size IMAGE_SIZE]\n",
      "                             [--g_conv_dim G_CONV_DIM]\n",
      "                             [--d_conv_dim D_CONV_DIM] --use_reconst_loss\n",
      "                             USE_RECONST_LOSS --use_labels USE_LABELS\n",
      "                             [--num_classes NUM_CLASSES]\n",
      "                             [--train_iters TRAIN_ITERS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--lr LR]\n",
      "                             [--beta1 BETA1] [--beta2 BETA2] [--mode MODE]\n",
      "                             [--model_path MODEL_PATH]\n",
      "                             [--sample_path SAMPLE_PATH]\n",
      "                             [--mnist_path MNIST_PATH] [--svhn_path SVHN_PATH]\n",
      "                             [--log_step LOG_STEP] [--sample_step SAMPLE_STEP]\n",
      "ipykernel_launcher.py: error: argument --use_reconst_loss is required\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from torch.backends import cudnn\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "def main(config):\n",
    "    svhn_loader, mnist_loader = get_loader(config)\n",
    "    \n",
    "    solver = Solver(config, svhn_loader, mnist_loader)\n",
    "    cudnn.benchmark = True \n",
    "    \n",
    "    # create directories if not exist\n",
    "    if not os.path.exists(config.model_path):\n",
    "        os.makedirs(config.model_path)\n",
    "    if not os.path.exists(config.sample_path):\n",
    "        os.makedirs(config.sample_path)\n",
    "    \n",
    "    if config.mode == 'train':\n",
    "        solver.train()\n",
    "    elif config.mode == 'sample':\n",
    "        solver.sample()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # model hyper-parameters\n",
    "    parser.add_argument('--image_size', type=int, default=32)\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--use_reconst_loss', required=True, type=str2bool)\n",
    "    parser.add_argument('--use_labels', required=True, type=str2bool)\n",
    "    parser.add_argument('--num_classes', type=int, default=10)\n",
    "    \n",
    "    # training hyper-parameters\n",
    "    parser.add_argument('--train_iters', type=int, default=40000)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--num_workers', type=int, default=2)\n",
    "    parser.add_argument('--lr', type=float, default=0.0002)\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "    \n",
    "    # misc\n",
    "    parser.add_argument('--mode', type=str, default='train')\n",
    "    parser.add_argument('--model_path', type=str, default='./models')\n",
    "    parser.add_argument('--sample_path', type=str, default='./samples')\n",
    "    parser.add_argument('--mnist_path', type=str, default='./mnist')\n",
    "    parser.add_argument('--svhn_path', type=str, default='./svhn')\n",
    "    parser.add_argument('--log_step', type=int , default=10)\n",
    "    parser.add_argument('--sample_step', type=int , default=500)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
