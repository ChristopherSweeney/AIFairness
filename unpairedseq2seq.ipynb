{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from torchtext import vocab\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenize(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s).decode('utf8'))]\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "def convert_to_int(x):\n",
    "    return int(x)\n",
    "txt_field = data.Field(sequential=True, \n",
    "                       tokenize=tokenize, \n",
    "                       include_lengths=True, \n",
    "                       use_vocab=True)\n",
    "label_field = data.Field(sequential=False,\n",
    "                         tokenize = convert_to_int,\n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)\n",
    "train_fields = [\n",
    "    ('toxic', label_field), # process it as label\n",
    "    ('comment_text', txt_field) # process it as text\n",
    "]\n",
    "trains = data.TabularDataset.splits(path='./data/kaggle_toxicity/', \n",
    "                                            format='csv', \n",
    "                                            train='train.csv',\n",
    "                                            fields=train_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:46, 1.85MB/s]                               \n",
      "100%|██████████| 400000/400000 [00:40<00:00, 9999.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181941, 100])\n"
     ]
    }
   ],
   "source": [
    "# vec = vocab.Vectors('model.txt', './data/embeddings/2017_wiki_dump/')\n",
    "txt_field.build_vocab(trains[0], vectors=\"glove.6B.100d\")\n",
    "# label_field.build_vocab(trains[0])\n",
    "print(txt_field.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindl, valdl = data.BucketIterator.splits(datasets=(trains[0],trains[0]), # specify train and validation Tabulardataset\n",
    "                                            batch_sizes=(3,3),  # batch size of train and validation\n",
    "                                            sort_key=lambda x: len(x.comment_text), # on what attribute the text should be sorted\n",
    "                                            device=-1, # -1 mean cpu and 0 or None mean gpu\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53191\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'bc5cb8e625eea4d7'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-f255e99598b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (420963, 105241)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# BucketIterator return a batch object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# torchtext.data.batch.Batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext/data/iterator.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 yield Batch(minibatch, self.dataset, self.device,\n\u001b[0;32m--> 151\u001b[0;31m                             self.train)\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext/data/batch.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device, train)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext/data/field.pyc\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device, train)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \"\"\"\n\u001b[1;32m    187\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torchtext/data/field.pyc\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device, train)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, six.string_types)\n\u001b[0;32m--> 306\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'bc5cb8e625eea4d7'"
     ]
    }
   ],
   "source": [
    "print(len(traindl))\n",
    "# (420963, 105241)\n",
    "\n",
    "batch = next(iter(traindl)) # BucketIterator return a batch object\n",
    "print(type(batch))\n",
    "# torchtext.data.batch.Batch\n",
    "\n",
    "print(batch.toxic) # labels of the batch\n",
    "# tensor([ 0,  0,  0], device='cuda:0')\n",
    "\n",
    "print(batch.comment_text) # text index and length of the batch\n",
    "print(batch.dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    "            \n",
    "train_batch_it = BatchGenerator(trains[0], 'comment_text', 'toxic')\n",
    "print(next(iter(train_batch_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom deconvolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    \"\"\"Custom convolutional layer for simplicity.\"\"\"\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class G12(nn.Module):\n",
    "    \"\"\"Generator for transfering from mnist to svhn\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G12, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(1, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 3, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 3, 32, 32)\n",
    "        return out\n",
    "    \n",
    "class G21(nn.Module):\n",
    "    \"\"\"Generator for transfering from svhn to mnist\"\"\"\n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(G21, self).__init__()\n",
    "        # encoding blocks\n",
    "        self.conv1 = conv(3, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        \n",
    "        # residual blocks\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n",
    "        \n",
    "        # decoding blocks\n",
    "        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv2 = deconv(conv_dim, 1, 4, bn=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)    # (?, 128, 8, 8)\n",
    "        \n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)    # ( \" )\n",
    "        out = F.leaky_relu(self.conv4(out), 0.05)    # ( \" )\n",
    "        \n",
    "        out = F.leaky_relu(self.deconv1(out), 0.05)  # (?, 64, 16, 16)\n",
    "        out = F.tanh(self.deconv2(out))              # (?, 1, 32, 32)\n",
    "        return out\n",
    "    \n",
    "class D1(nn.Module):\n",
    "    \"\"\"Discriminator for mnist.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D1, self).__init__()\n",
    "        self.conv1 = conv(1, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out\n",
    "\n",
    "class D2(nn.Module):\n",
    "    \"\"\"Discriminator for svhn.\"\"\"\n",
    "    def __init__(self, conv_dim=64, use_labels=False):\n",
    "        super(D2, self).__init__()\n",
    "        self.conv1 = conv(3, conv_dim, 4, bn=False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        n_out = 11 if use_labels else 1\n",
    "        self.fc = conv(conv_dim*4, n_out, 4, 1, 0, False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n",
    "        out = self.fc(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loader(config):\n",
    "    \"\"\"Builds and returns Dataloader for MNIST and SVHN dataset.\"\"\"\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale(config.image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    svhn = datasets.SVHN(root=config.svhn_path, download=True, transform=transform)\n",
    "    mnist = datasets.MNIST(root=config.mnist_path, download=True, transform=transform)\n",
    "\n",
    "    svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\n",
    "                                              batch_size=config.batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=config.num_workers)\n",
    "\n",
    "    mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                               batch_size=config.batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=config.num_workers)\n",
    "    return svhn_loader, mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f4b22bbc27bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mG12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mD1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named model"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from model import G12, G21\n",
    "from model import D1, D2\n",
    "\n",
    "\n",
    "class Solver(object):\n",
    "    def __init__(self, config, svhn_loader, mnist_loader):\n",
    "        self.svhn_loader = svhn_loader\n",
    "        self.mnist_loader = mnist_loader\n",
    "        self.g12 = None\n",
    "        self.g21 = None\n",
    "        self.d1 = None\n",
    "        self.d2 = None\n",
    "        self.g_optimizer = None\n",
    "        self.d_optimizer = None\n",
    "        self.use_reconst_loss = config.use_reconst_loss\n",
    "        self.use_labels = config.use_labels\n",
    "        self.num_classes = config.num_classes\n",
    "        self.beta1 = config.beta1\n",
    "        self.beta2 = config.beta2\n",
    "        self.g_conv_dim = config.g_conv_dim\n",
    "        self.d_conv_dim = config.d_conv_dim\n",
    "        self.train_iters = config.train_iters\n",
    "        self.batch_size = config.batch_size\n",
    "        self.lr = config.lr\n",
    "        self.log_step = config.log_step\n",
    "        self.sample_step = config.sample_step\n",
    "        self.sample_path = config.sample_path\n",
    "        self.model_path = config.model_path\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds a generator and a discriminator.\"\"\"\n",
    "        self.g12 = G12(conv_dim=self.g_conv_dim)\n",
    "        self.g21 = G21(conv_dim=self.g_conv_dim)\n",
    "        self.d1 = D1(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        self.d2 = D2(conv_dim=self.d_conv_dim, use_labels=self.use_labels)\n",
    "        \n",
    "        g_params = list(self.g12.parameters()) + list(self.g21.parameters())\n",
    "        d_params = list(self.d1.parameters()) + list(self.d2.parameters())\n",
    "        \n",
    "        self.g_optimizer = optim.Adam(g_params, self.lr, [self.beta1, self.beta2])\n",
    "        self.d_optimizer = optim.Adam(d_params, self.lr, [self.beta1, self.beta2])\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.g12.cuda()\n",
    "            self.g21.cuda()\n",
    "            self.d1.cuda()\n",
    "            self.d2.cuda()\n",
    "    \n",
    "    def merge_images(self, sources, targets, k=10):\n",
    "        _, _, h, w = sources.shape\n",
    "        row = int(np.sqrt(self.batch_size))\n",
    "        merged = np.zeros([3, row*h, row*w*2])\n",
    "        for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "            i = idx // row\n",
    "            j = idx % row\n",
    "            merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "            merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "        return merged.transpose(1, 2, 0)\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        \"\"\"Converts numpy to variable.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "    \n",
    "    def to_data(self, x):\n",
    "        \"\"\"Converts variable to numpy.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cpu()\n",
    "        return x.data.numpy()\n",
    "    \n",
    "    def reset_grad(self):\n",
    "        \"\"\"Zeros the gradient buffers.\"\"\"\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        svhn_iter = iter(self.svhn_loader)\n",
    "        mnist_iter = iter(self.mnist_loader)\n",
    "        iter_per_epoch = min(len(svhn_iter), len(mnist_iter))\n",
    "        \n",
    "        # fixed mnist and svhn for sampling\n",
    "        fixed_svhn = self.to_var(svhn_iter.next()[0])\n",
    "        fixed_mnist = self.to_var(mnist_iter.next()[0])\n",
    "        \n",
    "        # loss if use_labels = True\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for step in range(self.train_iters+1):\n",
    "            # reset data_iter for each epoch\n",
    "            if (step+1) % iter_per_epoch == 0:\n",
    "                mnist_iter = iter(self.mnist_loader)\n",
    "                svhn_iter = iter(self.svhn_loader)\n",
    "            \n",
    "            # load svhn and mnist dataset\n",
    "            svhn, s_labels = svhn_iter.next() \n",
    "            svhn, s_labels = self.to_var(svhn), self.to_var(s_labels).long().squeeze()\n",
    "            mnist, m_labels = mnist_iter.next() \n",
    "            mnist, m_labels = self.to_var(mnist), self.to_var(m_labels)\n",
    "\n",
    "            if self.use_labels:\n",
    "                mnist_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*svhn.size(0)).long())\n",
    "                svhn_fake_labels = self.to_var(\n",
    "                    torch.Tensor([self.num_classes]*mnist.size(0)).long())\n",
    "            \n",
    "            #============ train D ============#\n",
    "            \n",
    "            # train with real images\n",
    "            self.reset_grad()\n",
    "            out = self.d1(mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, m_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            out = self.d2(svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, s_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean((out-1)**2)\n",
    "            \n",
    "            d_mnist_loss = d1_loss\n",
    "            d_svhn_loss = d2_loss\n",
    "            d_real_loss = d1_loss + d2_loss\n",
    "            d_real_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            # train with fake images\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                d2_loss = criterion(out, svhn_fake_labels)\n",
    "            else:\n",
    "                d2_loss = torch.mean(out**2)\n",
    "            \n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                d1_loss = criterion(out, mnist_fake_labels)\n",
    "            else:\n",
    "                d1_loss = torch.mean(out**2)\n",
    "            \n",
    "            d_fake_loss = d1_loss + d2_loss\n",
    "            d_fake_loss.backward()\n",
    "            self.d_optimizer.step()\n",
    "            \n",
    "            #============ train G ============#\n",
    "            \n",
    "            # train mnist-svhn-mnist cycle\n",
    "            self.reset_grad()\n",
    "            fake_svhn = self.g12(mnist)\n",
    "            out = self.d2(fake_svhn)\n",
    "            reconst_mnist = self.g21(fake_svhn)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, m_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((mnist - reconst_mnist)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "            # train svhn-mnist-svhn cycle\n",
    "            self.reset_grad()\n",
    "            fake_mnist = self.g21(svhn)\n",
    "            out = self.d1(fake_mnist)\n",
    "            reconst_svhn = self.g12(fake_mnist)\n",
    "            if self.use_labels:\n",
    "                g_loss = criterion(out, s_labels) \n",
    "            else:\n",
    "                g_loss = torch.mean((out-1)**2) \n",
    "\n",
    "            if self.use_reconst_loss:\n",
    "                g_loss += torch.mean((svhn - reconst_svhn)**2)\n",
    "\n",
    "            g_loss.backward()\n",
    "            self.g_optimizer.step()\n",
    "            \n",
    "            # print the log info\n",
    "            if (step+1) % self.log_step == 0:\n",
    "                print('Step [%d/%d], d_real_loss: %.4f, d_mnist_loss: %.4f, d_svhn_loss: %.4f, '\n",
    "                      'd_fake_loss: %.4f, g_loss: %.4f' \n",
    "                      %(step+1, self.train_iters, d_real_loss.data[0], d_mnist_loss.data[0], \n",
    "                        d_svhn_loss.data[0], d_fake_loss.data[0], g_loss.data[0]))\n",
    "\n",
    "            # save the sampled images\n",
    "            if (step+1) % self.sample_step == 0:\n",
    "                fake_svhn = self.g12(fixed_mnist)\n",
    "                fake_mnist = self.g21(fixed_svhn)\n",
    "                \n",
    "                mnist, fake_mnist = self.to_data(fixed_mnist), self.to_data(fake_mnist)\n",
    "                svhn , fake_svhn = self.to_data(fixed_svhn), self.to_data(fake_svhn)\n",
    "                \n",
    "                merged = self.merge_images(mnist, fake_svhn)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-m-s.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "                \n",
    "                merged = self.merge_images(svhn, fake_mnist)\n",
    "                path = os.path.join(self.sample_path, 'sample-%d-s-m.png' %(step+1))\n",
    "                scipy.misc.imsave(path, merged)\n",
    "                print ('saved %s' %path)\n",
    "            \n",
    "            if (step+1) % 5000 == 0:\n",
    "                # save the model parameters for each epoch\n",
    "                g12_path = os.path.join(self.model_path, 'g12-%d.pkl' %(step+1))\n",
    "                g21_path = os.path.join(self.model_path, 'g21-%d.pkl' %(step+1))\n",
    "                d1_path = os.path.join(self.model_path, 'd1-%d.pkl' %(step+1))\n",
    "                d2_path = os.path.join(self.model_path, 'd2-%d.pkl' %(step+1))\n",
    "                torch.save(self.g12.state_dict(), g12_path)\n",
    "                torch.save(self.g21.state_dict(), g21_path)\n",
    "                torch.save(self.d1.state_dict(), d1_path)\n",
    "                torch.save(self.d2.state_dict(), d2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--image_size IMAGE_SIZE]\n",
      "                             [--g_conv_dim G_CONV_DIM]\n",
      "                             [--d_conv_dim D_CONV_DIM] --use_reconst_loss\n",
      "                             USE_RECONST_LOSS --use_labels USE_LABELS\n",
      "                             [--num_classes NUM_CLASSES]\n",
      "                             [--train_iters TRAIN_ITERS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--lr LR]\n",
      "                             [--beta1 BETA1] [--beta2 BETA2] [--mode MODE]\n",
      "                             [--model_path MODEL_PATH]\n",
      "                             [--sample_path SAMPLE_PATH]\n",
      "                             [--mnist_path MNIST_PATH] [--svhn_path SVHN_PATH]\n",
      "                             [--log_step LOG_STEP] [--sample_step SAMPLE_STEP]\n",
      "ipykernel_launcher.py: error: argument --use_reconst_loss is required\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from torch.backends import cudnn\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "def main(config):\n",
    "    svhn_loader, mnist_loader = get_loader(config)\n",
    "    \n",
    "    solver = Solver(config, svhn_loader, mnist_loader)\n",
    "    cudnn.benchmark = True \n",
    "    \n",
    "    # create directories if not exist\n",
    "    if not os.path.exists(config.model_path):\n",
    "        os.makedirs(config.model_path)\n",
    "    if not os.path.exists(config.sample_path):\n",
    "        os.makedirs(config.sample_path)\n",
    "    \n",
    "    if config.mode == 'train':\n",
    "        solver.train()\n",
    "    elif config.mode == 'sample':\n",
    "        solver.sample()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # model hyper-parameters\n",
    "    parser.add_argument('--image_size', type=int, default=32)\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=64)\n",
    "    parser.add_argument('--use_reconst_loss', required=True, type=str2bool)\n",
    "    parser.add_argument('--use_labels', required=True, type=str2bool)\n",
    "    parser.add_argument('--num_classes', type=int, default=10)\n",
    "    \n",
    "    # training hyper-parameters\n",
    "    parser.add_argument('--train_iters', type=int, default=40000)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--num_workers', type=int, default=2)\n",
    "    parser.add_argument('--lr', type=float, default=0.0002)\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "    \n",
    "    # misc\n",
    "    parser.add_argument('--mode', type=str, default='train')\n",
    "    parser.add_argument('--model_path', type=str, default='./models')\n",
    "    parser.add_argument('--sample_path', type=str, default='./samples')\n",
    "    parser.add_argument('--mnist_path', type=str, default='./mnist')\n",
    "    parser.add_argument('--svhn_path', type=str, default='./svhn')\n",
    "    parser.add_argument('--log_step', type=int , default=10)\n",
    "    parser.add_argument('--sample_step', type=int , default=500)\n",
    "\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
