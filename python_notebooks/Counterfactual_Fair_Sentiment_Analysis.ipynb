{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train a Toxicity model using Keras.\"\"\"\n",
    "import keras\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cPickle\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "import FairAI\n",
    "# autoreload makes it easier to interactively work on code in the model_bias_analysis module.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions for parsing the data\n",
    "### parsing files from https://github.com/cbaziotis/ntua-slp-semeval2018/blob/master/dataloaders/task1.py\n",
    "def parse_e_c(data_file):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of lists corresponding to the emotion labels of the tweets\n",
    "\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    # dict.values() does not guarantee the order of the elements\n",
    "    # so we should avoid using a dict for the labels\n",
    "    y = [[int(l) for l in d[2:]] for d in data]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_oc(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the ordinal classification targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], int(d[3].split(':')[0])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_reg(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "    The test datasets for the EI-reg and V-reg English tasks have two parts:\n",
    "    1. The Tweet Test Set: tweets annotated for emotion/valence intensity;\n",
    "    2. The Mystery Test Set: automatically generated sentences to test for\n",
    "    unethical biases in NLP systems (with no emotion/valence annotations).\n",
    "\n",
    "    Mystery Test Set: the last 16,937 lines with 'mystery' in the ID\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the regression targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "        data = [d for d in data if \"mystery\" not in d[0]]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], float(d[3])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using the EEC corpus to tease out biases with respect to race and gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and util  helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(embeddings,s0):\n",
    "    X1=[]\n",
    "    regex = re.compile('[^a-zA-Z] ')\n",
    "    s1 = map(lambda x: regex.sub('', x.lower()[:-1]).split(\" \"),np.array(s0))\n",
    "    s2 = map(lambda x: filter(lambda x: x in embeddings,x),s1)\n",
    "    X =  map(lambda x: FairAI._np_normalize(embeddings[x]),s2)\n",
    "    for index,i in enumerate(X):\n",
    "            try:\n",
    "                len(np.mean(X[index],axis=0))\n",
    "                X1.append(np.mean(X[index],axis=0))\n",
    "            except:\n",
    "                print(X[index])\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coeff(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = np.mean(x)\n",
    "    my = np.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = np.sum(np.multiply(xm,ym))\n",
    "    r_den =np.sqrt(np.multiply(np.sum(np.square(xm)), np.sum(np.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "#     r = max(min(r, 1.0), -1.0)\n",
    "#     return 1 - np.square(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters & Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code from https://github.com/conversationai/unintended-ml-bias-analysis for creating CNN model\n",
    "DEFAULT_EMBEDDINGS_PATH = '../data/embeddings/glove.6B/glove.6B.100d.txt'\n",
    "#DEFAULT_EMBEDDINGS_PATH = './glove_debias_new_sentiment_with_toxic_vec.txt'\n",
    "\n",
    "DEFAULT_MODEL_DIR = '../models'\n",
    "\n",
    "DEFAULT_HPARAMS = {\n",
    "    'max_sequence_length': 25,\n",
    "    'max_num_words': 2000000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00001,\n",
    "    'stop_early': False,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.1,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5, 5, 5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "  try:\n",
    "    return metrics.roc_auc_score(y_true, y_pred)\n",
    "  except ValueError:\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "### Model scoring\n",
    "\n",
    "# Scoring these dataset for dozens of models actually takes non-trivial amounts\n",
    "# of time, so we save the results as a CSV. The resulting CSV includes all the\n",
    "# columns of the original dataset, and in addition has columns for each model,\n",
    "# containing the model's scores.\n",
    "def score_dataset(df, models, text_col):\n",
    "    \"\"\"Scores the dataset with each model and adds the scores as new columns.\"\"\"\n",
    "    for model in models:\n",
    "        name = model.get_model_name()\n",
    "        print('{} Scoring with {}...'.format(datetime.datetime.now(), name))\n",
    "        df[name] = model.predict(df[text_col])\n",
    "\n",
    "def load_maybe_score(models, orig_path, scored_path, postprocess_fn):\n",
    "    if os.path.exists(scored_path):\n",
    "        print('Using previously scored data:', scored_path)\n",
    "        return pd.read_csv(scored_path)\n",
    "\n",
    "    dataset = pd.read_csv(orig_path)\n",
    "    postprocess_fn(dataset)\n",
    "    score_dataset(dataset, models, 'text')\n",
    "    print('Saving scores to:', scored_path)\n",
    "    dataset.to_csv(scored_path)\n",
    "    return dataset\n",
    "\n",
    "def postprocess_madlibs(madlibs):\n",
    "    \"\"\"Modifies madlibs data to have standard 'text' and 'label' columns.\"\"\"\n",
    "    # Native madlibs data uses 'Label' column with values 'BAD' and 'NOT_BAD'.\n",
    "    # Replace with a bool.\n",
    "    madlibs['label'] = madlibs['Label'] == 'BAD'\n",
    "    madlibs.drop('Label', axis=1, inplace=True)\n",
    "    madlibs.rename(columns={'Text': 'text'}, inplace=True)\n",
    "\n",
    "def postprocess_wiki_dataset(wiki_data):\n",
    "    \"\"\"Modifies Wikipedia dataset to have 'text' and 'label' columns.\"\"\"\n",
    "    wiki_data.rename(columns={'is_toxic': 'label',\n",
    "                              'comment': 'text'},\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxModel(object):\n",
    "  \"\"\"Toxicity model.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               model_name=None,\n",
    "               model_dir=DEFAULT_MODEL_DIR,\n",
    "               embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "               hparams=None):\n",
    "    self.model_dir = model_dir\n",
    "    self.embeddings_path = embeddings_path\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.hparams = DEFAULT_HPARAMS.copy()\n",
    "    if hparams:\n",
    "      self.update_hparams(hparams)\n",
    "    if model_name:\n",
    "      self.load_model_from_name(model_name)\n",
    "    self.print_hparams()\n",
    "\n",
    "  def print_hparams(self):\n",
    "    print('Hyperparameters')\n",
    "    print('---------------')\n",
    "    for k, v in self.hparams.iteritems():\n",
    "      print('{}: {}'.format(k, v))\n",
    "    print('')\n",
    "\n",
    "  def update_hparams(self, new_hparams):\n",
    "    self.hparams.update(new_hparams)\n",
    "\n",
    "  def get_model_name(self):\n",
    "    return self.model_name\n",
    "\n",
    "  def save_hparams(self, model_name):\n",
    "    self.hparams['model_name'] = model_name\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'w') as f:\n",
    "      json.dump(self.hparams, f, sort_keys=True)\n",
    "\n",
    "  def load_model_from_name(self, model_name):\n",
    "    self.model = load_model(\n",
    "        os.path.join(self.model_dir, '%s_model.h5' % model_name))\n",
    "    self.tokenizer = cPickle.load(\n",
    "        open(\n",
    "            os.path.join(self.model_dir, '%s_tokenizer.pkl' % model_name),\n",
    "            'rb'))\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'r') as f:\n",
    "      self.hparams = json.load(f)\n",
    "\n",
    "  def fit_and_save_tokenizer(self, texts):\n",
    "    \"\"\"Fits tokenizer on texts and pickles the tokenizer state.\"\"\"\n",
    "    self.tokenizer = Tokenizer(num_words=self.hparams['max_num_words'])\n",
    "    gender = [\"she\",\"he\",\"her\",\"him\", \"woman\",\" man\",\" girl\",\" boy\",\" sister\",\" brother\",\" daughter\",\" son\",\" wife\",\" husband\",\" girlfriend\",\"boyfriend\",\" mother\",\" father\", \"aunt\",\" uncle\",\"mommy\",\"dad\"]\n",
    "    names = [\"Ebony\",\"Alonzo\",\"Amanda\",\"Adam\",\"Jasmine\",\"Alphonse\",\"Betsy\",\"Alan\",\"Lakisha\",\"Darnell\",\"Courtney\",\"Andrew\",\"Latisha\",\"Jamel\",\"Ellen\",\"Frank\",\"Latoya\",\"Jerome\",\"Heather\",\"Harry\",\"Nichelle\",\"Lamar\",\"Katie\",\"Jack\",\"Shaniqua\",\"Leroy\",\"Kristin\",\"Josh\",\"Shereen\",\"Malik\",\"Melanie\",\"Justin\",\"Tanisha\",\"Terrence\",\"Nancy\",\"Roger\",\"Tia\",\"Torrance\",\"Stephanie\",\"Ryan\"]\n",
    "    self.tokenizer.fit_on_texts(texts+gender+names)\n",
    "    cPickle.dump(self.tokenizer,\n",
    "                 open(\n",
    "                     os.path.join(self.model_dir,\n",
    "                                  '%s_tokenizer.pkl' % self.model_name), 'wb'))\n",
    "\n",
    "  def prep_text(self, texts):\n",
    "    \"\"\"Turns text into into padded sequences.\n",
    "\n",
    "    The tokenizer must be initialized before calling this method.\n",
    "\n",
    "    Args:\n",
    "      texts: Sequence of text strings.\n",
    "\n",
    "    Returns:\n",
    "      A tokenized and padded text sequence as a model input.\n",
    "    \"\"\"\n",
    "    print('\\n prepping: ', texts[1:3])\n",
    "    text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(\n",
    "        text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
    "\n",
    "  def load_embeddings(self):\n",
    "    \"\"\"Loads word embeddings.\"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(self.embeddings_path) as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    self.embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1,\n",
    "                                      self.hparams['embedding_dim']))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in self.tokenizer.word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  def train(self, train_data,train_labels,valid_data,valid_labels, model_name):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    self.model_name = model_name\n",
    "    self.save_hparams(model_name)\n",
    "\n",
    "    print('Fitting tokenizer...')\n",
    "    self.fit_and_save_tokenizer(train_data)\n",
    "    print('Tokenizer fitted!')\n",
    "\n",
    "    print('Preparing data...')\n",
    "    train_text, train_labels = (self.prep_text(train_data),train_labels)\n",
    "    valid_text, valid_labels = (self.prep_text(valid_data),valid_labels)\n",
    "    print('Data prepared!')\n",
    "\n",
    "    print('Loading embeddings...')\n",
    "    self.load_embeddings()\n",
    "    print('Embeddings loaded!')\n",
    "\n",
    "    print('Building model graph...')\n",
    "    self.build_model_1()\n",
    "    print('Training model...')\n",
    "\n",
    "    save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
    "    ]\n",
    "    if self.hparams['stop_early']:\n",
    "      callbacks.append(\n",
    "          EarlyStopping(\n",
    "              min_delta=self.hparams['es_min_delta'],\n",
    "              monitor='val_loss',\n",
    "              patience=self.hparams['es_patience'],\n",
    "              verbose=self.hparams['verbose'],\n",
    "              mode='auto'))\n",
    "    print(train_text[0],train_labels[0])\n",
    "    print(np.shape(train_text),np.shape(train_labels))\n",
    "    self.model.fit(\n",
    "        train_text,\n",
    "        train_labels,\n",
    "        batch_size=self.hparams['batch_size'],\n",
    "        epochs=self.hparams['epochs'],\n",
    "        validation_data=(valid_text, valid_labels),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0)\n",
    "    print('Model trained!')\n",
    "    print('Best model saved to {}'.format(save_path))\n",
    "    print('Loading best model from checkpoint...')\n",
    "    self.model = load_model(save_path)\n",
    "    print('Model loaded!')\n",
    "    \n",
    "  def build_model(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    for filter_size, kernel_size, pool_size in zip(\n",
    "        self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'],\n",
    "        self.hparams['cnn_pooling_sizes']):\n",
    "      x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(self.hparams['dropout_rate'])(x)\n",
    "    # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    \n",
    "    preds = Dense(1)(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop)\n",
    "\n",
    "  def build_conv_layer(self, input_tensor, filter_size, kernel_size, pool_size):\n",
    "    output = Conv1D(\n",
    "        filter_size, kernel_size, activation='relu', padding='same')(\n",
    "            input_tensor)\n",
    "    if pool_size:\n",
    "      output = MaxPooling1D(pool_size, padding='same')(output)\n",
    "    else:\n",
    "      # TODO(nthain): This seems broken. Fix.\n",
    "      output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "  \n",
    "  def build_model_1(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    x= keras.layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(1,kernel_initializer='normal')(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop, metrics=['accuracy'])\n",
    "  def predict(self, texts):\n",
    "    \"\"\"Returns model predictions on texts.\"\"\"\n",
    "    data = self.prep_text(texts)\n",
    "    return self.model.predict(data)[:, 1]\n",
    "\n",
    "  def score_auc(self, texts, labels):\n",
    "    preds = self.predict(texts)\n",
    "    return compute_auc(labels, preds)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need a way to index the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_templates(templates):\n",
    "    template_index = {}\n",
    "    template_reverse_index = {}\n",
    "    for index, template in enumerate(templates):\n",
    "        template_reverse_index[template] = index\n",
    "        template_index[index] = template\n",
    "    return template_index, template_reverse_index\n",
    "\n",
    "def entries_by_template(template_id):\n",
    "    template = self.template_index[template_id]\n",
    "    output = df[df['Template'] == template]\n",
    "    return output\n",
    "\n",
    "def get_counterfactuals(entry_id, limit=None, not_similar_at=None, different_at=None):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        not_similar_at: similarities to filter by\n",
    "        different_at: difference to filter by\n",
    "    \"\"\"\n",
    "    template_id = self.template_reverse_index(self.df[entry_id][template])\n",
    "    candidates = data_by_template(template_id)\n",
    "    if different_at is not None:\n",
    "        for attr in different_at:\n",
    "            candidates = candidates[candidates[attr] != self.df[entry_id][attr]]\n",
    "    if not_similar_at is not None:\n",
    "        for attr in not_similar_at:\n",
    "            candidates = candidates[candidates[attr] == self.df[entry_id][attr]]\n",
    "    return candidates.sample(n=limit) if limit is not None else candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Sentence  Template  Person  Gender  Race  Emotion word\n",
       "Emotion                                                              \n",
       "anger    1400      1400      1400    1400    1400  1400          1400\n",
       "fear     1400      1400      1400    1400    1400  1400          1400\n",
       "joy      1400      1400      1400    1400    1400  1400          1400\n",
       "sadness  1400      1400      1400    1400    1400  1400          1400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna()\n",
    "a = df.groupby(\"Emotion\")\n",
    "a.aggregate('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '<person subject> feels <emotion word>.',\n",
       "  1: 'The situation makes <person object> feel <emotion word>.',\n",
       "  2: 'I made <person object> feel <emotion word>.',\n",
       "  3: '<person subject> made me feel <emotion word>.',\n",
       "  4: '<person subject> found himself/herself in a/an <emotional situation word> situation.',\n",
       "  5: '<person subject> told us all about the recent <emotional situation word> events.',\n",
       "  6: 'The conversation with <person object> was <emotional situation word>.'},\n",
       " {'<person subject> feels <emotion word>.': 0,\n",
       "  '<person subject> found himself/herself in a/an <emotional situation word> situation.': 4,\n",
       "  '<person subject> made me feel <emotion word>.': 3,\n",
       "  '<person subject> told us all about the recent <emotional situation word> events.': 5,\n",
       "  'I made <person object> feel <emotion word>.': 2,\n",
       "  'The conversation with <person object> was <emotional situation word>.': 6,\n",
       "  'The situation makes <person object> feel <emotion word>.': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_templates = df.Template.unique()\n",
    "templates_index, templates_reverse_index = index_templates(unique_templates)\n",
    "templates_index, templates_reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_counterfactuals(limit=5, emotion_word='angry', max_iters=10):\n",
    "    found_candidates = False\n",
    "    wanted = None\n",
    "    iterations = 0\n",
    "    while not found_candidates and iterations < max_iters:\n",
    "        iterations += 1\n",
    "        \n",
    "        key = random.sample(templates_index.keys(), 1)[0]\n",
    "        template = templates_index[key]\n",
    "        candidates = df[df['Template'] == template]\n",
    "        wanted = candidates[candidates['Emotion word'] == emotion_word]\n",
    "        found_candidates = wanted.shape[0] > limit\n",
    "    assert wanted is not None and wanted.shape[0] > 0, ' could not find wanted '\n",
    "    return wanted.sample(n=limit)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made Alphonse feel happy.',\n",
       " 'I made Latoya feel happy.',\n",
       " 'I made Nancy feel happy.',\n",
       " 'I made Ryan feel happy.',\n",
       " 'I made Shereen feel happy.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_random_counterfactuals(limit=5, emotion_word=\"happy\").Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counterfactual_sentences(emotion_word, limit):\n",
    "    sentences = get_random_counterfactuals(limit=limit, emotion_word=emotion_word).Sentence\n",
    "    return list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lamar made me feel happy.',\n",
       " 'Ryan made me feel happy.',\n",
       " 'Heather made me feel happy.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_counterfactual_sentences('happy', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTF_ToxModel(ToxModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.ctf_limit = kwargs['ctf_limit'] if 'ctf_limit' in kwargs else 5\n",
    "        self.ctf_reg = kwargs['ctf_reg'] if 'ctf_reg' in kwargs else 100\n",
    "        if 'ctf_limit' in kwargs:\n",
    "            del(kwargs['ctf_limit'])\n",
    "        if 'ctf_reg' in kwargs:\n",
    "            del(kwargs['ctf_reg'])\n",
    "        super(CTF_ToxModel, self).__init__(*args, **kwargs)\n",
    "    def custom_loss(self, ytrue, ypred):\n",
    "        mse_error = tf.losses.mean_squared_error(ytrue, ypred)\n",
    "        ctf_error = self.get_counterfactual_mse(ypred)\n",
    "        return mse_error + ctf_error\n",
    "    \n",
    "    \n",
    "    def get_counterfactual_mse(self, ypred):\n",
    "        \"\"\"\n",
    "        For now, the error is the standard deviation of prediction probabilities\n",
    "        \"\"\"\n",
    "        sentences = self.get_random_counterfactuals()\n",
    "        ctf_preds = self._predict(sentences)\n",
    "        \n",
    "        # different loss fn?\n",
    "        # kl? cross-entropy? \n",
    "        # Regularization \n",
    "        # Look in different demographic\n",
    "        # split templates data for train vs evaluation to avoid overfitting\n",
    "        #print(\"shape of ypred: \", tf.shape(ypred), \"type of ypred: \", type(ypred))\n",
    "        \n",
    "        multiply = tf.constant([self.ctf_limit])\n",
    "        ypred_ = tf.constant(np.arange(0,10))\n",
    "        ##print(\"tiled: \", tiled)\n",
    "        \n",
    "        pred_matrix = tf.reshape(tiled, [multiply[0], tf.shape(ypred)[0]])\n",
    "        err = tf.losses.mean_squared_error(pred_matrix, ctf_preds)\n",
    "        return err\n",
    "    \n",
    "    def _predict(self, texts):\n",
    "        data = self.prep_text(texts)\n",
    "        output = self.model.predict(data)\n",
    "        #print('\\n done PREDICTING, out = ', output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_random_counterfactuals(self):\n",
    "        \"\"\"\n",
    "        Generate a vector of counterfactuals. Same template and emotion \n",
    "        word implies they should have the same rating\n",
    "        \"\"\"\n",
    "        emotions = ['angry', 'happy']\n",
    "        import random\n",
    "        emotion_word = random.sample(emotions, 1)[0]\n",
    "        sentences = get_counterfactual_sentences(emotion_word=emotion_word, limit=self.ctf_limit)\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def embeddings_for_sentences(self, sentences):\n",
    "        sentence_embeddings = [self.prep_text(sentence) for sentence in sentences]\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def ctf_build_model(self):\n",
    "        \"\"\"Builds model graph.\"\"\"\n",
    "        sequence_input = Input(\n",
    "            shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            len(self.tokenizer.word_index) + 1,\n",
    "            self.hparams['embedding_dim'],\n",
    "            weights=[self.embedding_matrix],\n",
    "            input_length=self.hparams['max_sequence_length'],\n",
    "            trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        x= keras.layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        preds = Dense(1,kernel_initializer='normal')(x)\n",
    "\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        # Now, compile with the loss becoming the custom loss\n",
    "        self.model.compile(\n",
    "            loss=self.custom_loss, optimizer=rmsprop, metrics=['accuracy'])    \n",
    "        \n",
    "    def ctf_train(self, train_data,train_labels,valid_data,valid_labels, model_name):\n",
    "        \"\"\"Trains the model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data)\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data),train_labels)\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data),valid_labels)\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.ctf_build_model()\n",
    "        print('Training model...')\n",
    "\n",
    "        save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
    "        ]\n",
    "        if self.hparams['stop_early']:\n",
    "          callbacks.append(\n",
    "              EarlyStopping(\n",
    "                  min_delta=self.hparams['es_min_delta'],\n",
    "                  monitor='val_loss',\n",
    "                  patience=self.hparams['es_patience'],\n",
    "                  verbose=self.hparams['verbose'],\n",
    "                  mode='auto'))\n",
    "        print(train_text[0],train_labels[0])\n",
    "        print(np.shape(train_text),np.shape(train_labels))\n",
    "        self.model.fit(\n",
    "            train_text,\n",
    "            train_labels,\n",
    "            batch_size=self.hparams['batch_size'],\n",
    "            epochs=self.hparams['epochs'],\n",
    "            validation_data=(valid_text, valid_labels),\n",
    "            callbacks=callbacks,\n",
    "            verbose=0)\n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(save_path))\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(save_path)\n",
    "        print('Model loaded!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11718230016",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7964beaaa5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11718230016"
     ]
    }
   ],
   "source": [
    "# Activate enable memory growth\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 500\n",
    "column_names=[\"LSTM GloVe Debiased\",\n",
    "              \"5-CTF LSTM GloVe Debiased-reg-100\",\"10-CTF LSTM GloVe Debiased-reg-100\",\n",
    "              \"5-CTF LSTM GloVe Debiased-reg-1\",\"10-CTF LSTM GloVe Debiased-reg-1\"]\n",
    "df_aa=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                         np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_w=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_f=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_m=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LSTM GloVe Debiased</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LSTM GloVe Debiased 5-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "\n",
       "                    10-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "\n",
       "                    5-CTF LSTM GloVe Debiased-reg-1  \\\n",
       "0.0 0.0 0.0 0.0 0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "\n",
       "                    10-CTF LSTM GloVe Debiased-reg-1  \n",
       "0.0 0.0 0.0 0.0 0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM or CNN Models with Our Debiased Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11718230016",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f7c2e03676a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToxModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pearson score \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpearson_coeff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-02e69a356dc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, train_labels, valid_data, valid_labels, model_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building model graph...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-02e69a356dc2>\u001b[0m in \u001b[0;36mbuild_model_1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         trainable=self.hparams['embedding_trainable'])\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2195\u001b[0m     \"\"\"\n\u001b[1;32m   2196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2198\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    167\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 11718230016"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt')\n",
    "model.train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM or CNN Models with Our Debiased Word Embeddings, CTF Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1.1: 5 Counterfactuals, same_template, different person, ctf_reg=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "\n",
      " prepping:  ['The situation makes Malik feel angry.', 'The situation makes Roger feel angry.']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'tiled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-bcac82383fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"custom_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctf_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pearson score \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpearson_coeff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-62553bf676f3>\u001b[0m in \u001b[0;36mctf_train\u001b[0;34m(self, train_data, train_labels, valid_data, valid_labels, model_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building model graph...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctf_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-62553bf676f3>\u001b[0m in \u001b[0;36mctf_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Now, compile with the loss becoming the custom loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         self.model.compile(\n\u001b[0;32m---> 83\u001b[0;31m             loss=self.custom_loss, optimizer=rmsprop, metrics=['accuracy'])    \n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mctf_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/audace/Library/Python/2.7/lib/python/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 342\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/audace/Library/Python/2.7/lib/python/site-packages/keras/engine/training_utils.pyc\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-62553bf676f3>\u001b[0m in \u001b[0;36mcustom_loss\u001b[0;34m(self, ytrue, ypred)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmse_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mctf_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_counterfactual_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmse_error\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mctf_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-62553bf676f3>\u001b[0m in \u001b[0;36mget_counterfactual_mse\u001b[0;34m(self, ypred)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m##print(\"tiled: \", tiled)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpred_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctf_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'tiled' is not defined"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=5, ctf_reg=100)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1.2:  5 Counterfactuals, same_template, different person, ctf_reg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=5, ctf_reg=1)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic 2.1: 10 counterfactuals, same template, different person, ctf_reg=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=10, ctf_reg=100)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic 2.2: 10 counterfactuals, same template, different person, ctf_reg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=10, ctf_reg=1)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for various bias groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palette for debiasing\n",
    "global_palette = {column_names[0]:'green',column_names[1]:\"orange\", column_names[2]:\"blue\", \n",
    "                  column_names[3]:'red', column_names[4]:'yellow'}\n",
    "xticklabels = column_names[:]\n",
    "xticklabels\n",
    "df_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_aa = df_aa[column_names[:]]\n",
    "_df_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = seaborn.stripplot(data=_df_aa,palette=global_palette)\n",
    "#plot.get_figure().savefig('../images/df_aa.png', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=xticklabels,rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"AA$\\uparrow$- E$\\downarrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "for i in xticklabels:\n",
    "    print(i, \" mean \", np.mean(df_aa[i]),\"range\", np.min(np.abs(df_aa[i])),np.max(np.abs(df_aa[i])))\n",
    "#plot.get_figure().savefig('../images/df_aa.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
