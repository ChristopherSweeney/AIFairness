{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from model_tool import ToxModel\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import model_tool\n",
    "import model_bias_analysis\n",
    "\n",
    "# autoreload makes it easier to interactively work on code in the model_bias_analysis module.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook shows how one can rebalance a dataset and use debiased word embeddings to create fairer classifiers for Toxicity classification. We show that using debiased word embeddings can improve fairness via metrics proposed in http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_9.pdf. This Notebook contains code from https://github.com/conversationai/unintended-ml-bias-analysis. And is split up into 3 parts\n",
    "\n",
    "- Dataset Loading and Model Training\n",
    "- Evaluate Model Fairness\n",
    "- Visulize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split\n",
    "hparams_100 = {\n",
    "    'max_sequence_length': 250,\n",
    "    'max_num_words': 10000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'stop_early': True,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 20,\n",
    "    'dropout_rate': 0.3,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5, 5, 5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': True\n",
    "}\n",
    "hparams_101 = {\n",
    "    'max_sequence_length': 250,\n",
    "    'max_num_words': 10000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'stop_early': True,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 20,\n",
    "    'dropout_rate': 0.3,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [4, 4, 4],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': True\n",
    "}\n",
    "hparams_102 = {\n",
    "    'max_sequence_length': 250,\n",
    "    'max_num_words': 10000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'stop_early': True,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 20,\n",
    "    'dropout_rate': 0.3,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [6, 6, 6],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(name,data,word_embeddings_path,params=[]):\n",
    "    count = 100\n",
    "    for i in params:\n",
    "        model_version = name+\"_\"+str(count)\n",
    "        count+=1\n",
    "        model = ToxModel(hparams=i)\n",
    "        print(\"Training {model_version}\")\n",
    "        model.train(data['train'], data['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = model_version)\n",
    "        print(\"Testing Model\")\n",
    "        test = pd.read_csv(data['test'])\n",
    "        print(model.score_auc(test['comment'], test['is_toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.16978, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 9s - loss: 0.2356 - acc: 0.9181 - val_loss: 0.1698 - val_acc: 0.9383\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 0.16978 to 0.15647, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1626 - acc: 0.9407 - val_loss: 0.1565 - val_acc: 0.9450\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.15647 to 0.14717, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1442 - acc: 0.9474 - val_loss: 0.1472 - val_acc: 0.9477\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.14717 to 0.13458, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1326 - acc: 0.9515 - val_loss: 0.1346 - val_acc: 0.9485\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.13458 to 0.13023, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1236 - acc: 0.9544 - val_loss: 0.1302 - val_acc: 0.9524\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "train_models('wiki_debias_random_cnn_v3',random,'../data/glove.6B.100d.txt',[hparams_100,hparams_101,hparams_102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models('wiki_cnn_v3',wiki,'../data/glove.6B.100d.txt',[hparams_100,hparams_101,hparams_102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models('wiki_debias_cnn_v3',debias,'../data/glove.6B.100d.txt',[hparams_100,hparams_101,hparams_102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models('cnn_debias_tox_v3_debiased_WE',debias,'../data/glove_debias.txt',[hparams_100,hparams_101,hparams_102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models('we_wiki_cnn',wiki,'../data/glove_debias.txt',[hparams_100,hparams_101,hparams_102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = '../models/'\n",
    "\n",
    "wiki_model_names = ['wiki_cnn_v3_{}'.format(i) for i in xrange(100, 103)]\n",
    "wiki_models = [model_tool.ToxModel(name) for name in wiki_model_names]\n",
    "\n",
    "random_model_names = ['wiki_debias_random_cnn_v3_{}'.format(i) for i in xrange(100, 103)]\n",
    "random_models = [model_tool.ToxModel(name) for name in random_model_names]\n",
    "\n",
    "debias_model_names = ['wiki_debias_cnn_v3_{}'.format(i) for i in xrange(100, 103)]\n",
    "debias_models = [model_tool.ToxModel(name) for name in debias_model_names]\n",
    "\n",
    "we_debias_model_names = ['cnn_debias_tox_v3_debiased_WE_{}'.format(i) for i in xrange(100, 103)]\n",
    "we_debias_models = [model_tool.ToxModel(name,embeddings_path='../data/glove_debias.txt') for name in we_debias_model_names]\n",
    "\n",
    "we_wiki_model_names = ['we_wiki_cnn_{}'.format(i) for i in xrange(100, 103)]\n",
    "we_wiki_models = [model_tool.ToxModel(name,embeddings_path='../data/glove_debias.txt') for name in we_wiki_model_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_families = [wiki_models, random_models, debias_models,we_debias_models,we_wiki_models]\n",
    "all_model_families_names = [wiki_model_names, random_model_names, debias_model_names,we_debias_model_names,we_wiki_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = wiki_models + random_models + debias_models+ we_debias_models+we_wiki_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/eval_datasets/bias_madlibs_77k_scored.csv')\n",
    "madlibs = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/eval_datasets/bias_madlibs_77k.csv',\n",
    "    scored_path='../data/eval_datasets/bias_madlibs_77k_scored.csv',\n",
    "    postprocess_fn=model_tool.postprocess_madlibs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/wiki_test_scored.csv')\n",
    "wiki_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/wiki_test.csv',\n",
    "    scored_path='../data/wiki_test_scored.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/wiki_debias_test_scored_newest.csv')\n",
    "os.remove('../data/wiki_debias_random_test_scored_newest.csv')\n",
    "\n",
    "wiki_debias_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/wiki_debias_test.csv',\n",
    "    scored_path='../data/wiki_debias_test_scored_newest.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)\n",
    "\n",
    "wiki_random_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/wiki_debias_random_test.csv',\n",
    "    scored_path='../data/wiki_debias_random_test_scored_newest.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, wiki_model_names, 'label')\n",
    "print('\\n\\nrandom models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, random_model_names, 'label')\n",
    "print('\\n\\ndebias models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, debias_model_names, 'label');\n",
    "print('\\n\\nwe debias models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, we_debias_model_names, 'label');\n",
    "print('\\n\\nwe wiki models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, we_wiki_model_names, 'label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_families_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in [('orig test', wiki_test),\n",
    "                              ('debias test', wiki_debias_test),\n",
    "                              ('random test', wiki_random_test),\n",
    "                              ('madlibs', madlibs)]:\n",
    "    print('\\n\\nAUCs on', dataset_name)\n",
    "    for model_family in all_model_families_names:\n",
    "        fam_name = model_bias_analysis.model_family_name(model_family)\n",
    "        fam_auc = model_bias_analysis.model_family_auc(dataset, model_family, 'label')\n",
    "        print('{:30s}  mean {:.4f}\\t median {:.4f}\\t stddev {:.4f}'.format(fam_name, fam_auc['mean'], fam_auc['median'], fam_auc['std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-term pinned AUC\n",
    "\n",
    "Per-term pinned AUC values show improved scores and less disaprity for the debiased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlibs_terms = model_bias_analysis.read_identity_terms('../data/bias_madlibs_data/adjectives_people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.add_subgroup_columns_from_text(madlibs, 'text', madlibs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_term_madlibs_aucs = model_bias_analysis.per_subgroup_aucs(madlibs, madlibs_terms, all_model_families_names, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_mean = 'wiki_cnn_v3_10_mean'\n",
    "random_mean = 'wiki_debias_random_cnn_v3_10_mean'\n",
    "debias_mean= 'wiki_debias_cnn_v3_10_mean'\n",
    "we_debias_mean = 'cnn_debias_tox_v3_debiased_WE_10_mean'\n",
    "we_wiki_mean = 'we_wiki_cnn_10_mean'\n",
    "\n",
    "\n",
    "for mean_col in [orig_mean, random_mean, debias_mean,we_debias_mean,we_wiki_mean]:\n",
    "    print('per-term AUC histogram: mean AUCs across terms for:', mean_col)\n",
    "    _raw_term_madlibs_aucs[mean_col].hist()\n",
    "    plt.gca().set_xlim((0.85, 1.0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_aucs = 'wiki_cnn_v3_10_aucs'\n",
    "random_aucs = 'wiki_debias_random_cnn_v3_10_aucs'\n",
    "debias_aucs= 'wiki_debias_cnn_v3_10_aucs'\n",
    "we_debias_aucs = 'cnn_debias_tox_v3_debiased_WE_10_aucs'\n",
    "we_wiki_aucs = 'we_wiki_cnn_10_aucs'\n",
    "for title, auc_collection_col in [('original',orig_aucs), ('random',random_aucs), ('debias',debias_aucs),('we_debias',we_debias_aucs),('we_wiki',we_wiki_aucs)]:\n",
    "    print(auc_collection_col)\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_madlibs_aucs, 'subgroup', auc_collection_col, title='Per-term AUC distributions for ' + title,\n",
    "        file_name='madlibs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-term  AUCs on the wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.add_subgroup_columns_from_text(wiki_test, 'text', madlibs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_raw_term_wiki_aucs = model_bias_analysis.per_subgroup_aucs(wiki_test, madlibs_terms, all_model_families_names, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see a strong effect in per-term pinned AUCs on the wikipedia test set. This isn't surprising, as the dataset bias affects the test set too: there are few non-toxic examples using the bias terms, and so the AUC of the original models does not suffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC on tiny datasets is very noisy.\n",
    "_term_wiki_aucs_nottiny = _raw_term_wiki_aucs[_raw_term_wiki_aucs['subset_size'] > 50]\n",
    "\n",
    "for title, auc_collection_col in [('original',orig_aucs), ('random',random_aucs), ('debias',debias_aucs),('we_debias',we_debias_aucs),('we_wiki',we_wiki_aucs)]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(_term_wiki_aucs_nottiny, 'subgroup', auc_collection_col,\n",
    "                                              title='Per-term AUC distributions for ' + title, y_lim=(0.8, 1.02),\n",
    "                                              file_name='wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(jetpack): add column to bias madlibs dataset to allow selecting by template\n",
    "# to more easily observe discrepancies in scores for the same template.\n",
    "\n",
    "TEST_TERMS = ['tall', 'wikipedia', 'teacher', 'music', 'box',\n",
    "              'lesbian', 'gay', 'bisexual', 'transgender', 'queer',\n",
    "              'homosexual', 'heterosexual', 'straight',\n",
    "              'muslim', 'jewish', 'jew', 'christian',\n",
    "              'feminist', 'black', 'white']\n",
    "\n",
    "def _print_template_scores(models):\n",
    "    texts = ['i am a {} person'.format(term) for term in TEST_TERMS]\n",
    "    median_scores = [{'median': np.median([model.predict([text]) for model in models]),\n",
    "                      'stddev': np.std([model.predict([text]) for model in models])}\n",
    "                     for text in texts]\n",
    "    scored_texts = zip(median_scores, texts)\n",
    "    for (score, text) in sorted(scored_texts, key=lambda (score, _text): score['median']):\n",
    "        print('{:.3f}  (std: {:.4f})  {}'.format(score['median'], score['stddev'], text))\n",
    "    plt.hist([x['median'] for x in median_scores])\n",
    "    plt.gca().set_xlim([0, 1])\n",
    "    plt.gca().set_ylim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(wiki_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(random_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(debias_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality of Opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate equality of opportunity, we compare false negative rates on a per-term basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_table_means(term_neg_rates):\n",
    "    \"\"\"Helper to display just the mean values of the negative rates.\"\"\"\n",
    "    return (term_neg_rates\n",
    "            [['subgroup',\n",
    "              'orig_fnr_mean',\n",
    "              'random_fnr_mean',\n",
    "              'debias_fnr_mean',\n",
    "              'orig_tnr_mean',\n",
    "              'random_tnr_mean',\n",
    "              'debias_tnr_mean',\n",
    "            ]]\n",
    "            .sort_values('orig_fnr_mean')\n",
    "           )\n",
    "\n",
    "def neg_table_stddevs(term_neg_rates):\n",
    "    \"\"\"Helper to display just the standard deviation values of the negative rates.\"\"\"\n",
    "    return (term_neg_rates\n",
    "            [['subgroup',\n",
    "              'orig_tnr_std',\n",
    "              'random_tnr_std',\n",
    "              'debias_tnr_std',\n",
    "              'orig_tnr_std',\n",
    "              'random_tnr_std',\n",
    "              'debias_tnr_std',\n",
    "              'orig_fnr_mean',  # just for sorting\n",
    "            ]]\n",
    "            .sort_values('orig_fnr_mean')\n",
    "            .drop('orig_fnr_mean', axis=1)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# threshold = .50 \n",
    "\n",
    "_raw_term_neg_rates_50 = model_bias_analysis.per_subgroup_negative_rates(madlibs, madlibs_terms, all_model_families_names, 0.5, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold using per-model equal error rate on overall madlibs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the equal error rate for all models on the overall madlibs dataset in order to\n",
    "# compute the false/true negative rates table at the EER for each model.\n",
    "\n",
    "# Flattened list of all models.\n",
    "_all_model_names = []\n",
    "for model_family_names in all_model_families_names:\n",
    "    _all_model_names.extend(model_family_names)\n",
    "_model_eers_madlibs = model_bias_analysis.per_model_eer(madlibs, 'label', _all_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_raw_term_neg_rates_madlibs_eer = model_bias_analysis.per_subgroup_negative_rates(\n",
    "    madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_term_neg_rates_madlibs_eer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negative rates: TN / (TN + FP)\n",
    "\n",
    "_raw_term_neg_rates_madlibs_eer_sorted = _raw_term_neg_rates_madlibs_eer.sort_values('cnn_debias_tox_v3_debiased_WE_10_fnr_mean')\n",
    "orig_tnr = 'wiki_cnn_v3_10_tnr_values'\n",
    "random_tnr = 'wiki_debias_random_cnn_v3_10_tnr_values'\n",
    "debias_tnr= 'wiki_debias_cnn_v3_10_tnr_values'\n",
    "we_debias_tnr = 'cnn_debias_tox_v3_debiased_WE_10_tnr_values'\n",
    "we_wiki_tnr = 'we_wiki_cnn_10_tnr_values'\n",
    "for title, tnr_values_col in [('original',orig_tnr), ('random',random_tnr), ('debias',debias_tnr),('we_debias',we_debias_tnr),('we_wiki',we_wiki_tnr)]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_neg_rates_madlibs_eer_sorted, 'subgroup', tnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term true negative rates for ' + title, file_name='madlibs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rates: 1 - TNR. This is just the above graphs except flipped to show false positives instead of true negatives.\n",
    "\n",
    "# _term_neg_rates_madlibs_eer_tnr_sorted = _term_neg_rates_madlibs_eer.sort_values('orig_tnr_mean')\n",
    "\n",
    "for title, tnr_values_col in [('original',orig_tnr), ('random',random_tnr), ('debias',debias_tnr),('we_debias',we_debias_tnr),('we_wiki',we_wiki_tnr)]:\n",
    "    term_fpr_values = []\n",
    "    for _i, row in _raw_term_neg_rates_madlibs_eer.iterrows():\n",
    "        tnr_values = row[tnr_values_col]\n",
    "        fpr_values = [1 - tnr for tnr in tnr_values]\n",
    "        term_fpr_values.append({'subgroup': row['subgroup'], 'fpr_values': fpr_values})\n",
    "    fpr_df = pd.DataFrame(term_fpr_values)\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        fpr_df, 'subgroup', 'fpr_values', y_lim=(0, 1.02),\n",
    "        title='Per-term false positive rates for ' + title,\n",
    "        file_name='madlibs_' + tnr_values_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negative rates: FN / (FN + TP). 1 - TPR.\n",
    "\n",
    "# Should we use the same ordering as the true negative rate plots?\n",
    "_raw_term_neg_rates_madlibs_eer_sorted = _raw_term_neg_rates_madlibs_eer.sort_values('cnn_debias_tox_v3_debiased_WE_10_fnr_mean')\n",
    "orig_fnr = 'wiki_cnn_v3_10_fnr_values'\n",
    "random_fnr = 'wiki_debias_random_cnn_v3_10_fnr_values'\n",
    "debias_fnr= 'wiki_debias_cnn_v3_10_fnr_values'\n",
    "we_debias_fnr = 'cnn_debias_tox_v3_debiased_WE_10_fnr_values'\n",
    "we_wiki_fnr = 'we_wiki_cnn_10_fnr_values'\n",
    "for title, fnr_values_col in [('original',orig_fnr), ('random',random_fnr), ('debias',debias_fnr),('we_debias',we_debias_fnr),('we_wiki',we_wiki_fnr)]:\n",
    "    #_term_neg_rates_madlibs_eer_fnr_sorted\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_neg_rates_madlibs_eer_sorted, 'subgroup', fnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term false negative rates for ' + title,\n",
    "        file_name='madlibs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold using per-model equal error rate on Wikipedia test set\n",
    "\n",
    "The EERs computed on the wikipedia test set are similar, and so we don't see much difference in the per-term negative rates plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the equal error rate for all models on the wikipedia dataset.\n",
    "\n",
    "_model_eers_wiki = model_bias_analysis.per_model_eer(wiki_test, 'label', _all_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_raw_term_neg_rates_wiki_eer = model_bias_analysis.per_subgroup_negative_rates(\n",
    "    madlibs, madlibs_terms, all_model_families_names, _model_eers_wiki, 'label')\n",
    "_term_neg_rates_wiki_eer = _raw_term_neg_rates_wiki_eer.rename(columns=column_renamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negative rates: TN / (TN + FP)\n",
    "\n",
    "_term_neg_rates_wiki_eer_tnr_sorted = _term_neg_rates_wiki_eer.sort_values('orig_tnr_mean')\n",
    "\n",
    "for title, tnr_values_col in [('original model', 'orig_tnr_values'),\n",
    "                              ('random treatment', 'random_tnr_values'),\n",
    "                              ('debiasing treatment', 'debias_tnr_values')]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _term_neg_rates_wiki_eer_tnr_sorted, 'subgroup', tnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term true negative rates for ' + title, file_name='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negative rates: FN / (FN + TP). 1 - TPR.\n",
    "\n",
    "# Should we use the same ordering as the true negative rate plots?\n",
    "_term_neg_rates_wiki_eer_fnr_sorted = _term_neg_rates_wiki_eer.sort_values('orig_fnr_mean')\n",
    "\n",
    "for title, fnr_values_col in [('original model', 'orig_fnr_values'),\n",
    "                              ('random treatment', 'random_fnr_values'),\n",
    "                              ('debiasing treatment', 'debias_fnr_values')]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _term_neg_rates_wiki_eer_fnr_sorted, 'subgroup', fnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term false negative rates for ' + title, file_name='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_fnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_tnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_auc_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_auc_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_eers_madlibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_fnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_tnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
