{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D,Conv1D,MaxPooling1D,UpSampling1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _np_normalize(v):\n",
    "  \"\"\"Returns the input vector, normalized.\"\"\"\n",
    "  return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "def load_vectors(client, analogies):\n",
    "  \"\"\"Loads and returns analogies and embeddings.\n",
    "\n",
    "  Args:\n",
    "    client: the client to query.\n",
    "    analogies: a list of analogies.\n",
    "\n",
    "  Returns:\n",
    "    A tuple with:\n",
    "    - the embedding matrix itself\n",
    "    - a dictionary mapping from strings to their corresponding indices\n",
    "      in the embedding matrix\n",
    "    - the list of words, in the order they are found in the embedding matrix\n",
    "  \"\"\"\n",
    "  words_unfiltered = set()\n",
    "  for analogy in analogies:\n",
    "    words_unfiltered.update(analogy)\n",
    "  print (\"found %d unique words\" % len(words_unfiltered))\n",
    "\n",
    "  vecs = []\n",
    "  words = []\n",
    "  index_map = {}\n",
    "  for word in words_unfiltered:\n",
    "    try:\n",
    "      vecs.append(_np_normalize(client.word_vec(word)))\n",
    "      index_map[word] = len(words)\n",
    "      words.append(word)\n",
    "    except KeyError:\n",
    "      print(\"word not found: %s\" , word)\n",
    "  print (\"words not filtered out: %d\" , len(words))\n",
    "\n",
    "  return np.array(vecs), index_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analogies(filename):\n",
    "    analogies = []\n",
    "    with open(filename, \"r\") as fast_file:\n",
    "        for line in fast_file:\n",
    "            line = line.strip()\n",
    "            # in the analogy file, comments start with :\n",
    "            if line[0] == \":\":\n",
    "                continue\n",
    "            words = line.split()\n",
    "            # there are no misformatted lines in the analogy file, so this should\n",
    "            # only happen once we're done reading all analogies.\n",
    "            if len(words) != 4:\n",
    "                print(\"Invalid line: \" , line)\n",
    "                continue\n",
    "            analogies.append(words)\n",
    "    print(\"loaded %d analogies\" % len(analogies))\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(embeddings,analogies):\n",
    "    X = []\n",
    "    y = []\n",
    "    l=[]\n",
    "    for i in analogies:\n",
    "        if all(j.lower() in embeddings for j in i):\n",
    "            B = _np_normalize(embeddings[i[1]])\n",
    "            C = _np_normalize(embeddings[i[2]])\n",
    "            A = _np_normalize(embeddings[i[0]])\n",
    "            D = _np_normalize(embeddings[i[3]])\n",
    "            X.append(B+C-A)\n",
    "            y.append(D)\n",
    "            l.append(i[3])\n",
    "    return (X,y,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X,y,l) = load_vectors(embeddings,analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 19544 analogies\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "local_dir_name = 'data'\n",
    "\n",
    "WORD2VEC_FILE = os.path.join(local_dir_name+\"/embeddings\", \"GoogleNews-vectors-negative300.bin.gz\")\n",
    "ANALOGIES_FILE = os.path.join(local_dir_name, \"questions-words.txt\")\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_FILE, binary=True)\n",
    "analogies = load_analogies(ANALOGIES_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 905 unique words\n",
      "words not filtered out: %d 905\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wv = load_vectors(embeddings, analogies)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f31b450cec6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=300))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(1024))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(BatchNormalization(momentum=0.8))\n",
    "model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "model.add(Reshape(self.img_shape))\n",
    "\n",
    "model.summary()\n",
    "input = Input(shape=(300,))\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"uniform\", input_shape=(300,))`\n",
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\")`\n",
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_36 (Sequential)   (None, 300)               120084    \n",
      "_________________________________________________________________\n",
      "sequential_37 (Sequential)   (None, 1)                 120501    \n",
      "=================================================================\n",
      "Total params: 240,585\n",
      "Trainable params: 240,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5670 samples, validate on 631 samples\n",
      "Epoch 1/10\n",
      "5670/5670 [==============================] - 1s 137us/step - loss: 0.6526 - acc: 0.6995 - val_loss: 0.5617 - val_acc: 0.7195\n",
      "Epoch 2/10\n",
      "5670/5670 [==============================] - 0s 26us/step - loss: 0.4566 - acc: 0.7316 - val_loss: 0.3402 - val_acc: 0.8304\n",
      "Epoch 3/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.2797 - acc: 0.9078 - val_loss: 0.2038 - val_acc: 0.9366\n",
      "Epoch 4/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.1762 - acc: 0.9483 - val_loss: 0.1365 - val_acc: 0.9556\n",
      "Epoch 5/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.1338 - acc: 0.9541 - val_loss: 0.1168 - val_acc: 0.9620\n",
      "Epoch 6/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.1193 - acc: 0.9586 - val_loss: 0.1103 - val_acc: 0.9588\n",
      "Epoch 7/10\n",
      "5670/5670 [==============================] - 0s 26us/step - loss: 0.1103 - acc: 0.9610 - val_loss: 0.1063 - val_acc: 0.9620\n",
      "Epoch 8/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.1050 - acc: 0.9638 - val_loss: 0.1043 - val_acc: 0.9604\n",
      "Epoch 9/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.1011 - acc: 0.9646 - val_loss: 0.1026 - val_acc: 0.9620\n",
      "Epoch 10/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.0976 - acc: 0.9653 - val_loss: 0.1068 - val_acc: 0.9540\n",
      "trained discriminator\n",
      "Epoch 1/20\n",
      "905/905 [==============================] - 1s 805us/step - loss: 0.9177\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 0s 67us/step - loss: 0.6935\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 0s 71us/step - loss: 0.6932\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 0s 74us/step - loss: 0.6931\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 0s 68us/step - loss: 0.6932\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 0s 75us/step - loss: 0.6931\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 0s 69us/step - loss: 0.6931\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 0s 73us/step - loss: 0.6931\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 0s 73us/step - loss: 0.6931\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 0s 69us/step - loss: 0.6931\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 0s 73us/step - loss: 0.6931\n",
      "Epoch 12/20\n",
      "905/905 [==============================] - 0s 71us/step - loss: 0.6931\n",
      "Epoch 13/20\n",
      "905/905 [==============================] - 0s 68us/step - loss: 0.6931\n",
      "Epoch 14/20\n",
      "905/905 [==============================] - 0s 70us/step - loss: 0.6931\n",
      "Epoch 15/20\n",
      "905/905 [==============================] - 0s 73us/step - loss: 0.6931\n",
      "Epoch 16/20\n",
      "905/905 [==============================] - 0s 66us/step - loss: 0.6931\n",
      "Epoch 17/20\n",
      "905/905 [==============================] - 0s 62us/step - loss: 0.6931\n",
      "Epoch 18/20\n",
      "905/905 [==============================] - 0s 70us/step - loss: 0.6931\n",
      "Epoch 19/20\n",
      "905/905 [==============================] - 0s 72us/step - loss: 0.6931\n",
      "Epoch 20/20\n",
      "905/905 [==============================] - 0s 68us/step - loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "class GAN(object):\n",
    "    def __init__(self):\n",
    "\n",
    "    \n",
    "        self.OPTIMIZER = Adam(lr=0.0002, decay=8e-9)\n",
    "\n",
    "        self.G = self.generator()\n",
    "        self.G.compile(loss='mse', optimizer=self.OPTIMIZER)\n",
    "\n",
    "        self.D = self.discriminator()\n",
    "        self.D.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "\n",
    "        self.stacked_G_D = self.stacked()\n",
    "        self.stacked_G_D.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        print(self.stacked_G_D.summary())\n",
    "        \n",
    "    def generator(self):\n",
    "            input_size = 300\n",
    "            hidden_size = 64\n",
    "            output_size = 300\n",
    "\n",
    "            model = Sequential()\n",
    "            # Encoder\n",
    "            model.add(Dense(164, activation='relu',input_shape=(input_size,)))\n",
    "            model.add(Dense(64, activation='relu',input_shape=(input_size,)))\n",
    "\n",
    "            # Decoder\n",
    "            model.add(Dense(164,activation='sigmoid'))   \n",
    "            model.add(Dense(output_size, activation='sigmoid'))   \n",
    "            return model\n",
    "\n",
    "    def discriminator(self):\n",
    "\n",
    "        model_adv = Sequential()\n",
    "        model_adv.add(Dense(300, init='uniform',input_shape=(300,)))\n",
    "        model_adv.add(Activation('relu'))\n",
    "        model_adv.add(Dense(100, init='uniform'))\n",
    "        model_adv.add(Activation('relu'))\n",
    "        model_adv.add(Dense(1, init='uniform'))\n",
    "        model_adv.add(Activation('sigmoid'))\n",
    "        return model_adv\n",
    "\n",
    "    def stacked(self):\n",
    "            model = Sequential()\n",
    "            model.add(self.G)\n",
    "            model.add(self.D)\n",
    "\n",
    "            return model\n",
    "\n",
    "    def train(self, epochs=200, batch = 32, save_interval = 200):\n",
    "\n",
    "            #train discriminator\n",
    "            d_loss = self.D.fit(train_vectors, train_targets,epochs=10,batch_size=128,validation_data=(test_vectors,test_targets))\n",
    "            self.D.trainable = False\n",
    "            print(\"trained discriminator\")\n",
    "            # train generator\n",
    "            g_loss = self.stacked_G_D.fit(wv,np.ones(len(wv))*.5,batch_size=64,epochs=20,shuffle=True)\n",
    "    #         print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))\n",
    "\n",
    "\n",
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_mean_absolute_error(y_true, y_pred):\n",
    "    \n",
    "    return losses.mean_absolute_error(y_true,y_pred)-losses.mean_absolute_error(a_true,a_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n",
    "\n",
    "def adv_mean_absolute_error(y_true, y_pred):\n",
    "    y_pred_np = y_pred.eval() \n",
    "    a_pred= map(lambda x: lr.predict([x])[1],y_pred_np)\n",
    "#     a_pred= K.map_fn(lambda x: lr.predict([x.eval(session=K.get_session())])[1],y_pred)\n",
    "    a_true = np.ones(len(y_true))*.5\n",
    "    return losses.mean_absolute_error(y_true,y_pred)-losses.mean_absolute_error(a_true,a_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adv_mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def adv_mean_absolute_error(y_true, y_pred):\n",
    "    a_pred= K.map_fn(lambda x: lr.predict(x[1]),y_pred)\n",
    "    l_adv = K.mean(K.binary_crossentropy(y_true, a_pred), axis=-1)\n",
    "    return K.mean(K.abs(y_pred - y_true), axis=-1)-l_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_binary_crossentropy(y_true, y_pred):\n",
    "return -K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities_wv = np.array(map(lambda x: embeddings[x.lower()], filter(lambda x: x.lower() in embeddings,gender)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "905/905 [==============================] - 1s 1ms/step - loss: 45.5713\n",
      "Epoch 2/100\n",
      "905/905 [==============================] - 0s 19us/step - loss: 37.0855\n",
      "Epoch 3/100\n",
      "905/905 [==============================] - 0s 19us/step - loss: 30.4100\n",
      "Epoch 4/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: 24.0541\n",
      "Epoch 5/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: 18.6177\n",
      "Epoch 6/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: 13.8553\n",
      "Epoch 7/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: 9.5540\n",
      "Epoch 8/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: 6.0487\n",
      "Epoch 9/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: 3.1738\n",
      "Epoch 10/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: 0.7754\n",
      "Epoch 11/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -1.2408\n",
      "Epoch 12/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -2.9777\n",
      "Epoch 13/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -4.4941\n",
      "Epoch 14/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -5.7675\n",
      "Epoch 15/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -6.8558\n",
      "Epoch 16/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -7.7437\n",
      "Epoch 17/100\n",
      "905/905 [==============================] - 0s 24us/step - loss: -8.5714\n",
      "Epoch 18/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -9.2698\n",
      "Epoch 19/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -9.9276\n",
      "Epoch 20/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -10.5427\n",
      "Epoch 21/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -11.1214\n",
      "Epoch 22/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -11.6705\n",
      "Epoch 23/100\n",
      "905/905 [==============================] - 0s 25us/step - loss: -12.1974\n",
      "Epoch 24/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -12.6944\n",
      "Epoch 25/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -13.1661\n",
      "Epoch 26/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -13.6129\n",
      "Epoch 27/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -14.0335\n",
      "Epoch 28/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -14.4258\n",
      "Epoch 29/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -14.7826\n",
      "Epoch 30/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -15.1013\n",
      "Epoch 31/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -15.3821\n",
      "Epoch 32/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -15.6267\n",
      "Epoch 33/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -15.8354\n",
      "Epoch 34/100\n",
      "905/905 [==============================] - 0s 24us/step - loss: -16.0103\n",
      "Epoch 35/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.1565\n",
      "Epoch 36/100\n",
      "905/905 [==============================] - 0s 25us/step - loss: -16.2772\n",
      "Epoch 37/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.3757\n",
      "Epoch 38/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.4547\n",
      "Epoch 39/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -16.5221\n",
      "Epoch 40/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.5786\n",
      "Epoch 41/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.6267\n",
      "Epoch 42/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -16.6676\n",
      "Epoch 43/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.7023\n",
      "Epoch 44/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.7320\n",
      "Epoch 45/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.7579\n",
      "Epoch 46/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.7801\n",
      "Epoch 47/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.7997\n",
      "Epoch 48/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.8169\n",
      "Epoch 49/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.8311\n",
      "Epoch 50/100\n",
      "905/905 [==============================] - 0s 25us/step - loss: -16.8449\n",
      "Epoch 51/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.8578\n",
      "Epoch 52/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.8689\n",
      "Epoch 53/100\n",
      "905/905 [==============================] - ETA: 0s - loss: -16.874 - 0s 22us/step - loss: -16.8793\n",
      "Epoch 54/100\n",
      "905/905 [==============================] - 0s 26us/step - loss: -16.8881\n",
      "Epoch 55/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.8969\n",
      "Epoch 56/100\n",
      "905/905 [==============================] - 0s 24us/step - loss: -16.9048\n",
      "Epoch 57/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9113\n",
      "Epoch 58/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9176\n",
      "Epoch 59/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9237\n",
      "Epoch 60/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9293\n",
      "Epoch 61/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -16.9353\n",
      "Epoch 62/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9393\n",
      "Epoch 63/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9449\n",
      "Epoch 64/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.9487\n",
      "Epoch 65/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9527\n",
      "Epoch 66/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9573\n",
      "Epoch 67/100\n",
      "905/905 [==============================] - 0s 23us/step - loss: -16.9608\n",
      "Epoch 68/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9652\n",
      "Epoch 69/100\n",
      "905/905 [==============================] - 0s 24us/step - loss: -16.9682\n",
      "Epoch 70/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9721\n",
      "Epoch 71/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9748\n",
      "Epoch 72/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9779\n",
      "Epoch 73/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9815\n",
      "Epoch 74/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9848\n",
      "Epoch 75/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9870\n",
      "Epoch 76/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9897\n",
      "Epoch 77/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9924\n",
      "Epoch 78/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9947\n",
      "Epoch 79/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -16.9976\n",
      "Epoch 80/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -16.9998\n",
      "Epoch 81/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0025\n",
      "Epoch 82/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0046\n",
      "Epoch 83/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0069\n",
      "Epoch 84/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0085\n",
      "Epoch 85/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0118\n",
      "Epoch 86/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0131\n",
      "Epoch 87/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0152\n",
      "Epoch 88/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0171\n",
      "Epoch 89/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0194\n",
      "Epoch 90/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0213\n",
      "Epoch 91/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0230\n",
      "Epoch 92/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0252\n",
      "Epoch 93/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0277\n",
      "Epoch 94/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0291\n",
      "Epoch 95/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0311\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905/905 [==============================] - 0s 22us/step - loss: -17.0329\n",
      "Epoch 97/100\n",
      "905/905 [==============================] - 0s 21us/step - loss: -17.0339\n",
      "Epoch 98/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -17.0364\n",
      "Epoch 99/100\n",
      "905/905 [==============================] - 0s 20us/step - loss: -17.0377\n",
      "Epoch 100/100\n",
      "905/905 [==============================] - 0s 22us/step - loss: -17.0400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa92291cdd0>"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(shape=(input_size,))\n",
    "\n",
    "encoder = Dense(input_size, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "encoder = Dense(int(input_size/ 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(input_size / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_size, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=x, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='kullback_leibler_divergence')\n",
    "autoencoder.fit(wv,wv,batch_size=900,epochs=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "905/905 [==============================] - 1s 1ms/step - loss: 0.2530\n",
      "Epoch 2/100\n",
      "905/905 [==============================] - 0s 201us/step - loss: 0.2465\n",
      "Epoch 3/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.2388\n",
      "Epoch 4/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.2291\n",
      "Epoch 5/100\n",
      "905/905 [==============================] - 0s 211us/step - loss: 0.2168\n",
      "Epoch 6/100\n",
      "905/905 [==============================] - 0s 203us/step - loss: 0.2011\n",
      "Epoch 7/100\n",
      "905/905 [==============================] - 0s 214us/step - loss: 0.1813\n",
      "Epoch 8/100\n",
      "905/905 [==============================] - 0s 214us/step - loss: 0.1569\n",
      "Epoch 9/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.1281\n",
      "Epoch 10/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0961\n",
      "Epoch 11/100\n",
      "905/905 [==============================] - 0s 212us/step - loss: 0.0643\n",
      "Epoch 12/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.0375\n",
      "Epoch 13/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.0194\n",
      "Epoch 14/100\n",
      "905/905 [==============================] - 0s 198us/step - loss: 0.0098\n",
      "Epoch 15/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0058\n",
      "Epoch 16/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.0043\n",
      "Epoch 17/100\n",
      "905/905 [==============================] - 0s 216us/step - loss: 0.0038\n",
      "Epoch 18/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0036\n",
      "Epoch 19/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0035\n",
      "Epoch 20/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0035\n",
      "Epoch 21/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 22/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 23/100\n",
      "905/905 [==============================] - 0s 212us/step - loss: 0.0034\n",
      "Epoch 24/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 26/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 27/100\n",
      "905/905 [==============================] - 0s 203us/step - loss: 0.0034\n",
      "Epoch 28/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 29/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0034\n",
      "Epoch 30/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0034\n",
      "Epoch 31/100\n",
      "905/905 [==============================] - 0s 211us/step - loss: 0.0034\n",
      "Epoch 32/100\n",
      "905/905 [==============================] - 0s 219us/step - loss: 0.0034\n",
      "Epoch 33/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.0034\n",
      "Epoch 34/100\n",
      "905/905 [==============================] - 0s 220us/step - loss: 0.0034\n",
      "Epoch 35/100\n",
      "905/905 [==============================] - 0s 204us/step - loss: 0.0034\n",
      "Epoch 36/100\n",
      "905/905 [==============================] - 0s 204us/step - loss: 0.0033\n",
      "Epoch 37/100\n",
      "905/905 [==============================] - 0s 211us/step - loss: 0.0033\n",
      "Epoch 38/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 39/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 40/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 41/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 42/100\n",
      "905/905 [==============================] - 0s 207us/step - loss: 0.0033\n",
      "Epoch 43/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 44/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 45/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.0033\n",
      "Epoch 46/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 47/100\n",
      "905/905 [==============================] - 0s 212us/step - loss: 0.0033\n",
      "Epoch 48/100\n",
      "905/905 [==============================] - 0s 212us/step - loss: 0.0033\n",
      "Epoch 49/100\n",
      "905/905 [==============================] - 0s 211us/step - loss: 0.0033\n",
      "Epoch 50/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0033\n",
      "Epoch 51/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 52/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 53/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 54/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 55/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 56/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 57/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 58/100\n",
      "905/905 [==============================] - 0s 202us/step - loss: 0.0033\n",
      "Epoch 59/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.0033\n",
      "Epoch 60/100\n",
      "905/905 [==============================] - 0s 207us/step - loss: 0.0033\n",
      "Epoch 61/100\n",
      "905/905 [==============================] - 0s 199us/step - loss: 0.0033\n",
      "Epoch 62/100\n",
      "905/905 [==============================] - 0s 210us/step - loss: 0.0033\n",
      "Epoch 63/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.0033\n",
      "Epoch 64/100\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.003 - 0s 218us/step - loss: 0.0033\n",
      "Epoch 65/100\n",
      "905/905 [==============================] - 0s 216us/step - loss: 0.0033\n",
      "Epoch 66/100\n",
      "905/905 [==============================] - 0s 204us/step - loss: 0.0033\n",
      "Epoch 67/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 68/100\n",
      "905/905 [==============================] - 0s 207us/step - loss: 0.0033\n",
      "Epoch 69/100\n",
      "905/905 [==============================] - 0s 203us/step - loss: 0.0033\n",
      "Epoch 70/100\n",
      "905/905 [==============================] - 0s 219us/step - loss: 0.0033\n",
      "Epoch 71/100\n",
      "905/905 [==============================] - 0s 214us/step - loss: 0.0033\n",
      "Epoch 72/100\n",
      "905/905 [==============================] - 0s 216us/step - loss: 0.0033\n",
      "Epoch 73/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 74/100\n",
      "905/905 [==============================] - 0s 203us/step - loss: 0.0033\n",
      "Epoch 75/100\n",
      "905/905 [==============================] - 0s 204us/step - loss: 0.0033\n",
      "Epoch 76/100\n",
      "905/905 [==============================] - 0s 196us/step - loss: 0.0033\n",
      "Epoch 77/100\n",
      "905/905 [==============================] - 0s 202us/step - loss: 0.0033\n",
      "Epoch 78/100\n",
      "905/905 [==============================] - 0s 201us/step - loss: 0.0033\n",
      "Epoch 79/100\n",
      "905/905 [==============================] - 0s 199us/step - loss: 0.0033\n",
      "Epoch 80/100\n",
      "905/905 [==============================] - 0s 207us/step - loss: 0.0033\n",
      "Epoch 81/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 82/100\n",
      "905/905 [==============================] - 0s 203us/step - loss: 0.0033\n",
      "Epoch 83/100\n",
      "905/905 [==============================] - 0s 212us/step - loss: 0.0033\n",
      "Epoch 84/100\n",
      "905/905 [==============================] - ETA: 0s - loss: 0.003 - 0s 220us/step - loss: 0.0033\n",
      "Epoch 85/100\n",
      "905/905 [==============================] - 0s 215us/step - loss: 0.0033\n",
      "Epoch 86/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.0033\n",
      "Epoch 87/100\n",
      "905/905 [==============================] - 0s 213us/step - loss: 0.0033\n",
      "Epoch 88/100\n",
      "905/905 [==============================] - 0s 217us/step - loss: 0.0033\n",
      "Epoch 89/100\n",
      "905/905 [==============================] - 0s 216us/step - loss: 0.0033\n",
      "Epoch 90/100\n",
      "905/905 [==============================] - 0s 226us/step - loss: 0.0033\n",
      "Epoch 91/100\n",
      "905/905 [==============================] - 0s 215us/step - loss: 0.0033\n",
      "Epoch 92/100\n",
      "905/905 [==============================] - 0s 215us/step - loss: 0.0033\n",
      "Epoch 93/100\n",
      "905/905 [==============================] - 0s 206us/step - loss: 0.0033\n",
      "Epoch 94/100\n",
      "905/905 [==============================] - 0s 202us/step - loss: 0.0033\n",
      "Epoch 95/100\n",
      "905/905 [==============================] - 0s 205us/step - loss: 0.0033\n",
      "Epoch 96/100\n",
      "905/905 [==============================] - 0s 201us/step - loss: 0.0033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "905/905 [==============================] - 0s 207us/step - loss: 0.0033\n",
      "Epoch 98/100\n",
      "905/905 [==============================] - 0s 208us/step - loss: 0.0033\n",
      "Epoch 99/100\n",
      "905/905 [==============================] - 0s 209us/step - loss: 0.0033\n",
      "Epoch 100/100\n",
      "905/905 [==============================] - 0s 217us/step - loss: 0.0033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8f91ff3d0>"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(shape=(input_size,1))\n",
    "\n",
    "# Encoder\n",
    "conv1_1 = Conv1D(32, 3, activation='relu', padding='same')(x)\n",
    "pool1 = MaxPooling1D(2, padding='same')(conv1_1)\n",
    "conv1_2 = Conv1D(32, 3, activation='relu', padding='same')(pool1)\n",
    "h = MaxPooling1D(2, padding='same')(conv1_2)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "conv2_1 = Conv1D(32, 3, activation='relu', padding='same')(h)\n",
    "up1 = UpSampling1D(2)(conv2_1)\n",
    "conv2_2 = Conv1D(32, 3, activation='relu', padding='same')(up1)\n",
    "up2 = UpSampling1D(2)(conv2_2)\n",
    "r = Conv1D(1, 3, activation='sigmoid', padding='same')(up2)\n",
    "\n",
    "autoencoder = Model(input=x, output=r)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(np.expand_dims(wv,axis=2),np.expand_dims(wv,axis=2),batch_size=900,epochs=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_44 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 300)               90300     \n",
      "=================================================================\n",
      "Total params: 90,300\n",
      "Trainable params: 90,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "905/905 [==============================] - 1s 1ms/step - loss: 1.2621\n",
      "Epoch 2/20\n",
      "905/905 [==============================] - 0s 13us/step - loss: 1.1870\n",
      "Epoch 3/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 1.1328\n",
      "Epoch 4/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 1.0880\n",
      "Epoch 5/20\n",
      "905/905 [==============================] - 0s 13us/step - loss: 1.0491\n",
      "Epoch 6/20\n",
      "905/905 [==============================] - 0s 13us/step - loss: 1.0149\n",
      "Epoch 7/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 0.9835\n",
      "Epoch 8/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.9540\n",
      "Epoch 9/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 0.9263\n",
      "Epoch 10/20\n",
      "905/905 [==============================] - 0s 12us/step - loss: 0.8990\n",
      "Epoch 11/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.8726\n",
      "Epoch 12/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.8482\n",
      "Epoch 13/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 0.8238\n",
      "Epoch 14/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.8002\n",
      "Epoch 15/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 0.7780\n",
      "Epoch 16/20\n",
      "905/905 [==============================] - 0s 16us/step - loss: 0.7559\n",
      "Epoch 17/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.7349\n",
      "Epoch 18/20\n",
      "905/905 [==============================] - 0s 15us/step - loss: 0.7148\n",
      "Epoch 19/20\n",
      "905/905 [==============================] - 0s 14us/step - loss: 0.6954\n",
      "Epoch 20/20\n",
      "905/905 [==============================] - 0s 16us/step - loss: 0.6760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8f6ccc350>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "input_size = 300\n",
    "hidden_size = 64\n",
    "output_size = 300\n",
    "\n",
    "x = Input(shape=(input_size,))\n",
    "\n",
    "# Encoder\n",
    "h = Dense(300, activation=None, activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "# h = Dense(164, activation='relu',activity_regularizer=regularizers.l1(10e-5))(h)\n",
    "# # Decoder\n",
    "# r = Dense(output_size, activation='relu',activity_regularizer=regularizers.l1(10e-5))(h)\n",
    "# r = Dense(output_size, activation='sigmoid')(r)\n",
    "\n",
    "autoencoder = Model(input=x, output=h)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(wv,wv,batch_size=900,epochs=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "#is this like radial basis function making everything equal\n",
    "op = []\n",
    "for i in gender:\n",
    "    if i.lower() in embeddings:\n",
    "        o=np.expand_dims(embeddings[i.lower()],axis=0)\n",
    "        op.append(lr.predict_proba([autoencoder.predict(o)[0,:]])[0][1])\n",
    "plt.bar(np.arange(len(op)),op)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "#is this like radial basis function making everything equal\n",
    "\n",
    "op = []\n",
    "for i in gender:\n",
    "    if i.lower() in embeddings:\n",
    "        op.append(lr.predict_proba([gan.G.predict(np.expand_dims(embeddings[i.lower()],axis=0))[0,:]])[0][1])\n",
    "plt.bar(np.arange(len(op)),op)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "#is this like radial basis function making everything equal\n",
    "op = []\n",
    "for i in gender:\n",
    "    if i.lower() in embeddings:\n",
    "        op.append(lr.predict_proba([embeddings[i.lower()]])[0][1])\n",
    "plt.bar(np.arange(len(op)),op)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = embeddings[\"athens\"]\n",
    "b = embeddings[\"greece\"]\n",
    "c = embeddings[\"moscow\"]\n",
    "d = embeddings[\"male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'greece', 0.7920663952827454),\n",
       " (u'moscow', 0.7169557213783264),\n",
       " (u'europe', 0.5698994398117065),\n",
       " (u'malta', 0.5560809373855591),\n",
       " (u'serbia', 0.5415053367614746),\n",
       " (u'cristiano_ronaldo', 0.5405558347702026),\n",
       " (u'norway', 0.5325840711593628),\n",
       " (u'portugal', 0.5311784744262695),\n",
       " (u'russia', 0.5276082158088684),\n",
       " (u'spain', 0.5275852680206299)]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similar_by_vector(b+c-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Website_http://www.cgi.com', 0.25435882806777954),\n",
       " (u'AP_HOCKEY_NEWS', 0.22141584753990173),\n",
       " (u'aide_Melanie_Fonder', 0.21890948712825775),\n",
       " (u'week.But', 0.21821339428424835),\n",
       " (u'TBLISI_Georgia', 0.21708080172538757),\n",
       " (u'Cindy_Crawford', 0.2166801244020462),\n",
       " (u'Ovidiu_Rom', 0.2155206799507141),\n",
       " (u'Residential_Tenancies_Amendment', 0.2123904973268509),\n",
       " (u'EPP_ED_PL', 0.21208089590072632),\n",
       " (u'etwcf', 0.21192854642868042)]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similar_by_vector(gan.G.predict(np.expand_dims(b+c-a,axis=0))[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_41 to have 2 dimensions, but got array with shape (1, 300, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-438-72d8ded3c003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1771\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_41 to have 2 dimensions, but got array with shape (1, 300, 1)"
     ]
    }
   ],
   "source": [
    "embeddings.similar_by_vector(autoencoder.predict(np.expand_dims(np.expand_dims(b+c-a,axis=0),axis=2))[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'her', 0.9999999403953552),\n",
       " (u'she', 0.7834683656692505),\n",
       " (u'herself', 0.7382731437683105),\n",
       " (u'Her', 0.7029306888580322),\n",
       " (u'hers', 0.6714211702346802),\n",
       " (u'his', 0.6359063386917114),\n",
       " (u'She', 0.622606635093689),\n",
       " (u'my', 0.5852576494216919),\n",
       " (u\"Je_t'aime_Papou\", 0.5452724695205688),\n",
       " (u'mother', 0.5382469296455383)]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similar_by_vector(embeddings[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Website_http://www.cgi.com', 0.2545045018196106),\n",
       " (u'AP_HOCKEY_NEWS', 0.2215508222579956),\n",
       " (u'aide_Melanie_Fonder', 0.21896588802337646),\n",
       " (u'week.But', 0.21843330562114716),\n",
       " (u'TBLISI_Georgia', 0.21695643663406372),\n",
       " (u'Cindy_Crawford', 0.2166750431060791),\n",
       " (u'Ovidiu_Rom', 0.21495576202869415),\n",
       " (u'Residential_Tenancies_Amendment', 0.21293756365776062),\n",
       " (u'EPP_ED_PL', 0.21205320954322815),\n",
       " (u'Gretel_Packer', 0.2117219865322113)]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similar_by_vector(gan.G.predict(np.expand_dims(embeddings[p],axis=0))[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'salty_snack', 0.2719143033027649),\n",
       " (u'eaten', 0.2561814785003662),\n",
       " (u'\\xc9lys\\xe9e_Palace', 0.2546929717063904),\n",
       " (u'LiveSmart_Adult_Lamb', 0.2510547637939453),\n",
       " (u'Jamie_Gorelick', 0.2503967881202698),\n",
       " (u'Governor_Goodhair', 0.24670206010341644),\n",
       " (u'coyotes_circling', 0.24638763070106506),\n",
       " (u'aide_Rashid_Qureshi', 0.2457980513572693),\n",
       " (u'drummers_drumming_purchased', 0.24541160464286804),\n",
       " (u'finicky_eating_habits', 0.24303007125854492)]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.similar_by_vector(autoencoder.predict(np.expand_dims(embeddings[p],axis=0))[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"her\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_36 to have 3 dimensions, but got array with shape (1, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-381-2a2f943ca969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1770\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1771\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_36 to have 3 dimensions, but got array with shape (1, 300)"
     ]
    }
   ],
   "source": [
    "lr.predict_proba([autoencoder.predict(np.expand_dims(embeddings[p],axis=0))[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9966915, 0.0033085]])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba([gan.G.predict(np.expand_dims(embeddings[p],axis=0))[0,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32957804, 0.67042196]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba([embeddings[p]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "m\n",
      "m\n",
      "0.7\n",
      "m\n",
      "0.8\n",
      "m\n",
      "0.9\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_accuracy(X,l,embeddings,model=autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_accuracy(X,y,embeddings,n = 5,samples=100,model=None):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in np.random.choice(np.shape(X)[0],samples):\n",
    "        count+=1\n",
    "        if count % 10 ==0:\n",
    "            print(count/float(samples))\n",
    "        vec = model.predict(np.expand_dims(X[i],axis=0))[0,:] if model else X[i]\n",
    "        for j in embeddings.similar_by_vector(vec,topn=n):\n",
    "            try:\n",
    "                if l[i] == str(j[0]): \n",
    "                    correct+=1\n",
    "                    break\n",
    "            except:\n",
    "                print(\"m\")\n",
    "    return float(correct)/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protected Groups\n",
    "\n",
    "Race,\n",
    "Color,\n",
    "Religion or creed,\n",
    "National origin or ancestry,\n",
    "Sex,\n",
    "Age,\n",
    "Physical or mental disability,\n",
    "Veteran status,\n",
    "Genetic information,\n",
    "Citizenship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = ['lesbian', 'gay', 'bisexual', 'transgender', 'trans', 'queer', \n",
    "              'lgbt', 'lgbtq', 'homosexual', 'straight', 'heterosexual', 'male', \n",
    "              'female', 'nonbinary', 'african', 'african american', 'black', 'white', \n",
    "              'european', 'hispanic', 'latino', 'latina', 'latinx', 'mexican', 'canadian', \n",
    "              'american', 'asian', 'indian', 'middle eastern', 'chinese', 'japanese', \n",
    "              'christian', 'muslim', 'jewish', 'buddhist', 'catholic', 'protestant', 'sikh', \n",
    "              'taoist', 'old', 'older', 'young', 'younger', 'teenage', 'millenial', 'middle aged', \n",
    "              'elderly', 'blind', 'deaf', 'paralyzed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities=[\n",
    "'German',\n",
    "#'African_American',\n",
    "'Mexican',\n",
    "'Irish',\n",
    "'English',\n",
    "'American',\n",
    "'Italian',\n",
    "'Polish',\n",
    "'French',\n",
    "'Scottish',\n",
    "#'Puerto_Rican',\n",
    "'Norwegian',\n",
    "'Dutch',\n",
    "'Swedish',\n",
    "'Chinese',\n",
    "'Indian',\n",
    "'Russian',\n",
    "'Filipino'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "religions =[ \n",
    "    'Zionist',\n",
    "    'Catholic',\n",
    "    'Christian',\n",
    "    'Islamic',\n",
    "    'Protestant',\n",
    "    'Taoist',\n",
    "    'Atheist',\n",
    "    'Hindu',\n",
    "    'Buddhist',\n",
    "    'Diasporic',\n",
    "    'Sikh',\n",
    "    'Juche',\n",
    "    'Jewish',\n",
    "    'Bahai',\n",
    "    'Jains',\n",
    "    'Shinto',\n",
    "    'Pagan'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender= [\"male\",\"female\",\"her\",\"him\",\"man\",\"woman\",\"boy\",\"girl\",\"his\",\"hers\",\"mom\",\"dad\"]#perhaps try all terms intraprotected group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(embeddings,text):\n",
    "    tokens = text.split()\n",
    "    words = filter(lambda x: x in embeddings,[x.lower() for x in words])\n",
    "    if len(words)>0:\n",
    "        return np.mean(embeddings[words],axis = 0)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_toxicity(model,embeddings,words):\n",
    "    words = filter(lambda x: x in embeddings,[x.lower() for x in words])\n",
    "    if len(words)>0:\n",
    "        vector = np.mean(embeddings[words],axis = 0)\n",
    "        prob = model.predict_prob(vector)\n",
    "        return prob\n",
    "    else: return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_toxicity(model,embeddings,text):\n",
    "    tokens = text.split()\n",
    "    toxicity = words_to_toxicity(model,embeddings,tokens)\n",
    "    return toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_toxicity_table(identities,embeddings,model,add=None):\n",
    "    words = []\n",
    "    toxicities = []\n",
    "    for word in sorted(identities):\n",
    "        word = add + word if add else word\n",
    "        word = word.lower()\n",
    "        toxicities.append(text_to_toxicity(model, embeddings,word.lower()))\n",
    "        words.append(word)\n",
    "    return zip(words,toxicities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    lexicon = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_set(model,targets,sentences,dim=300):\n",
    "    vectors = np.zeros((len(sentences),dim))\n",
    "    labels = np.zeros((len(sentences)))\n",
    "    count=0\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        words = filter(lambda x: x in model,[x.lower() for x in sentence.split()])\n",
    "        if len(words)>0:\n",
    "            vectors[count,:] = np.mean(model[words],axis = 0)\n",
    "            labels[count] = targets[i] \n",
    "            count+=1\n",
    "    print(count, \" sentences in embeddings, \", len(sentences) - count, \" sentences not in embeddings\")\n",
    "    return train_test_split(vectors, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOVE\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = 'data/glove.6B/glove.6B.300d.txt'\n",
    "# word2vec_output_file = 'data/glove.6B/glove.6B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file,binary=False)\n",
    "\n",
    "#Word2vec\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('data/embeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "#universal sentence encoder\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "# # Import the Universal Sentence Encoder's TF Hub module\n",
    "# embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle\n",
    "\n",
    "#df = pd.read_csv(\"data/kaggle_toxicity/train.csv\")\n",
    "\n",
    "#word sentiment\n",
    "\n",
    "pos_words = load_lexicon(\"data/opinion_lexicon/positive-words.txt\")\n",
    "neg_words = load_lexicon(\"data/opinion_lexicon/negative-words.txt\")\n",
    "\n",
    "#imbd\n",
    "\n",
    "# import numpy\n",
    "# from keras.datasets import imdb\n",
    "# from matplotlib import pyplot\n",
    "# # load the dataset\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "# X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "# y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize word sentiment dataset or toxicity dataset (wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9492868462757528"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment word dataset from positve/negative reviews\n",
    "#'''\n",
    "pos_words_fil = filter(lambda x:  x in embeddings,map(lambda x: x,pos_words))\n",
    "neg_words_fil = filter(lambda x:  x in embeddings,map(lambda x: x,neg_words))\n",
    "pos_vectors = embeddings[pos_words_fil]\n",
    "neg_vectors = embeddings[neg_words_fil]\n",
    "vectors = np.concatenate([pos_vectors, neg_vectors])\n",
    "targets = np.array([0 for entry in pos_vectors] + [1 for entry in neg_vectors])\n",
    "labels = list(pos_vectors) + list(neg_vectors)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_vectors,train_targets)\n",
    "lr.score(test_vectors,test_targets)\n",
    "#'''\n",
    "#toxicity tweet dataset\n",
    "#''''''\n",
    "# toxic_sentences = df[df[\"toxic\"]==1][\"comment_text\"]\n",
    "# num_toxic_samples = len(toxic_sentences)\n",
    "# non_toxic_sentences = df[df[\"toxic\"]==0][\"comment_text\"].sample(num_toxic_samples)\n",
    "# sentences = pd.concat([toxic_sentences, non_toxic_sentences])\n",
    "# targets  = np.zeros(num_toxic_samples*2)\n",
    "# targets[0:num_toxic_samples] = 1\n",
    "# train_vectors, test_vectors, train_targets, test_targets = generate_train_test_set(embeddings,targets,sentences)\n",
    "# sentences = map(lambda x: re.sub('[^A-Za-z0-9 ]+', '', x.lower()),list(sentences))\n",
    "# # message_embeddings = None\n",
    "# # with tf.Session() as session:\n",
    "# #     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "# #     message_embeddings = session.run(embed(sentences))\n",
    "# #     print np.shape(message_embeddings)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vec = TfidfVectorizer(ngram_range=(1,2),\n",
    "#                min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "#                smooth_idf=1, sublinear_tf=1)\n",
    "# trn_term_doc = vec.fit_transform(sentences)\n",
    "\n",
    "# train_vectors, test_vectors, train_targets, test_targets = train_test_split(trn_term_doc.todense(), targets,test_size=0.1, random_state=0)\n",
    "\n",
    "#'''\n",
    "\n",
    "\n",
    "    \n",
    "# import re, string\n",
    "# re_tok = re.compile(f'([{string.punctuation}])')\n",
    "# def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "# n = train.shape[0]\n",
    "# vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "#                min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "#                smooth_idf=1, sublinear_tf=1 )\n",
    "# trn_term_doc = vec.fit_transform(train[COMMENT])\n",
    "# test_term_doc = vec.transform(test[COMMENT])\n",
    "#imbd movie ratings?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5670, 300)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, kernel_initializer=\"uniform\", input_shape=(300,))`\n",
      "  \n",
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5670 samples, validate on 631 samples\n",
      "Epoch 1/10\n",
      "5670/5670 [==============================] - 0s 82us/step - loss: 0.2265 - acc: 0.9028 - val_loss: 0.1315 - val_acc: 0.9493\n",
      "Epoch 2/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.1079 - acc: 0.9621 - val_loss: 0.1183 - val_acc: 0.9509\n",
      "Epoch 3/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.0755 - acc: 0.9730 - val_loss: 0.1198 - val_acc: 0.9509\n",
      "Epoch 4/10\n",
      "5670/5670 [==============================] - 0s 25us/step - loss: 0.0547 - acc: 0.9802 - val_loss: 0.1699 - val_acc: 0.9445\n",
      "Epoch 5/10\n",
      "5670/5670 [==============================] - 0s 25us/step - loss: 0.0487 - acc: 0.9817 - val_loss: 0.1582 - val_acc: 0.9556\n",
      "Epoch 6/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.0248 - acc: 0.9919 - val_loss: 0.1848 - val_acc: 0.9540\n",
      "Epoch 7/10\n",
      "5670/5670 [==============================] - 0s 28us/step - loss: 0.0139 - acc: 0.9951 - val_loss: 0.2288 - val_acc: 0.9588\n",
      "Epoch 8/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.2769 - val_acc: 0.9477\n",
      "Epoch 9/10\n",
      "5670/5670 [==============================] - 0s 27us/step - loss: 0.0184 - acc: 0.9944 - val_loss: 0.1729 - val_acc: 0.9588\n",
      "Epoch 10/10\n",
      "5670/5670 [==============================] - 0s 26us/step - loss: 0.0129 - acc: 0.9952 - val_loss: 0.2239 - val_acc: 0.9493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa928993e90>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adv = Sequential()\n",
    "model_adv.add(Dense(300, init='uniform',input_shape=(300,)))\n",
    "model_adv.add(Activation('relu'))\n",
    "model_adv.add(Dense(100, init='uniform'))\n",
    "model_adv.add(Activation('relu'))\n",
    "model_adv.add(Dense(1, init='uniform'))\n",
    "model_adv.add(Activation('sigmoid'))\n",
    "\n",
    "model_adv.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.02),metrics=['accuracy'])\n",
    "model_adv.fit(train_vectors, train_targets,epochs=10,batch_size=128,validation_data=(test_vectors,test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-3ceb33da1361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_adv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "model_adv.evaluate(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _np_normalize(v):\n",
    "  \"\"\"Returns the input vector, normalized.\"\"\"\n",
    "  return v / np.linalg.norm(v)\n",
    "words = set()\n",
    "for a in analogies:\n",
    "  words.update(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 100 artists>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "good = []\n",
    "bad=  []\n",
    "\n",
    "for word in pos_words:\n",
    "    a = wordnet.synsets(word)\n",
    "    if len(a)>0:\n",
    "        syn = a[0] .lemmas()[0]\n",
    "        if len(syn.antonyms())>0:\n",
    "            good.append(word)\n",
    "            bad.append(syn.antonyms()[0].name()) \n",
    "good_vectors=[]\n",
    "bad_vectors = []\n",
    "for i,j in zip(good,bad):\n",
    "    if i in embeddings and j in embeddings:\n",
    "        good_vectors.append(_np_normalize(embeddings[i]))\n",
    "        bad_vectors.append(_np_normalize(embeddings[j]))\n",
    "\n",
    "good_vectors = np.array(good_vectors)\n",
    "bad_vectors = np.array(bad_vectors)\n",
    "X = good_vectors-bad_vectors\n",
    "m = np.cov(X.T)\n",
    "evals, evecs = np.linalg.eig(m)\n",
    "vec = _np_normalize(np.real(evecs[:, np.argmax(evals)]))\n",
    "plt.bar(np.arange(100),evals[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lesbian [(u'gay', 0.8117855787277222), (u'lesbians', 0.7656285762786865)]\n",
      "gay [(u'homosexual', 0.8145633339881897), (u'lesbian', 0.8117855787277222)]\n",
      "bisexual [(u'heterosexual', 0.6771868467330933), (u'bisexuals', 0.6483285427093506)]\n",
      "transgender [(u'transgendered', 0.8865242600440979), (u'transgendered_GLBT', 0.7773934602737427)]\n",
      "trans [(u'sodium_TSC', 0.6333322525024414), (u'transgender_transsexual', 0.49470755457878113)]\n",
      "queer [(u'genderqueer', 0.7026611566543579), (u'LGBTQ', 0.6990917325019836)]\n",
      "lgbt [(u'LGBT', 0.6653385162353516), (u'LGBTQ', 0.6460728049278259)]\n",
      "homosexual [(u'gay', 0.8145633339881897), (u'homosexuals', 0.7716525793075562)]\n",
      "straight [(u'consecutive', 0.7644870281219482), (u'staight', 0.5966242551803589)]\n",
      "heterosexual [(u'heterosexuals', 0.7658922672271729), (u'homosexual', 0.710830569267273)]\n",
      "male [(u'female', 0.8405333161354065), (u'males', 0.7579617500305176)]\n",
      "female [(u'male', 0.8405333757400513), (u'Female', 0.7159764766693115)]\n",
      "african [(u'africa', 0.680280327796936), (u'ghana', 0.6450865864753723)]\n",
      "black [(u'white', 0.8092213869094849), (u'Responded_Letterman_How', 0.6182776689529419)]\n",
      "white [(u'black', 0.8092213869094849), (u'blue', 0.6606028079986572)]\n",
      "european [(u'europe', 0.7335599064826965), (u'germany', 0.6502110958099365)]\n",
      "hispanic [(u'latino', 0.7220249176025391), (u'Hispanic', 0.71739262342453)]\n",
      "latino [(u'hispanic', 0.7220250368118286), (u'Latino', 0.6628822088241577)]\n",
      "latina [(u'sucking_dick', 0.6476465463638306), (u'latinas', 0.6459453701972961)]\n",
      "mexican [(u'mexicans', 0.6493428945541382), (u'mexico', 0.6476786136627197)]\n",
      "canadian [(u'canada', 0.6989885568618774), (u'alberta', 0.645421028137207)]\n",
      "american [(u'america', 0.7169357538223267), (u'americans', 0.6877550482749939)]\n",
      "asian [(u'chinese', 0.6251923441886902), (u'african', 0.6127210855484009)]\n",
      "indian [(u'india', 0.6967039704322815), (u'american', 0.6599416136741638)]\n",
      "chinese [(u'japanese', 0.6502295732498169), (u'asian', 0.6251923441886902)]\n",
      "japanese [(u'japan', 0.6607722043991089), (u'chinese', 0.6502295732498169)]\n",
      "christian [(u'christians', 0.7363371849060059), (u'christianity', 0.6904211044311523)]\n",
      "muslim [(u'muslims', 0.7917742729187012), (u'Muslim', 0.7285441756248474)]\n",
      "jewish [(u'jews', 0.6818791031837463), (u'muslim', 0.6525880098342896)]\n",
      "buddhist [(u'buddhism', 0.6611950993537903), (u'Buddhism', 0.6371399164199829)]\n",
      "catholic [(u'Catholic', 0.7182316780090332), (u'Roman_Catholic', 0.6908203363418579)]\n",
      "protestant [(u'Protestant', 0.7294514179229736), (u'protestant_churches', 0.5800493955612183)]\n",
      "sikh [(u'sikhs', 0.6773761510848999), (u'gandhi', 0.6656152009963989)]\n",
      "old [(u'yearold', 0.7316408753395081), (u'boy', 0.5828141570091248)]\n",
      "older [(u'younger', 0.8020766377449036), (u'Older', 0.6751844882965088)]\n",
      "young [(u'By_Yoon_Ja', 0.678579568862915), (u'By_Soh_Ji', 0.6605980396270752)]\n",
      "younger [(u'older', 0.8020766377449036), (u'Elena_Losina_co', 0.6406033039093018)]\n",
      "teenage [(u'teenaged', 0.81182861328125), (u'teen', 0.6743793487548828)]\n",
      "millenial [(u'millennial', 0.5928958654403687), (u'millennialist', 0.5331144332885742)]\n",
      "elderly [(u'Elderly', 0.736482560634613), (u'eldery', 0.6419861316680908)]\n",
      "blind [(u'sightless', 0.5840786695480347), (u'blinded', 0.5577540397644043)]\n",
      "deaf [(u'deaf_blind', 0.7083539962768555), (u'Deaf', 0.6863464713096619)]\n",
      "paralyzed [(u'partially_paralyzed', 0.650324821472168), (u'paralyzing', 0.6440768241882324)]\n"
     ]
    }
   ],
   "source": [
    "for i in filter(lambda x: x.lower() in embeddings,identities):\n",
    "    print i, embeddings.most_similar(i)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 100 artists>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "good = ['good','beautiful','happy','positive','compentent','smart','interesting','pleasant','best','honest','wealthy','wonderful']\n",
    "bad = ['bad','ugly','sad','negative','incompetent','dumb','boring','horrible','worst','corrupt','poor','awful']\n",
    "good_vectors=[]\n",
    "bad_vectors = []\n",
    "for i,j in zip(good,bad):\n",
    "    if i in embeddings and j in embeddings:\n",
    "        good_vectors.append(_np_normalize(embeddings[i]))\n",
    "        bad_vectors.append(_np_normalize(embeddings[j]))\n",
    "\n",
    "good_vectors = np.array(good_vectors)\n",
    "bad_vectors = np.array(bad_vectors)\n",
    "X = good_vectors-bad_vectors\n",
    "m = np.cov(X.T)\n",
    "evals, evecs = np.linalg.eig(m)\n",
    "vec = _np_normalize(np.real(evecs[:, np.argmax(evals)]))\n",
    "plt.bar(np.arange(100),evals[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  sentiment_score\n",
      "1       ugly        -1.098870\n",
      "0  beautiful         0.824158\n"
     ]
    }
   ],
   "source": [
    "words = [\"beautiful\",\"ugly\"]#filter(lambda x: x.lower() in embeddings,identities)\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda w: embeddings[w.lower()].dot(vec))\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1893, 300), (4344, 300))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pos_vectors),np.shape(neg_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fair regression terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((len(nationalities),300))\n",
    "for i,j in enumerate(nationalities):\n",
    "    N[i,:] = embeddings[j.lower()]\n",
    "R = np.zeros((len(religions),300))\n",
    "for i,j in enumerate(religions):\n",
    "    R[i,:] = embeddings[j.lower()]\n",
    "G = np.zeros((len(gender),300))\n",
    "for i,j in enumerate(gender):\n",
    "    G[i,:] = embeddings[j.lower()]\n",
    "regularizers = [N,R,G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
