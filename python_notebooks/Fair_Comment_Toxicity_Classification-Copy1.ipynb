{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from model_tool import ToxModel\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import model_tool\n",
    "import model_bias_analysis\n",
    "\n",
    "# autoreload makes it easier to interactively work on code in the model_bias_analysis module.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook shows how one can rebalance a dataset and use debiased word embeddings to create fairer classifiers for Toxicity classification. We show that using debiased word embeddings can improve fairness via metrics proposed in http://www.aies-conference.com/wp-content/papers/main/AIES_2018_paper_9.pdf. This Notebook contains code from https://github.com/conversationai/unintended-ml-bias-analysis. And is split up into 3 parts\n",
    "\n",
    "- Dataset Loading and Model Training\n",
    "- Evaluate Model Fairness\n",
    "- Visulize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/toxicity/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/toxicity/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/toxicity/wiki_debias_random_%s.csv' % split\n",
    "    \n",
    "hparams_100 = {\n",
    "    'max_sequence_length': 250,\n",
    "    'max_num_words': 10000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'stop_early': True,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 4,\n",
    "    'dropout_rate': 0.3,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5, 5, 5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(name,data,word_embeddings_path,params=[]):\n",
    "    count = 100\n",
    "    for i in params:\n",
    "        model_version = name+\"_\"+str(count)\n",
    "        count+=1\n",
    "        model = ToxModel(hparams=i,embeddings_path = word_embeddings_path)\n",
    "        print(\"Training {model_version}\")\n",
    "        model.train(data['train'], data['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = model_version)\n",
    "        print(\"Testing Model\")\n",
    "        test = pd.read_csv(data['test'])\n",
    "        print(model.score_auc(test['comment'], test['is_toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17061, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 9s - loss: 0.2315 - acc: 0.9193 - val_loss: 0.1706 - val_acc: 0.9385\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17061 to 0.14678, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1619 - acc: 0.9411 - val_loss: 0.1468 - val_acc: 0.9462\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14678 to 0.13713, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1432 - acc: 0.9470 - val_loss: 0.1371 - val_acc: 0.9485\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13713 to 0.12961, saving model to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1316 - acc: 0.9515 - val_loss: 0.1296 - val_acc: 0.9520\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9556638376113306\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16575, saving model to ../models/wiki_debias_random_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.2382 - acc: 0.9171 - val_loss: 0.1658 - val_acc: 0.9396\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16575 to 0.14642, saving model to ../models/wiki_debias_random_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1619 - acc: 0.9414 - val_loss: 0.1464 - val_acc: 0.9462\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14642 to 0.13637, saving model to ../models/wiki_debias_random_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1439 - acc: 0.9471 - val_loss: 0.1364 - val_acc: 0.9487\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13637 to 0.12914, saving model to ../models/wiki_debias_random_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1331 - acc: 0.9517 - val_loss: 0.1291 - val_acc: 0.9523\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9550077790921715\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17118, saving model to ../models/wiki_debias_random_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.2389 - acc: 0.9175 - val_loss: 0.1712 - val_acc: 0.9387\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17118 to 0.14801, saving model to ../models/wiki_debias_random_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1653 - acc: 0.9393 - val_loss: 0.1480 - val_acc: 0.9461\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14801 to 0.13646, saving model to ../models/wiki_debias_random_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1451 - acc: 0.9465 - val_loss: 0.1365 - val_acc: 0.9500\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13646 to 0.13008, saving model to ../models/wiki_debias_random_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1340 - acc: 0.9511 - val_loss: 0.1301 - val_acc: 0.9520\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9549350758639906\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16564, saving model to ../models/wiki_debias_random_cnn_v3_103_model.h5\n",
      " - 9s - loss: 0.2241 - acc: 0.9222 - val_loss: 0.1656 - val_acc: 0.9400\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16564 to 0.14569, saving model to ../models/wiki_debias_random_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1595 - acc: 0.9417 - val_loss: 0.1457 - val_acc: 0.9462\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14569 to 0.13602, saving model to ../models/wiki_debias_random_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1421 - acc: 0.9474 - val_loss: 0.1360 - val_acc: 0.9493\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13602 to 0.13348, saving model to ../models/wiki_debias_random_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1321 - acc: 0.9509 - val_loss: 0.1335 - val_acc: 0.9492\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.954497050407484\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17074, saving model to ../models/wiki_debias_random_cnn_v3_104_model.h5\n",
      " - 9s - loss: 0.2371 - acc: 0.9166 - val_loss: 0.1707 - val_acc: 0.9386\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17074 to 0.14543, saving model to ../models/wiki_debias_random_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1614 - acc: 0.9411 - val_loss: 0.1454 - val_acc: 0.9461\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14543 to 0.14428, saving model to ../models/wiki_debias_random_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1433 - acc: 0.9475 - val_loss: 0.1443 - val_acc: 0.9484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14428 to 0.12928, saving model to ../models/wiki_debias_random_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1321 - acc: 0.9516 - val_loss: 0.1293 - val_acc: 0.9516\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9551762651565763\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16779, saving model to ../models/wiki_debias_random_cnn_v3_105_model.h5\n",
      " - 9s - loss: 0.2347 - acc: 0.9191 - val_loss: 0.1678 - val_acc: 0.9395\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16779 to 0.14823, saving model to ../models/wiki_debias_random_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1611 - acc: 0.9415 - val_loss: 0.1482 - val_acc: 0.9444\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14823 to 0.13738, saving model to ../models/wiki_debias_random_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1432 - acc: 0.9471 - val_loss: 0.1374 - val_acc: 0.9482\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13738 to 0.13026, saving model to ../models/wiki_debias_random_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1318 - acc: 0.9514 - val_loss: 0.1303 - val_acc: 0.9515\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9553727127380521\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17151, saving model to ../models/wiki_debias_random_cnn_v3_106_model.h5\n",
      " - 10s - loss: 0.2339 - acc: 0.9182 - val_loss: 0.1715 - val_acc: 0.9379\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17151 to 0.15513, saving model to ../models/wiki_debias_random_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1635 - acc: 0.9407 - val_loss: 0.1551 - val_acc: 0.9447\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15513 to 0.14033, saving model to ../models/wiki_debias_random_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1448 - acc: 0.9471 - val_loss: 0.1403 - val_acc: 0.9480\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14033 to 0.13241, saving model to ../models/wiki_debias_random_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1329 - acc: 0.9508 - val_loss: 0.1324 - val_acc: 0.9499\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9556660213352121\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17097, saving model to ../models/wiki_debias_random_cnn_v3_107_model.h5\n",
      " - 10s - loss: 0.2476 - acc: 0.9090 - val_loss: 0.1710 - val_acc: 0.9376\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17097 to 0.15101, saving model to ../models/wiki_debias_random_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1624 - acc: 0.9413 - val_loss: 0.1510 - val_acc: 0.9426\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15101 to 0.13971, saving model to ../models/wiki_debias_random_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1439 - acc: 0.9470 - val_loss: 0.1397 - val_acc: 0.9472\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13971 to 0.12940, saving model to ../models/wiki_debias_random_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1324 - acc: 0.9519 - val_loss: 0.1294 - val_acc: 0.9517\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9552667391903805\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17420, saving model to ../models/wiki_debias_random_cnn_v3_108_model.h5\n",
      " - 10s - loss: 0.2462 - acc: 0.9140 - val_loss: 0.1742 - val_acc: 0.9362\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17420 to 0.14976, saving model to ../models/wiki_debias_random_cnn_v3_108_model.h5\n",
      " - 8s - loss: 0.1650 - acc: 0.9402 - val_loss: 0.1498 - val_acc: 0.9454\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14976 to 0.14470, saving model to ../models/wiki_debias_random_cnn_v3_108_model.h5\n",
      " - 8s - loss: 0.1451 - acc: 0.9468 - val_loss: 0.1447 - val_acc: 0.9456\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14470 to 0.13267, saving model to ../models/wiki_debias_random_cnn_v3_108_model.h5\n",
      " - 8s - loss: 0.1327 - acc: 0.9518 - val_loss: 0.1327 - val_acc: 0.9504\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9558444463534241\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17052, saving model to ../models/wiki_debias_random_cnn_v3_109_model.h5\n",
      " - 11s - loss: 0.2323 - acc: 0.9197 - val_loss: 0.1705 - val_acc: 0.9380\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17052 to 0.14849, saving model to ../models/wiki_debias_random_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1637 - acc: 0.9402 - val_loss: 0.1485 - val_acc: 0.9441\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14849 to 0.14164, saving model to ../models/wiki_debias_random_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1447 - acc: 0.9467 - val_loss: 0.1416 - val_acc: 0.9485\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14164 to 0.13233, saving model to ../models/wiki_debias_random_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1324 - acc: 0.9513 - val_loss: 0.1323 - val_acc: 0.9496\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_random_cnn_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9546271324857288\n"
     ]
    }
   ],
   "source": [
    "train_models('wiki_debias_random_cnn_v3',random,'../data/embeddings/glove.6B/glove.6B.100d.txt',[hparams_100]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17539, saving model to ../models/wiki_cnn_v3_100_model.h5\n",
      " - 11s - loss: 0.2432 - acc: 0.9150 - val_loss: 0.1754 - val_acc: 0.9363\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17539 to 0.15374, saving model to ../models/wiki_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1687 - acc: 0.9386 - val_loss: 0.1537 - val_acc: 0.9420\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15374 to 0.15233, saving model to ../models/wiki_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1500 - acc: 0.9450 - val_loss: 0.1523 - val_acc: 0.9464\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.15233 to 0.13943, saving model to ../models/wiki_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1376 - acc: 0.9488 - val_loss: 0.1394 - val_acc: 0.9502\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9513875682648768\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17384, saving model to ../models/wiki_cnn_v3_101_model.h5\n",
      " - 12s - loss: 0.2387 - acc: 0.9157 - val_loss: 0.1738 - val_acc: 0.9373\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17384 to 0.16080, saving model to ../models/wiki_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1638 - acc: 0.9403 - val_loss: 0.1608 - val_acc: 0.9402\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.16080 to 0.15557, saving model to ../models/wiki_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1456 - acc: 0.9460 - val_loss: 0.1556 - val_acc: 0.9467\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.15557 to 0.13103, saving model to ../models/wiki_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1342 - acc: 0.9504 - val_loss: 0.1310 - val_acc: 0.9508\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9558438375311871\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17616, saving model to ../models/wiki_cnn_v3_102_model.h5\n",
      " - 12s - loss: 0.2404 - acc: 0.9164 - val_loss: 0.1762 - val_acc: 0.9362\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17616 to 0.15401, saving model to ../models/wiki_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1665 - acc: 0.9393 - val_loss: 0.1540 - val_acc: 0.9433\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15401 to 0.14273, saving model to ../models/wiki_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1475 - acc: 0.9456 - val_loss: 0.1427 - val_acc: 0.9475\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14273 to 0.13376, saving model to ../models/wiki_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1355 - acc: 0.9502 - val_loss: 0.1338 - val_acc: 0.9506\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9533957706150775\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17188, saving model to ../models/wiki_cnn_v3_103_model.h5\n",
      " - 12s - loss: 0.2425 - acc: 0.9152 - val_loss: 0.1719 - val_acc: 0.9369\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17188 to 0.15394, saving model to ../models/wiki_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1659 - acc: 0.9390 - val_loss: 0.1539 - val_acc: 0.9445\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15394 to 0.14219, saving model to ../models/wiki_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1477 - acc: 0.9458 - val_loss: 0.1422 - val_acc: 0.9457\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14219 to 0.14077, saving model to ../models/wiki_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1355 - acc: 0.9501 - val_loss: 0.1408 - val_acc: 0.9492\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9532247839595732\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18355, saving model to ../models/wiki_cnn_v3_104_model.h5\n",
      " - 13s - loss: 0.2472 - acc: 0.9134 - val_loss: 0.1836 - val_acc: 0.9346\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18355 to 0.15117, saving model to ../models/wiki_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1680 - acc: 0.9380 - val_loss: 0.1512 - val_acc: 0.9434\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15117 to 0.13966, saving model to ../models/wiki_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1491 - acc: 0.9447 - val_loss: 0.1397 - val_acc: 0.9473\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13966 to 0.13430, saving model to ../models/wiki_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1374 - acc: 0.9491 - val_loss: 0.1343 - val_acc: 0.9493\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9535619073927438\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18446, saving model to ../models/wiki_cnn_v3_105_model.h5\n",
      " - 13s - loss: 0.2484 - acc: 0.9144 - val_loss: 0.1845 - val_acc: 0.9338\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18446 to 0.15119, saving model to ../models/wiki_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1692 - acc: 0.9380 - val_loss: 0.1512 - val_acc: 0.9447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15119 to 0.14127, saving model to ../models/wiki_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1490 - acc: 0.9454 - val_loss: 0.1413 - val_acc: 0.9475\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14127 to 0.13298, saving model to ../models/wiki_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1367 - acc: 0.9491 - val_loss: 0.1330 - val_acc: 0.9505\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9546075797625027\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18016, saving model to ../models/wiki_cnn_v3_106_model.h5\n",
      " - 14s - loss: 0.2494 - acc: 0.9117 - val_loss: 0.1802 - val_acc: 0.9344\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18016 to 0.15644, saving model to ../models/wiki_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1681 - acc: 0.9381 - val_loss: 0.1564 - val_acc: 0.9435\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15644 to 0.14084, saving model to ../models/wiki_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1496 - acc: 0.9449 - val_loss: 0.1408 - val_acc: 0.9475\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 8s - loss: 0.1369 - acc: 0.9498 - val_loss: 0.1409 - val_acc: 0.9461\n",
      "Epoch 00004: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9497202975265715\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17829, saving model to ../models/wiki_cnn_v3_107_model.h5\n",
      " - 14s - loss: 0.2483 - acc: 0.9123 - val_loss: 0.1783 - val_acc: 0.9345\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17829 to 0.15809, saving model to ../models/wiki_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1709 - acc: 0.9377 - val_loss: 0.1581 - val_acc: 0.9421\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15809 to 0.14453, saving model to ../models/wiki_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1511 - acc: 0.9445 - val_loss: 0.1445 - val_acc: 0.9467\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14453 to 0.13820, saving model to ../models/wiki_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1397 - acc: 0.9485 - val_loss: 0.1382 - val_acc: 0.9495\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9526563407075157\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17758, saving model to ../models/wiki_cnn_v3_108_model.h5\n",
      " - 15s - loss: 0.2416 - acc: 0.9156 - val_loss: 0.1776 - val_acc: 0.9359\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17758 to 0.15191, saving model to ../models/wiki_cnn_v3_108_model.h5\n",
      " - 8s - loss: 0.1677 - acc: 0.9377 - val_loss: 0.1519 - val_acc: 0.9435\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 8s - loss: 0.1494 - acc: 0.9442 - val_loss: 0.1569 - val_acc: 0.9460\n",
      "Epoch 00003: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9400085879268735\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17435, saving model to ../models/wiki_cnn_v3_109_model.h5\n",
      " - 15s - loss: 0.2374 - acc: 0.9168 - val_loss: 0.1744 - val_acc: 0.9357\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17435 to 0.15782, saving model to ../models/wiki_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1669 - acc: 0.9381 - val_loss: 0.1578 - val_acc: 0.9432\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15782 to 0.15058, saving model to ../models/wiki_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1469 - acc: 0.9458 - val_loss: 0.1506 - val_acc: 0.9465\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.15058 to 0.13223, saving model to ../models/wiki_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1348 - acc: 0.9506 - val_loss: 0.1322 - val_acc: 0.9501\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_cnn_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9556150052670129\n"
     ]
    }
   ],
   "source": [
    "train_models('wiki_cnn_v3',wiki,'../data/embeddings/glove.6B/glove.6B.100d.txt',[hparams_100]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17303, saving model to ../models/wiki_debias_cnn_v3_100_model.h5\n",
      " - 16s - loss: 0.2444 - acc: 0.9142 - val_loss: 0.1730 - val_acc: 0.9374\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17303 to 0.14716, saving model to ../models/wiki_debias_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1653 - acc: 0.9399 - val_loss: 0.1472 - val_acc: 0.9460\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14716 to 0.13858, saving model to ../models/wiki_debias_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1452 - acc: 0.9472 - val_loss: 0.1386 - val_acc: 0.9492\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13858 to 0.13062, saving model to ../models/wiki_debias_cnn_v3_100_model.h5\n",
      " - 8s - loss: 0.1334 - acc: 0.9511 - val_loss: 0.1306 - val_acc: 0.9513\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9535183754180162\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17274, saving model to ../models/wiki_debias_cnn_v3_101_model.h5\n",
      " - 16s - loss: 0.2542 - acc: 0.9096 - val_loss: 0.1727 - val_acc: 0.9380\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17274 to 0.15348, saving model to ../models/wiki_debias_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1650 - acc: 0.9406 - val_loss: 0.1535 - val_acc: 0.9460\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15348 to 0.14436, saving model to ../models/wiki_debias_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1452 - acc: 0.9470 - val_loss: 0.1444 - val_acc: 0.9461\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14436 to 0.13089, saving model to ../models/wiki_debias_cnn_v3_101_model.h5\n",
      " - 8s - loss: 0.1342 - acc: 0.9511 - val_loss: 0.1309 - val_acc: 0.9518\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9546087870159259\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18734, saving model to ../models/wiki_debias_cnn_v3_102_model.h5\n",
      " - 17s - loss: 0.2387 - acc: 0.9178 - val_loss: 0.1873 - val_acc: 0.9338\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18734 to 0.16381, saving model to ../models/wiki_debias_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1659 - acc: 0.9396 - val_loss: 0.1638 - val_acc: 0.9434\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.16381 to 0.13994, saving model to ../models/wiki_debias_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1470 - acc: 0.9459 - val_loss: 0.1399 - val_acc: 0.9492\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13994 to 0.13176, saving model to ../models/wiki_debias_cnn_v3_102_model.h5\n",
      " - 8s - loss: 0.1352 - acc: 0.9507 - val_loss: 0.1318 - val_acc: 0.9515\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.952936672970678\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17469, saving model to ../models/wiki_debias_cnn_v3_103_model.h5\n",
      " - 18s - loss: 0.2418 - acc: 0.9168 - val_loss: 0.1747 - val_acc: 0.9372\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17469 to 0.15135, saving model to ../models/wiki_debias_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1651 - acc: 0.9401 - val_loss: 0.1513 - val_acc: 0.9450\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15135 to 0.14610, saving model to ../models/wiki_debias_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1467 - acc: 0.9464 - val_loss: 0.1461 - val_acc: 0.9482\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14610 to 0.14426, saving model to ../models/wiki_debias_cnn_v3_103_model.h5\n",
      " - 8s - loss: 0.1346 - acc: 0.9505 - val_loss: 0.1443 - val_acc: 0.9500\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.950984817875896\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17050, saving model to ../models/wiki_debias_cnn_v3_104_model.h5\n",
      " - 18s - loss: 0.2309 - acc: 0.9196 - val_loss: 0.1705 - val_acc: 0.9385\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17050 to 0.14913, saving model to ../models/wiki_debias_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1627 - acc: 0.9409 - val_loss: 0.1491 - val_acc: 0.9460\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14913 to 0.14213, saving model to ../models/wiki_debias_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1443 - acc: 0.9469 - val_loss: 0.1421 - val_acc: 0.9491\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14213 to 0.13860, saving model to ../models/wiki_debias_cnn_v3_104_model.h5\n",
      " - 8s - loss: 0.1326 - acc: 0.9511 - val_loss: 0.1386 - val_acc: 0.9510\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9541530071727723\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16977, saving model to ../models/wiki_debias_cnn_v3_105_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 19s - loss: 0.2382 - acc: 0.9184 - val_loss: 0.1698 - val_acc: 0.9381\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16977 to 0.14791, saving model to ../models/wiki_debias_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1635 - acc: 0.9411 - val_loss: 0.1479 - val_acc: 0.9453\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14791 to 0.13681, saving model to ../models/wiki_debias_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1453 - acc: 0.9476 - val_loss: 0.1368 - val_acc: 0.9490\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13681 to 0.13249, saving model to ../models/wiki_debias_cnn_v3_105_model.h5\n",
      " - 8s - loss: 0.1341 - acc: 0.9510 - val_loss: 0.1325 - val_acc: 0.9515\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9538015699256188\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17395, saving model to ../models/wiki_debias_cnn_v3_106_model.h5\n",
      " - 19s - loss: 0.2323 - acc: 0.9190 - val_loss: 0.1740 - val_acc: 0.9373\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17395 to 0.14998, saving model to ../models/wiki_debias_cnn_v3_106_model.h5\n",
      " - 8s - loss: 0.1665 - acc: 0.9401 - val_loss: 0.1500 - val_acc: 0.9451\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 8s - loss: 0.1475 - acc: 0.9455 - val_loss: 0.1506 - val_acc: 0.9482\n",
      "Epoch 00003: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.938737339546099\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16946, saving model to ../models/wiki_debias_cnn_v3_107_model.h5\n",
      " - 20s - loss: 0.2349 - acc: 0.9186 - val_loss: 0.1695 - val_acc: 0.9387\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16946 to 0.14831, saving model to ../models/wiki_debias_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1630 - acc: 0.9404 - val_loss: 0.1483 - val_acc: 0.9439\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14831 to 0.13612, saving model to ../models/wiki_debias_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1446 - acc: 0.9467 - val_loss: 0.1361 - val_acc: 0.9499\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13612 to 0.13236, saving model to ../models/wiki_debias_cnn_v3_107_model.h5\n",
      " - 8s - loss: 0.1331 - acc: 0.9510 - val_loss: 0.1324 - val_acc: 0.9521\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.953162253289553\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16877, saving model to ../models/wiki_debias_cnn_v3_108_model.h5\n",
      " - 21s - loss: 0.2441 - acc: 0.9141 - val_loss: 0.1688 - val_acc: 0.9389\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16877 to 0.14674, saving model to ../models/wiki_debias_cnn_v3_108_model.h5\n",
      " - 8s - loss: 0.1639 - acc: 0.9406 - val_loss: 0.1467 - val_acc: 0.9457\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 8s - loss: 0.1452 - acc: 0.9473 - val_loss: 0.1540 - val_acc: 0.9471\n",
      "Epoch 00003: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.941000077015946\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17135, saving model to ../models/wiki_debias_cnn_v3_109_model.h5\n",
      " - 21s - loss: 0.2345 - acc: 0.9195 - val_loss: 0.1714 - val_acc: 0.9380\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17135 to 0.14965, saving model to ../models/wiki_debias_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1646 - acc: 0.9400 - val_loss: 0.1496 - val_acc: 0.9447\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14965 to 0.13894, saving model to ../models/wiki_debias_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1466 - acc: 0.9463 - val_loss: 0.1389 - val_acc: 0.9483\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13894 to 0.13548, saving model to ../models/wiki_debias_cnn_v3_109_model.h5\n",
      " - 8s - loss: 0.1346 - acc: 0.9503 - val_loss: 0.1355 - val_acc: 0.9512\n",
      "Model trained!\n",
      "Best model saved to ../models/wiki_debias_cnn_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9515739581196128\n"
     ]
    }
   ],
   "source": [
    "train_models('wiki_debias_cnn_v3',debias,'../data/embeddings/glove.6B/glove.6B.100d.txt',[hparams_100]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16905, saving model to ../models/cnn_debias_tox_v3_debiased_WE_100_model.h5\n",
      " - 22s - loss: 0.2260 - acc: 0.9208 - val_loss: 0.1690 - val_acc: 0.9392\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16905 to 0.14736, saving model to ../models/cnn_debias_tox_v3_debiased_WE_100_model.h5\n",
      " - 8s - loss: 0.1614 - acc: 0.9416 - val_loss: 0.1474 - val_acc: 0.9462\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14736 to 0.13789, saving model to ../models/cnn_debias_tox_v3_debiased_WE_100_model.h5\n",
      " - 8s - loss: 0.1439 - acc: 0.9472 - val_loss: 0.1379 - val_acc: 0.9486\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13789 to 0.13276, saving model to ../models/cnn_debias_tox_v3_debiased_WE_100_model.h5\n",
      " - 8s - loss: 0.1337 - acc: 0.9509 - val_loss: 0.1328 - val_acc: 0.9517\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9537518751565823\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16788, saving model to ../models/cnn_debias_tox_v3_debiased_WE_101_model.h5\n",
      " - 22s - loss: 0.2402 - acc: 0.9174 - val_loss: 0.1679 - val_acc: 0.9396\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16788 to 0.14769, saving model to ../models/cnn_debias_tox_v3_debiased_WE_101_model.h5\n",
      " - 8s - loss: 0.1626 - acc: 0.9413 - val_loss: 0.1477 - val_acc: 0.9460\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 8s - loss: 0.1453 - acc: 0.9472 - val_loss: 0.1511 - val_acc: 0.9425\n",
      "Epoch 00003: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9399143229343856\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17183, saving model to ../models/cnn_debias_tox_v3_debiased_WE_102_model.h5\n",
      " - 23s - loss: 0.2292 - acc: 0.9202 - val_loss: 0.1718 - val_acc: 0.9390\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17183 to 0.14548, saving model to ../models/cnn_debias_tox_v3_debiased_WE_102_model.h5\n",
      " - 8s - loss: 0.1609 - acc: 0.9414 - val_loss: 0.1455 - val_acc: 0.9473\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14548 to 0.13862, saving model to ../models/cnn_debias_tox_v3_debiased_WE_102_model.h5\n",
      " - 8s - loss: 0.1432 - acc: 0.9477 - val_loss: 0.1386 - val_acc: 0.9498\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13862 to 0.13517, saving model to ../models/cnn_debias_tox_v3_debiased_WE_102_model.h5\n",
      " - 8s - loss: 0.1332 - acc: 0.9511 - val_loss: 0.1352 - val_acc: 0.9515\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9542057996553853\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17513, saving model to ../models/cnn_debias_tox_v3_debiased_WE_103_model.h5\n",
      " - 23s - loss: 0.2440 - acc: 0.9153 - val_loss: 0.1751 - val_acc: 0.9372\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17513 to 0.15114, saving model to ../models/cnn_debias_tox_v3_debiased_WE_103_model.h5\n",
      " - 8s - loss: 0.1674 - acc: 0.9392 - val_loss: 0.1511 - val_acc: 0.9438\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15114 to 0.13939, saving model to ../models/cnn_debias_tox_v3_debiased_WE_103_model.h5\n",
      " - 8s - loss: 0.1476 - acc: 0.9456 - val_loss: 0.1394 - val_acc: 0.9479\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 8s - loss: 0.1352 - acc: 0.9505 - val_loss: 0.1492 - val_acc: 0.9447\n",
      "Epoch 00004: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9486909445535103\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17444, saving model to ../models/cnn_debias_tox_v3_debiased_WE_104_model.h5\n",
      " - 24s - loss: 0.2390 - acc: 0.9157 - val_loss: 0.1744 - val_acc: 0.9383\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17444 to 0.14969, saving model to ../models/cnn_debias_tox_v3_debiased_WE_104_model.h5\n",
      " - 8s - loss: 0.1632 - acc: 0.9412 - val_loss: 0.1497 - val_acc: 0.9460\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14969 to 0.13737, saving model to ../models/cnn_debias_tox_v3_debiased_WE_104_model.h5\n",
      " - 8s - loss: 0.1455 - acc: 0.9470 - val_loss: 0.1374 - val_acc: 0.9499\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13737 to 0.13293, saving model to ../models/cnn_debias_tox_v3_debiased_WE_104_model.h5\n",
      " - 8s - loss: 0.1340 - acc: 0.9509 - val_loss: 0.1329 - val_acc: 0.9519\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9539157584345596\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17018, saving model to ../models/cnn_debias_tox_v3_debiased_WE_105_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 24s - loss: 0.2344 - acc: 0.9190 - val_loss: 0.1702 - val_acc: 0.9395\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17018 to 0.14884, saving model to ../models/cnn_debias_tox_v3_debiased_WE_105_model.h5\n",
      " - 8s - loss: 0.1626 - acc: 0.9404 - val_loss: 0.1488 - val_acc: 0.9445\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14884 to 0.13668, saving model to ../models/cnn_debias_tox_v3_debiased_WE_105_model.h5\n",
      " - 8s - loss: 0.1438 - acc: 0.9470 - val_loss: 0.1367 - val_acc: 0.9493\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13668 to 0.13228, saving model to ../models/cnn_debias_tox_v3_debiased_WE_105_model.h5\n",
      " - 8s - loss: 0.1326 - acc: 0.9509 - val_loss: 0.1323 - val_acc: 0.9504\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9535967267741319\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16839, saving model to ../models/cnn_debias_tox_v3_debiased_WE_106_model.h5\n",
      " - 25s - loss: 0.2348 - acc: 0.9191 - val_loss: 0.1684 - val_acc: 0.9384\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16839 to 0.14696, saving model to ../models/cnn_debias_tox_v3_debiased_WE_106_model.h5\n",
      " - 8s - loss: 0.1622 - acc: 0.9407 - val_loss: 0.1470 - val_acc: 0.9459\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14696 to 0.13745, saving model to ../models/cnn_debias_tox_v3_debiased_WE_106_model.h5\n",
      " - 8s - loss: 0.1437 - acc: 0.9479 - val_loss: 0.1375 - val_acc: 0.9495\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 8s - loss: 0.1328 - acc: 0.9517 - val_loss: 0.1376 - val_acc: 0.9508\n",
      "Epoch 00004: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9481715794871552\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16597, saving model to ../models/cnn_debias_tox_v3_debiased_WE_107_model.h5\n",
      " - 25s - loss: 0.2348 - acc: 0.9190 - val_loss: 0.1660 - val_acc: 0.9400\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16597 to 0.14688, saving model to ../models/cnn_debias_tox_v3_debiased_WE_107_model.h5\n",
      " - 8s - loss: 0.1598 - acc: 0.9422 - val_loss: 0.1469 - val_acc: 0.9471\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14688 to 0.14209, saving model to ../models/cnn_debias_tox_v3_debiased_WE_107_model.h5\n",
      " - 8s - loss: 0.1426 - acc: 0.9477 - val_loss: 0.1421 - val_acc: 0.9466\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14209 to 0.12845, saving model to ../models/cnn_debias_tox_v3_debiased_WE_107_model.h5\n",
      " - 8s - loss: 0.1313 - acc: 0.9518 - val_loss: 0.1285 - val_acc: 0.9527\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9540569889978847\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.16912, saving model to ../models/cnn_debias_tox_v3_debiased_WE_108_model.h5\n",
      " - 26s - loss: 0.2345 - acc: 0.9190 - val_loss: 0.1691 - val_acc: 0.9391\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.16912 to 0.15910, saving model to ../models/cnn_debias_tox_v3_debiased_WE_108_model.h5\n",
      " - 8s - loss: 0.1646 - acc: 0.9404 - val_loss: 0.1591 - val_acc: 0.9433\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15910 to 0.13848, saving model to ../models/cnn_debias_tox_v3_debiased_WE_108_model.h5\n",
      " - 8s - loss: 0.1472 - acc: 0.9461 - val_loss: 0.1385 - val_acc: 0.9487\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.13848 to 0.13088, saving model to ../models/cnn_debias_tox_v3_debiased_WE_108_model.h5\n",
      " - 8s - loss: 0.1356 - acc: 0.9504 - val_loss: 0.1309 - val_acc: 0.9512\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9526484487832575\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17352, saving model to ../models/cnn_debias_tox_v3_debiased_WE_109_model.h5\n",
      " - 27s - loss: 0.2408 - acc: 0.9149 - val_loss: 0.1735 - val_acc: 0.9379\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17352 to 0.15685, saving model to ../models/cnn_debias_tox_v3_debiased_WE_109_model.h5\n",
      " - 8s - loss: 0.1636 - acc: 0.9405 - val_loss: 0.1569 - val_acc: 0.9414\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15685 to 0.14045, saving model to ../models/cnn_debias_tox_v3_debiased_WE_109_model.h5\n",
      " - 8s - loss: 0.1459 - acc: 0.9471 - val_loss: 0.1405 - val_acc: 0.9499\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14045 to 0.13241, saving model to ../models/cnn_debias_tox_v3_debiased_WE_109_model.h5\n",
      " - 8s - loss: 0.1340 - acc: 0.9509 - val_loss: 0.1324 - val_acc: 0.9508\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_debiased_WE_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9529565289863241\n"
     ]
    }
   ],
   "source": [
    "train_models('cnn_debias_tox_v3_debiased_WE',debias,'../data/embeddings/glove.6B/glove_debias_toxic_projection.txt',[hparams_100]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17723, saving model to ../models/we_wiki_cnn_100_model.h5\n",
      " - 27s - loss: 0.2487 - acc: 0.9129 - val_loss: 0.1772 - val_acc: 0.9358\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17723 to 0.15435, saving model to ../models/we_wiki_cnn_100_model.h5\n",
      " - 8s - loss: 0.1687 - acc: 0.9381 - val_loss: 0.1544 - val_acc: 0.9437\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15435 to 0.14781, saving model to ../models/we_wiki_cnn_100_model.h5\n",
      " - 8s - loss: 0.1498 - acc: 0.9449 - val_loss: 0.1478 - val_acc: 0.9440\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14781 to 0.14183, saving model to ../models/we_wiki_cnn_100_model.h5\n",
      " - 8s - loss: 0.1393 - acc: 0.9487 - val_loss: 0.1418 - val_acc: 0.9465\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9531741220901415\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17448, saving model to ../models/we_wiki_cnn_101_model.h5\n",
      " - 27s - loss: 0.2422 - acc: 0.9138 - val_loss: 0.1745 - val_acc: 0.9370\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17448 to 0.15624, saving model to ../models/we_wiki_cnn_101_model.h5\n",
      " - 8s - loss: 0.1671 - acc: 0.9398 - val_loss: 0.1562 - val_acc: 0.9416\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15624 to 0.14864, saving model to ../models/we_wiki_cnn_101_model.h5\n",
      " - 8s - loss: 0.1489 - acc: 0.9454 - val_loss: 0.1486 - val_acc: 0.9466\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14864 to 0.13314, saving model to ../models/we_wiki_cnn_101_model.h5\n",
      " - 8s - loss: 0.1385 - acc: 0.9495 - val_loss: 0.1331 - val_acc: 0.9502\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9534038935912211\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18358, saving model to ../models/we_wiki_cnn_102_model.h5\n",
      " - 28s - loss: 0.2427 - acc: 0.9157 - val_loss: 0.1836 - val_acc: 0.9338\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18358 to 0.15822, saving model to ../models/we_wiki_cnn_102_model.h5\n",
      " - 8s - loss: 0.1665 - acc: 0.9389 - val_loss: 0.1582 - val_acc: 0.9429\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15822 to 0.14131, saving model to ../models/we_wiki_cnn_102_model.h5\n",
      " - 8s - loss: 0.1494 - acc: 0.9446 - val_loss: 0.1413 - val_acc: 0.9459\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14131 to 0.13479, saving model to ../models/we_wiki_cnn_102_model.h5\n",
      " - 8s - loss: 0.1386 - acc: 0.9487 - val_loss: 0.1348 - val_acc: 0.9491\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9526709871108918\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17601, saving model to ../models/we_wiki_cnn_103_model.h5\n",
      " - 29s - loss: 0.2385 - acc: 0.9169 - val_loss: 0.1760 - val_acc: 0.9357\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17601 to 0.15167, saving model to ../models/we_wiki_cnn_103_model.h5\n",
      " - 8s - loss: 0.1684 - acc: 0.9380 - val_loss: 0.1517 - val_acc: 0.9445\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15167 to 0.14102, saving model to ../models/we_wiki_cnn_103_model.h5\n",
      " - 8s - loss: 0.1489 - acc: 0.9452 - val_loss: 0.1410 - val_acc: 0.9476\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14102 to 0.13480, saving model to ../models/we_wiki_cnn_103_model.h5\n",
      " - 8s - loss: 0.1369 - acc: 0.9493 - val_loss: 0.1348 - val_acc: 0.9500\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9536607663462742\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17230, saving model to ../models/we_wiki_cnn_104_model.h5\n",
      " - 29s - loss: 0.2384 - acc: 0.9163 - val_loss: 0.1723 - val_acc: 0.9378\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17230 to 0.14997, saving model to ../models/we_wiki_cnn_104_model.h5\n",
      " - 8s - loss: 0.1673 - acc: 0.9386 - val_loss: 0.1500 - val_acc: 0.9442\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.14997 to 0.14059, saving model to ../models/we_wiki_cnn_104_model.h5\n",
      " - 8s - loss: 0.1490 - acc: 0.9453 - val_loss: 0.1406 - val_acc: 0.9483\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14059 to 0.13575, saving model to ../models/we_wiki_cnn_104_model.h5\n",
      " - 8s - loss: 0.1383 - acc: 0.9495 - val_loss: 0.1358 - val_acc: 0.9499\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9505911238309973\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17518, saving model to ../models/we_wiki_cnn_105_model.h5\n",
      " - 30s - loss: 0.2347 - acc: 0.9168 - val_loss: 0.1752 - val_acc: 0.9368\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17518 to 0.15054, saving model to ../models/we_wiki_cnn_105_model.h5\n",
      " - 8s - loss: 0.1665 - acc: 0.9388 - val_loss: 0.1505 - val_acc: 0.9445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15054 to 0.14311, saving model to ../models/we_wiki_cnn_105_model.h5\n",
      " - 8s - loss: 0.1477 - acc: 0.9450 - val_loss: 0.1431 - val_acc: 0.9450\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14311 to 0.13481, saving model to ../models/we_wiki_cnn_105_model.h5\n",
      " - 8s - loss: 0.1373 - acc: 0.9493 - val_loss: 0.1348 - val_acc: 0.9495\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9520576255653864\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17639, saving model to ../models/we_wiki_cnn_106_model.h5\n",
      " - 30s - loss: 0.2363 - acc: 0.9175 - val_loss: 0.1764 - val_acc: 0.9352\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17639 to 0.15635, saving model to ../models/we_wiki_cnn_106_model.h5\n",
      " - 8s - loss: 0.1682 - acc: 0.9378 - val_loss: 0.1564 - val_acc: 0.9437\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15635 to 0.14172, saving model to ../models/we_wiki_cnn_106_model.h5\n",
      " - 8s - loss: 0.1502 - acc: 0.9447 - val_loss: 0.1417 - val_acc: 0.9478\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14172 to 0.13671, saving model to ../models/we_wiki_cnn_106_model.h5\n",
      " - 8s - loss: 0.1386 - acc: 0.9486 - val_loss: 0.1367 - val_acc: 0.9495\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9522898390824195\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17497, saving model to ../models/we_wiki_cnn_107_model.h5\n",
      " - 31s - loss: 0.2394 - acc: 0.9157 - val_loss: 0.1750 - val_acc: 0.9375\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17497 to 0.15537, saving model to ../models/we_wiki_cnn_107_model.h5\n",
      " - 8s - loss: 0.1678 - acc: 0.9387 - val_loss: 0.1554 - val_acc: 0.9406\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15537 to 0.14289, saving model to ../models/we_wiki_cnn_107_model.h5\n",
      " - 8s - loss: 0.1510 - acc: 0.9437 - val_loss: 0.1429 - val_acc: 0.9460\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14289 to 0.13966, saving model to ../models/we_wiki_cnn_107_model.h5\n",
      " - 8s - loss: 0.1398 - acc: 0.9482 - val_loss: 0.1397 - val_acc: 0.9489\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9513847334771266\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.18445, saving model to ../models/we_wiki_cnn_108_model.h5\n",
      " - 31s - loss: 0.2412 - acc: 0.9159 - val_loss: 0.1844 - val_acc: 0.9344\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.18445 to 0.15697, saving model to ../models/we_wiki_cnn_108_model.h5\n",
      " - 8s - loss: 0.1699 - acc: 0.9375 - val_loss: 0.1570 - val_acc: 0.9434\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15697 to 0.14649, saving model to ../models/we_wiki_cnn_108_model.h5\n",
      " - 8s - loss: 0.1504 - acc: 0.9443 - val_loss: 0.1465 - val_acc: 0.9443\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14649 to 0.13922, saving model to ../models/we_wiki_cnn_108_model.h5\n",
      " - 8s - loss: 0.1382 - acc: 0.9483 - val_loss: 0.1392 - val_acc: 0.9493\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.951806610233215\n",
      "Hyperparameters\n",
      "---------------\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "epochs: 4\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "max_num_words: 10000\n",
      "es_min_delta: 0\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "\n",
      "Training {model_version}\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.17257, saving model to ../models/we_wiki_cnn_109_model.h5\n",
      " - 32s - loss: 0.2377 - acc: 0.9169 - val_loss: 0.1726 - val_acc: 0.9375\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.17257 to 0.15003, saving model to ../models/we_wiki_cnn_109_model.h5\n",
      " - 8s - loss: 0.1672 - acc: 0.9382 - val_loss: 0.1500 - val_acc: 0.9452\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.15003 to 0.14200, saving model to ../models/we_wiki_cnn_109_model.h5\n",
      " - 8s - loss: 0.1491 - acc: 0.9452 - val_loss: 0.1420 - val_acc: 0.9466\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss improved from 0.14200 to 0.13737, saving model to ../models/we_wiki_cnn_109_model.h5\n",
      " - 8s - loss: 0.1377 - acc: 0.9491 - val_loss: 0.1374 - val_acc: 0.9484\n",
      "Model trained!\n",
      "Best model saved to ../models/we_wiki_cnn_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Testing Model\n",
      "0.9536607663462742\n"
     ]
    }
   ],
   "source": [
    "train_models('we_wiki_cnn',wiki,'../data/embeddings/glove.6B/glove_debias_toxic_projection.txt',[hparams_100]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = '../models/'\n",
    "\n",
    "wiki_model_names = ['wiki_cnn_v3_{}'.format(i) for i in xrange(100, 110)]\n",
    "wiki_models = [model_tool.ToxModel(name,model_dir=MODEL_DIR) for name in wiki_model_names]\n",
    "\n",
    "# random_model_names = ['wiki_debias_random_cnn_v3_{}'.format(i) for i in xrange(100, 110)]\n",
    "# random_models = [model_tool.ToxModel(name,model_dir=MODEL_DIR) for name in random_model_names]\n",
    "\n",
    "# debias_model_names = ['wiki_debias_cnn_v3_{}'.format(i) for i in xrange(100, 110)]\n",
    "# debias_models = [model_tool.ToxModel(name,model_dir=MODEL_DIR) for name in debias_model_names]\n",
    "\n",
    "# we_debias_model_names = ['cnn_debias_tox_v3_debiased_WE_{}'.format(i) for i in xrange(100, 110)]\n",
    "# we_debias_models = [model_tool.ToxModel(name,embeddings_path='./glove_debias_new_toxic.txt',model_dir=MODEL_DIR) for name in we_debias_model_names]\n",
    "\n",
    "# we_wiki_model_names = ['we_wiki_cnn_{}'.format(i) for i in xrange(100, 110)]\n",
    "# we_wiki_models = [model_tool.ToxModel(name,embeddings_path='./glove_debias_new_toxic.txt',model_dir=MODEL_DIR) for name in we_wiki_model_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_families = [wiki_models, random_models, debias_models,we_debias_models,we_wiki_models]\n",
    "all_model_families_names = [wiki_model_names, random_model_names, debias_model_names,we_debias_model_names,we_wiki_model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = wiki_models + random_models + debias_models+ we_debias_models+we_wiki_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/toxicity/eval_datasets/bias_madlibs_77k_scored.csv')\n",
    "madlibs = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/toxicity/eval_datasets/bias_madlibs_77k.csv',\n",
    "    scored_path='../data/toxicity/eval_datasets/bias_madlibs_77k_scored.csv',\n",
    "    postprocess_fn=model_tool.postprocess_madlibs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/toxicity/wiki_test_scored.csv')\n",
    "wiki_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/toxicity/wiki_test.csv',\n",
    "    scored_path='../data/toxicity/wiki_test_scored.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('../data/toxicity/wiki_debias_test_scored_newest.csv')\n",
    "os.remove('../data/toxicity/wiki_debias_random_test_scored_newest.csv')\n",
    "\n",
    "wiki_debias_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/toxicity/wiki_debias_test.csv',\n",
    "    scored_path='../data/toxicity/wiki_debias_test_scored_newest.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)\n",
    "\n",
    "wiki_random_test = model_tool.load_maybe_score(\n",
    "    all_models,\n",
    "    orig_path='../data/toxicity/wiki_debias_random_test.csv',\n",
    "    scored_path='../data/toxicity/wiki_debias_random_test_scored_newest.csv',\n",
    "    postprocess_fn=model_tool.postprocess_wiki_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, wiki_model_names, 'label')\n",
    "print('\\n\\nrandom models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, random_model_names, 'label')\n",
    "print('\\n\\ndebias models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, debias_model_names, 'label');\n",
    "print('\\n\\nwe debias models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, we_debias_model_names, 'label');\n",
    "print('\\n\\nwe wiki models:')\n",
    "model_bias_analysis.plot_model_family_auc(madlibs, we_wiki_model_names, 'label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_families_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in [('orig test', wiki_test),\n",
    "                              ('debias test', wiki_debias_test),\n",
    "                              ('random test', wiki_random_test),\n",
    "                              ('madlibs', madlibs)]:\n",
    "    print('\\n\\nAUCs on', dataset_name)\n",
    "    for model_family in all_model_families_names:\n",
    "        fam_name = model_bias_analysis.model_family_name(model_family)\n",
    "        fam_auc = model_bias_analysis.model_family_auc(dataset, model_family, 'label')\n",
    "        print('{:30s}  mean {:.4f}\\t median {:.4f}\\t stddev {:.4f}'.format(fam_name, fam_auc['mean'], fam_auc['median'], fam_auc['std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-term pinned AUC\n",
    "\n",
    "Per-term pinned AUC values show improved scores and less disaprity for the debiased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madlibs_terms = model_bias_analysis.read_identity_terms('../data/toxicity/bias_madlibs_data/adjectives_people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.add_subgroup_columns_from_text(madlibs, 'text', madlibs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_term_madlibs_aucs = model_bias_analysis.per_subgroup_aucs(madlibs, madlibs_terms, all_model_families_names, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_mean = 'wiki_cnn_v3_10_mean'\n",
    "random_mean = 'wiki_debias_random_cnn_v3_10_mean'\n",
    "debias_mean= 'wiki_debias_cnn_v3_10_mean'\n",
    "we_debias_mean = 'cnn_debias_tox_v3_debiased_WE_10_mean'\n",
    "we_wiki_mean = 'we_wiki_cnn_10_mean'\n",
    "\n",
    "\n",
    "for mean_col in [orig_mean, random_mean, debias_mean,we_debias_mean,we_wiki_mean]:\n",
    "    print('per-term AUC histogram: mean AUCs across terms for:', mean_col)\n",
    "    _raw_term_madlibs_aucs[mean_col].hist()\n",
    "    plt.gca().set_xlim((0.85, 1.0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execfile('model_bias_analysis.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_aucs = 'wiki_cnn_v3_10_aucs'\n",
    "random_aucs = 'wiki_debias_random_cnn_v3_10_aucs'\n",
    "debias_aucs= 'wiki_debias_cnn_v3_10_aucs'\n",
    "we_debias_aucs = 'cnn_debias_tox_v3_debiased_WE_10_aucs'\n",
    "we_wiki_aucs = 'we_wiki_cnn_10_aucs'\n",
    "for title, auc_collection_col in [('original model',orig_aucs), ('random treatment',random_aucs), ('debiased dataset treatment',debias_aucs),('debiased dataset and word embedding treatment',we_debias_aucs),('debiased word embedding treatment',we_wiki_aucs)]:\n",
    "    print(auc_collection_col)\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_madlibs_aucs, 'subgroup', auc_collection_col, title='Per-term AUC distributions for ' + title,\n",
    "        file_name='madlibs',y_lim=(0.85, .99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-term  AUCs on the wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.add_subgroup_columns_from_text(wiki_test, 'text', madlibs_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_raw_term_wiki_aucs = model_bias_analysis.per_subgroup_aucs(wiki_test, madlibs_terms, all_model_families_names, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see a strong effect in per-term pinned AUCs on the wikipedia test set. This isn't surprising, as the dataset bias affects the test set too: there are few non-toxic examples using the bias terms, and so the AUC of the original models does not suffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC on tiny datasets is very noisy.\n",
    "_term_wiki_aucs_nottiny = _raw_term_wiki_aucs[_raw_term_wiki_aucs['subset_size'] > 50]\n",
    "\n",
    "for title, auc_collection_col in [('original',orig_aucs), ('random',random_aucs), ('debias',debias_aucs),('we_debias',we_debias_aucs),('we_wiki',we_wiki_aucs)]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(_term_wiki_aucs_nottiny, 'subgroup', auc_collection_col,\n",
    "                                              title='Per-term AUC distributions for ' + title, y_lim=(0.8, 1.02),\n",
    "                                              file_name='wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(jetpack): add column to bias madlibs dataset to allow selecting by template\n",
    "# to more easily observe discrepancies in scores for the same template.\n",
    "\n",
    "TEST_TERMS = ['tall', 'wikipedia', 'teacher', 'music', 'box',\n",
    "              'lesbian', 'gay', 'bisexual', 'transgender', 'queer',\n",
    "              'homosexual', 'heterosexual', 'straight',\n",
    "              'muslim', 'jewish', 'jew', 'christian',\n",
    "              'feminist', 'black', 'white']\n",
    "\n",
    "def _print_template_scores(models):\n",
    "    texts = ['i am a {} person'.format(term) for term in TEST_TERMS]\n",
    "    median_scores = [{'median': np.median([model.predict([text]) for model in models]),\n",
    "                      'stddev': np.std([model.predict([text]) for model in models])}\n",
    "                     for text in texts]\n",
    "    scored_texts = zip(median_scores, texts)\n",
    "    for (score, text) in sorted(scored_texts, key=lambda (score, _text): score['median']):\n",
    "        print('{:.3f}  (std: {:.4f})  {}'.format(score['median'], score['stddev'], text))\n",
    "    plt.hist([x['median'] for x in median_scores])\n",
    "    plt.gca().set_xlim([0, 1])\n",
    "    plt.gca().set_ylim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(wiki_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(random_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_template_scores(debias_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality of Opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate equality of opportunity, we compare false negative rates on a per-term basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_table_means(term_neg_rates):\n",
    "    \"\"\"Helper to display just the mean values of the negative rates.\"\"\"\n",
    "    return (term_neg_rates\n",
    "            [['subgroup',\n",
    "              'orig_fnr_mean',\n",
    "              'random_fnr_mean',\n",
    "              'debias_fnr_mean',\n",
    "              'orig_tnr_mean',\n",
    "              'random_tnr_mean',\n",
    "              'debias_tnr_mean',\n",
    "            ]]\n",
    "            .sort_values('orig_fnr_mean')\n",
    "           )\n",
    "\n",
    "def neg_table_stddevs(term_neg_rates):\n",
    "    \"\"\"Helper to display just the standard deviation values of the negative rates.\"\"\"\n",
    "    return (term_neg_rates\n",
    "            [['subgroup',\n",
    "              'orig_tnr_std',\n",
    "              'random_tnr_std',\n",
    "              'debias_tnr_std',\n",
    "              'orig_tnr_std',\n",
    "              'random_tnr_std',\n",
    "              'debias_tnr_std',\n",
    "              'orig_fnr_mean',  # just for sorting\n",
    "            ]]\n",
    "            .sort_values('orig_fnr_mean')\n",
    "            .drop('orig_fnr_mean', axis=1)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# threshold = .50 \n",
    "\n",
    "_raw_term_neg_rates_50 = model_bias_analysis.per_subgroup_negative_rates(madlibs, madlibs_terms, all_model_families_names, 0.5, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold using per-model equal error rate on overall madlibs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Compute the equal error rate for all models on the overall madlibs dataset in order to\n",
    "# compute the false/true negative rates table at the EER for each model.\n",
    "\n",
    "# Flattened list of all models.\n",
    "_all_model_names = []\n",
    "for model_family_names in all_model_families_names:\n",
    "    _all_model_names.extend(model_family_names)\n",
    "_model_eers_madlibs = model_bias_analysis.per_model_eer(madlibs, 'label', _all_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_raw_term_neg_rates_madlibs_eer = model_bias_analysis.per_subgroup_negative_rates(\n",
    "    madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_term_neg_rates_madlibs_eer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True negative rates: TN / (TN + FP)\n",
    "\n",
    "_raw_term_neg_rates_madlibs_eer_sorted = _raw_term_neg_rates_madlibs_eer.sort_values('wiki_debias_cnn_v3_10_tnr_values')\n",
    "orig_tnr = 'wiki_cnn_v3_10_tnr_values'\n",
    "random_tnr = 'wiki_debias_random_cnn_v3_10_tnr_values'\n",
    "debias_tnr= 'wiki_debias_cnn_v3_10_tnr_values'\n",
    "we_debias_tnr = 'cnn_debias_tox_v3_debiased_WE_10_tnr_values'\n",
    "we_wiki_tnr = 'we_wiki_cnn_10_tnr_values'\n",
    "for title, tnr_values_col in [('original',orig_tnr), ('random',random_tnr), ('debias',debias_tnr),('we_debias',we_debias_tnr),('we_wiki',we_wiki_tnr)]:\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_neg_rates_madlibs_eer_sorted, 'subgroup', tnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term true negative rates for ' + title, file_name='madlibs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False positive rates: 1 - TNR. This is just the above graphs except flipped to show false positives instead of true negatives.\n",
    "\n",
    "# _term_neg_rates_madlibs_eer_tnr_sorted = _term_neg_rates_madlibs_eer.sort_values('orig_tnr_mean')\n",
    "\n",
    "for title, tnr_values_col in [('original model',orig_tnr), ('random treatment',random_tnr), ('debiased dataset treatment',debias_tnr),('debiased dataset and word embeddings treatment',we_debias_tnr),('debiased word embeddings treatment',we_wiki_tnr)]:\n",
    "    term_fpr_values = []\n",
    "    for _i, row in _raw_term_neg_rates_madlibs_eer_sorted.iterrows():\n",
    "        tnr_values = row[tnr_values_col]\n",
    "        fpr_values = [1 - tnr for tnr in tnr_values]\n",
    "        term_fpr_values.append({'subgroup': row['subgroup'], 'fpr_values': fpr_values})\n",
    "    fpr_df = pd.DataFrame(term_fpr_values)\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        fpr_df, 'subgroup', 'fpr_values', y_lim=(0, 1.02),\n",
    "        title='Per-term false positive rates for ' + title,\n",
    "        file_name='madlibs_' + tnr_values_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False negative rates: FN / (FN + TP). 1 - TPR.\n",
    "\n",
    "# Should we use the same ordering as the true negative rate plots?\n",
    "_raw_term_neg_rates_madlibs_eer_sorted = _raw_term_neg_rates_madlibs_eer.sort_values('wiki_debias_cnn_v3_10_fnr_values')\n",
    "orig_fnr = 'wiki_cnn_v3_10_fnr_values'\n",
    "random_fnr = 'wiki_debias_random_cnn_v3_10_fnr_values'\n",
    "debias_fnr= 'wiki_debias_cnn_v3_10_fnr_values'\n",
    "we_debias_fnr = 'cnn_debias_tox_v3_debiased_WE_10_fnr_values'\n",
    "we_wiki_fnr = 'we_wiki_cnn_10_fnr_values'\n",
    "for title, fnr_values_col in [('original model',orig_fnr), ('random treatment',random_fnr), ('debiased dataset treatment',debias_fnr),('debiased dataset and word embedding treatment',we_debias_fnr),('debiased word embedding treatment',we_wiki_fnr)]:\n",
    "    #_term_neg_rates_madlibs_eer_fnr_sorted\n",
    "    model_bias_analysis.per_subgroup_scatterplots(\n",
    "        _raw_term_neg_rates_madlibs_eer_sorted, 'subgroup', fnr_values_col, y_lim=(0, 1.02),\n",
    "        title='Per-term false negative rates for ' + title,\n",
    "        file_name='madlibs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold using per-model equal error rate on Wikipedia test set\n",
    "\n",
    "The EERs computed on the wikipedia test set are similar, and so we don't see much difference in the per-term negative rates plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Compute the equal error rate for all models on the wikipedia dataset.\n",
    "\n",
    "# _model_eers_wiki = model_bias_analysis.per_model_eer(wiki_test, 'label', _all_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _raw_term_neg_rates_wiki_eer = model_bias_analysis.per_subgroup_negative_rates(\n",
    "#     madlibs, madlibs_terms, all_model_families_names, _model_eers_wiki, 'label')\n",
    "# _term_neg_rates_wiki_eer = _raw_term_neg_rates_wiki_eer.rename(columns=column_renamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # True negative rates: TN / (TN + FP)\n",
    "\n",
    "# _term_neg_rates_wiki_eer_tnr_sorted = _term_neg_rates_wiki_eer.sort_values('orig_tnr_mean')\n",
    "\n",
    "# for title, tnr_values_col in [('original model', 'orig_tnr_values'),\n",
    "#                               ('random treatment', 'random_tnr_values'),\n",
    "#                               ('debiasing treatment', 'debias_tnr_values')]:\n",
    "#     model_bias_analysis.per_subgroup_scatterplots(\n",
    "#         _term_neg_rates_wiki_eer_tnr_sorted, 'subgroup', tnr_values_col, y_lim=(0, 1.02),\n",
    "#         title='Per-term true negative rates for ' + title, file_name='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # False negative rates: FN / (FN + TP). 1 - TPR.\n",
    "\n",
    "# # Should we use the same ordering as the true negative rate plots?\n",
    "# _term_neg_rates_wiki_eer_fnr_sorted = _term_neg_rates_wiki_eer.sort_values('orig_fnr_mean')\n",
    "\n",
    "# for title, fnr_values_col in [('original model', 'orig_fnr_values'),\n",
    "#                               ('random treatment', 'random_fnr_values'),\n",
    "#                               ('debiasing treatment', 'debias_fnr_values')]:\n",
    "#     model_bias_analysis.per_subgroup_scatterplots(\n",
    "#         _term_neg_rates_wiki_eer_fnr_sorted, 'subgroup', fnr_values_col, y_lim=(0, 1.02),\n",
    "#         title='Per-term false negative rates for ' + title, file_name='wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_model_families_names print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_fnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, .5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_tnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, .5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_auc_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_auc_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_fnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_analysis.per_subgroup_tnr_diff_from_overall(madlibs, madlibs_terms, all_model_families_names, _model_eers_madlibs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp\n",
    "for index, row in madlibs.iterrows():\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
