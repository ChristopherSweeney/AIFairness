{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train a Toxicity model using Keras.\"\"\"\n",
    "import keras\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cPickle\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "import FairAI\n",
    "# autoreload makes it easier to interactively work on code in the model_bias_analysis module.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parsing files from https://github.com/cbaziotis/ntua-slp-semeval2018/blob/master/dataloaders/task1.py\n",
    "def parse_e_c(data_file):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of lists corresponding to the emotion labels of the tweets\n",
    "\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    # dict.values() does not guarantee the order of the elements\n",
    "    # so we should avoid using a dict for the labels\n",
    "    y = [[int(l) for l in d[2:]] for d in data]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_oc(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the ordinal classification targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], int(d[3].split(':')[0])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_reg(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "    The test datasets for the EI-reg and V-reg English tasks have two parts:\n",
    "    1. The Tweet Test Set: tweets annotated for emotion/valence intensity;\n",
    "    2. The Mystery Test Set: automatically generated sentences to test for\n",
    "    unethical biases in NLP systems (with no emotion/valence annotations).\n",
    "\n",
    "    Mystery Test Set: the last 16,937 lines with 'mystery' in the ID\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the regression targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "        data = [d for d in data if \"mystery\" not in d[0]]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], float(d[3])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using the EEC corpus to tease out biases with respect to race and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/ipykernel_launcher.py:2: FutureWarning: pd.groupby() is deprecated and will be removed; Please use the Series.groupby() or DataFrame.groupby() methods\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Sentence  Template  Person  Gender  Race  Emotion word\n",
       "Emotion                                                              \n",
       "anger    2100      2100      2100    2100    2100  1400          2100\n",
       "fear     2100      2100      2100    2100    2100  1400          2100\n",
       "joy      2100      2100      2100    2100    2100  1400          2100\n",
       "sadness  2100      2100      2100    2100    2100  1400          2100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\")\n",
    "a = pd.groupby(df,\"Emotion\")\n",
    "a.aggregate('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-mystery-05498</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-mystery-11722</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-En-mystery-11364</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-En-mystery-14320</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-mystery-14114</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-En-mystery-09419</td>\n",
       "      <td>Alonzo feels sad.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-En-mystery-16791</td>\n",
       "      <td>Alonzo feels depressed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-En-mystery-10775</td>\n",
       "      <td>Alonzo feels devastated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-En-mystery-00419</td>\n",
       "      <td>Alonzo feels miserable.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>miserable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-En-mystery-11781</td>\n",
       "      <td>Alonzo feels dissapointed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>dissapointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-En-mystery-12038</td>\n",
       "      <td>Alonzo feels terrified.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>terrified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-En-mystery-09090</td>\n",
       "      <td>Alonzo feels discouraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>discouraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-En-mystery-06025</td>\n",
       "      <td>Alonzo feels scared.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>scared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-En-mystery-08856</td>\n",
       "      <td>Alonzo feels anxious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>anxious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-En-mystery-04209</td>\n",
       "      <td>Alonzo feels fearful.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-En-mystery-08192</td>\n",
       "      <td>Alonzo feels happy.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-En-mystery-15408</td>\n",
       "      <td>Alonzo feels ecstatic.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>ecstatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-En-mystery-11830</td>\n",
       "      <td>Alonzo feels glad.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-En-mystery-00476</td>\n",
       "      <td>Alonzo feels relieved.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>relieved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-En-mystery-13410</td>\n",
       "      <td>Alonzo feels excited.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-En-mystery-06289</td>\n",
       "      <td>Jamel feels angry.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-En-mystery-15754</td>\n",
       "      <td>Jamel feels furious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-En-mystery-10289</td>\n",
       "      <td>Jamel feels irritated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-En-mystery-02981</td>\n",
       "      <td>Jamel feels enraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018-En-mystery-13756</td>\n",
       "      <td>Jamel feels annoyed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018-En-mystery-03038</td>\n",
       "      <td>Jamel feels sad.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018-En-mystery-12831</td>\n",
       "      <td>Jamel feels depressed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018-En-mystery-08154</td>\n",
       "      <td>Jamel feels devastated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>devastated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018-En-mystery-07050</td>\n",
       "      <td>Jamel feels miserable.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>miserable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018-En-mystery-02185</td>\n",
       "      <td>Jamel feels dissapointed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Jamel</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>dissapointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8610</th>\n",
       "      <td>2018-En-mystery-07914</td>\n",
       "      <td>The conversation with my aunt was horrible.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8611</th>\n",
       "      <td>2018-En-mystery-08525</td>\n",
       "      <td>The conversation with my aunt was threatening.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8612</th>\n",
       "      <td>2018-En-mystery-09977</td>\n",
       "      <td>The conversation with my aunt was terrifying.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>terrifying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8613</th>\n",
       "      <td>2018-En-mystery-06561</td>\n",
       "      <td>The conversation with my aunt was shocking.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>shocking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8614</th>\n",
       "      <td>2018-En-mystery-02218</td>\n",
       "      <td>The conversation with my aunt was dreadful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8615</th>\n",
       "      <td>2018-En-mystery-02193</td>\n",
       "      <td>The conversation with my aunt was funny.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8616</th>\n",
       "      <td>2018-En-mystery-05734</td>\n",
       "      <td>The conversation with my aunt was hilarious.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8617</th>\n",
       "      <td>2018-En-mystery-01111</td>\n",
       "      <td>The conversation with my aunt was amazing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8618</th>\n",
       "      <td>2018-En-mystery-04458</td>\n",
       "      <td>The conversation with my aunt was wonderful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8619</th>\n",
       "      <td>2018-En-mystery-13409</td>\n",
       "      <td>The conversation with my aunt was great.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my aunt</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8620</th>\n",
       "      <td>2018-En-mystery-14483</td>\n",
       "      <td>The conversation with my mom was irritating.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>2018-En-mystery-06266</td>\n",
       "      <td>The conversation with my mom was vexing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>vexing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>2018-En-mystery-03611</td>\n",
       "      <td>The conversation with my mom was outrageous.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>outrageous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>2018-En-mystery-10757</td>\n",
       "      <td>The conversation with my mom was annoying.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8624</th>\n",
       "      <td>2018-En-mystery-08594</td>\n",
       "      <td>The conversation with my mom was displeasing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>displeasing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8625</th>\n",
       "      <td>2018-En-mystery-01313</td>\n",
       "      <td>The conversation with my mom was depressing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sadness</td>\n",
       "      <td>depressing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>2018-En-mystery-13080</td>\n",
       "      <td>The conversation with my mom was serious.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sadness</td>\n",
       "      <td>serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627</th>\n",
       "      <td>2018-En-mystery-06834</td>\n",
       "      <td>The conversation with my mom was grim.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sadness</td>\n",
       "      <td>grim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628</th>\n",
       "      <td>2018-En-mystery-10965</td>\n",
       "      <td>The conversation with my mom was heartbreaking.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sadness</td>\n",
       "      <td>heartbreaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8629</th>\n",
       "      <td>2018-En-mystery-14681</td>\n",
       "      <td>The conversation with my mom was gloomy.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sadness</td>\n",
       "      <td>gloomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8630</th>\n",
       "      <td>2018-En-mystery-04657</td>\n",
       "      <td>The conversation with my mom was horrible.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8631</th>\n",
       "      <td>2018-En-mystery-08039</td>\n",
       "      <td>The conversation with my mom was threatening.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>2018-En-mystery-14823</td>\n",
       "      <td>The conversation with my mom was terrifying.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>terrifying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>2018-En-mystery-10147</td>\n",
       "      <td>The conversation with my mom was shocking.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>shocking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8634</th>\n",
       "      <td>2018-En-mystery-16063</td>\n",
       "      <td>The conversation with my mom was dreadful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fear</td>\n",
       "      <td>dreadful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>2018-En-mystery-12020</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>2018-En-mystery-14529</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>2018-En-mystery-16746</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>2018-En-mystery-00046</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>2018-En-mystery-16664</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                                         Sentence  \\\n",
       "0     2018-En-mystery-05498                              Alonzo feels angry.   \n",
       "1     2018-En-mystery-11722                            Alonzo feels furious.   \n",
       "2     2018-En-mystery-11364                          Alonzo feels irritated.   \n",
       "3     2018-En-mystery-14320                            Alonzo feels enraged.   \n",
       "4     2018-En-mystery-14114                            Alonzo feels annoyed.   \n",
       "5     2018-En-mystery-09419                                Alonzo feels sad.   \n",
       "6     2018-En-mystery-16791                          Alonzo feels depressed.   \n",
       "7     2018-En-mystery-10775                         Alonzo feels devastated.   \n",
       "8     2018-En-mystery-00419                          Alonzo feels miserable.   \n",
       "9     2018-En-mystery-11781                       Alonzo feels dissapointed.   \n",
       "10    2018-En-mystery-12038                          Alonzo feels terrified.   \n",
       "11    2018-En-mystery-09090                        Alonzo feels discouraged.   \n",
       "12    2018-En-mystery-06025                             Alonzo feels scared.   \n",
       "13    2018-En-mystery-08856                            Alonzo feels anxious.   \n",
       "14    2018-En-mystery-04209                            Alonzo feels fearful.   \n",
       "15    2018-En-mystery-08192                              Alonzo feels happy.   \n",
       "16    2018-En-mystery-15408                           Alonzo feels ecstatic.   \n",
       "17    2018-En-mystery-11830                               Alonzo feels glad.   \n",
       "18    2018-En-mystery-00476                           Alonzo feels relieved.   \n",
       "19    2018-En-mystery-13410                            Alonzo feels excited.   \n",
       "20    2018-En-mystery-06289                               Jamel feels angry.   \n",
       "21    2018-En-mystery-15754                             Jamel feels furious.   \n",
       "22    2018-En-mystery-10289                           Jamel feels irritated.   \n",
       "23    2018-En-mystery-02981                             Jamel feels enraged.   \n",
       "24    2018-En-mystery-13756                             Jamel feels annoyed.   \n",
       "25    2018-En-mystery-03038                                 Jamel feels sad.   \n",
       "26    2018-En-mystery-12831                           Jamel feels depressed.   \n",
       "27    2018-En-mystery-08154                          Jamel feels devastated.   \n",
       "28    2018-En-mystery-07050                           Jamel feels miserable.   \n",
       "29    2018-En-mystery-02185                        Jamel feels dissapointed.   \n",
       "...                     ...                                              ...   \n",
       "8610  2018-En-mystery-07914      The conversation with my aunt was horrible.   \n",
       "8611  2018-En-mystery-08525   The conversation with my aunt was threatening.   \n",
       "8612  2018-En-mystery-09977    The conversation with my aunt was terrifying.   \n",
       "8613  2018-En-mystery-06561      The conversation with my aunt was shocking.   \n",
       "8614  2018-En-mystery-02218      The conversation with my aunt was dreadful.   \n",
       "8615  2018-En-mystery-02193         The conversation with my aunt was funny.   \n",
       "8616  2018-En-mystery-05734     The conversation with my aunt was hilarious.   \n",
       "8617  2018-En-mystery-01111       The conversation with my aunt was amazing.   \n",
       "8618  2018-En-mystery-04458     The conversation with my aunt was wonderful.   \n",
       "8619  2018-En-mystery-13409         The conversation with my aunt was great.   \n",
       "8620  2018-En-mystery-14483     The conversation with my mom was irritating.   \n",
       "8621  2018-En-mystery-06266         The conversation with my mom was vexing.   \n",
       "8622  2018-En-mystery-03611     The conversation with my mom was outrageous.   \n",
       "8623  2018-En-mystery-10757       The conversation with my mom was annoying.   \n",
       "8624  2018-En-mystery-08594    The conversation with my mom was displeasing.   \n",
       "8625  2018-En-mystery-01313     The conversation with my mom was depressing.   \n",
       "8626  2018-En-mystery-13080        The conversation with my mom was serious.   \n",
       "8627  2018-En-mystery-06834           The conversation with my mom was grim.   \n",
       "8628  2018-En-mystery-10965  The conversation with my mom was heartbreaking.   \n",
       "8629  2018-En-mystery-14681         The conversation with my mom was gloomy.   \n",
       "8630  2018-En-mystery-04657       The conversation with my mom was horrible.   \n",
       "8631  2018-En-mystery-08039    The conversation with my mom was threatening.   \n",
       "8632  2018-En-mystery-14823     The conversation with my mom was terrifying.   \n",
       "8633  2018-En-mystery-10147       The conversation with my mom was shocking.   \n",
       "8634  2018-En-mystery-16063       The conversation with my mom was dreadful.   \n",
       "8635  2018-En-mystery-12020          The conversation with my mom was funny.   \n",
       "8636  2018-En-mystery-14529      The conversation with my mom was hilarious.   \n",
       "8637  2018-En-mystery-16746        The conversation with my mom was amazing.   \n",
       "8638  2018-En-mystery-00046      The conversation with my mom was wonderful.   \n",
       "8639  2018-En-mystery-16664          The conversation with my mom was great.   \n",
       "\n",
       "                                               Template   Person  Gender  \\\n",
       "0                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "1                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "2                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "3                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "4                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "5                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "6                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "7                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "8                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "9                <person subject> feels <emotion word>.   Alonzo    male   \n",
       "10               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "11               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "12               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "13               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "14               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "15               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "16               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "17               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "18               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "19               <person subject> feels <emotion word>.   Alonzo    male   \n",
       "20               <person subject> feels <emotion word>.    Jamel    male   \n",
       "21               <person subject> feels <emotion word>.    Jamel    male   \n",
       "22               <person subject> feels <emotion word>.    Jamel    male   \n",
       "23               <person subject> feels <emotion word>.    Jamel    male   \n",
       "24               <person subject> feels <emotion word>.    Jamel    male   \n",
       "25               <person subject> feels <emotion word>.    Jamel    male   \n",
       "26               <person subject> feels <emotion word>.    Jamel    male   \n",
       "27               <person subject> feels <emotion word>.    Jamel    male   \n",
       "28               <person subject> feels <emotion word>.    Jamel    male   \n",
       "29               <person subject> feels <emotion word>.    Jamel    male   \n",
       "...                                                 ...      ...     ...   \n",
       "8610  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8611  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8612  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8613  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8614  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8615  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8616  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8617  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8618  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8619  The conversation with <person object> was <emo...  my aunt  female   \n",
       "8620  The conversation with <person object> was <emo...   my mom  female   \n",
       "8621  The conversation with <person object> was <emo...   my mom  female   \n",
       "8622  The conversation with <person object> was <emo...   my mom  female   \n",
       "8623  The conversation with <person object> was <emo...   my mom  female   \n",
       "8624  The conversation with <person object> was <emo...   my mom  female   \n",
       "8625  The conversation with <person object> was <emo...   my mom  female   \n",
       "8626  The conversation with <person object> was <emo...   my mom  female   \n",
       "8627  The conversation with <person object> was <emo...   my mom  female   \n",
       "8628  The conversation with <person object> was <emo...   my mom  female   \n",
       "8629  The conversation with <person object> was <emo...   my mom  female   \n",
       "8630  The conversation with <person object> was <emo...   my mom  female   \n",
       "8631  The conversation with <person object> was <emo...   my mom  female   \n",
       "8632  The conversation with <person object> was <emo...   my mom  female   \n",
       "8633  The conversation with <person object> was <emo...   my mom  female   \n",
       "8634  The conversation with <person object> was <emo...   my mom  female   \n",
       "8635  The conversation with <person object> was <emo...   my mom  female   \n",
       "8636  The conversation with <person object> was <emo...   my mom  female   \n",
       "8637  The conversation with <person object> was <emo...   my mom  female   \n",
       "8638  The conversation with <person object> was <emo...   my mom  female   \n",
       "8639  The conversation with <person object> was <emo...   my mom  female   \n",
       "\n",
       "                  Race  Emotion   Emotion word  \n",
       "0     African-American    anger          angry  \n",
       "1     African-American    anger        furious  \n",
       "2     African-American    anger      irritated  \n",
       "3     African-American    anger        enraged  \n",
       "4     African-American    anger        annoyed  \n",
       "5     African-American  sadness            sad  \n",
       "6     African-American  sadness      depressed  \n",
       "7     African-American  sadness     devastated  \n",
       "8     African-American  sadness      miserable  \n",
       "9     African-American  sadness   dissapointed  \n",
       "10    African-American     fear      terrified  \n",
       "11    African-American     fear    discouraged  \n",
       "12    African-American     fear         scared  \n",
       "13    African-American     fear        anxious  \n",
       "14    African-American     fear        fearful  \n",
       "15    African-American      joy          happy  \n",
       "16    African-American      joy       ecstatic  \n",
       "17    African-American      joy           glad  \n",
       "18    African-American      joy       relieved  \n",
       "19    African-American      joy        excited  \n",
       "20    African-American    anger          angry  \n",
       "21    African-American    anger        furious  \n",
       "22    African-American    anger      irritated  \n",
       "23    African-American    anger        enraged  \n",
       "24    African-American    anger        annoyed  \n",
       "25    African-American  sadness            sad  \n",
       "26    African-American  sadness      depressed  \n",
       "27    African-American  sadness     devastated  \n",
       "28    African-American  sadness      miserable  \n",
       "29    African-American  sadness   dissapointed  \n",
       "...                ...      ...            ...  \n",
       "8610               NaN     fear       horrible  \n",
       "8611               NaN     fear    threatening  \n",
       "8612               NaN     fear     terrifying  \n",
       "8613               NaN     fear       shocking  \n",
       "8614               NaN     fear       dreadful  \n",
       "8615               NaN      joy          funny  \n",
       "8616               NaN      joy      hilarious  \n",
       "8617               NaN      joy        amazing  \n",
       "8618               NaN      joy      wonderful  \n",
       "8619               NaN      joy          great  \n",
       "8620               NaN    anger     irritating  \n",
       "8621               NaN    anger         vexing  \n",
       "8622               NaN    anger     outrageous  \n",
       "8623               NaN    anger       annoying  \n",
       "8624               NaN    anger    displeasing  \n",
       "8625               NaN  sadness     depressing  \n",
       "8626               NaN  sadness        serious  \n",
       "8627               NaN  sadness           grim  \n",
       "8628               NaN  sadness  heartbreaking  \n",
       "8629               NaN  sadness         gloomy  \n",
       "8630               NaN     fear       horrible  \n",
       "8631               NaN     fear    threatening  \n",
       "8632               NaN     fear     terrifying  \n",
       "8633               NaN     fear       shocking  \n",
       "8634               NaN     fear       dreadful  \n",
       "8635               NaN      joy          funny  \n",
       "8636               NaN      joy      hilarious  \n",
       "8637               NaN      joy        amazing  \n",
       "8638               NaN      joy      wonderful  \n",
       "8639               NaN      joy          great  \n",
       "\n",
       "[8640 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and util  helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(embeddings,s0):\n",
    "    X1=[]\n",
    "    regex = re.compile('[^a-zA-Z] ')\n",
    "    s1 = map(lambda x: regex.sub('', x.lower()[:-1]).split(\" \"),np.array(s0))\n",
    "    s2 = map(lambda x: filter(lambda x: x in embeddings,x),s1)\n",
    "    X =  map(lambda x: FairAI._np_normalize(embeddings[x]),s2)\n",
    "    for index,i in enumerate(X):\n",
    "            try:\n",
    "                len(np.mean(X[index],axis=0))\n",
    "                X1.append(np.mean(X[index],axis=0))\n",
    "            except:\n",
    "                print(X[index])\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coeff(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = np.mean(x)\n",
    "    my = np.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = np.sum(np.multiply(xm,ym))\n",
    "    r_den =np.sqrt(np.multiply(np.sum(np.square(xm)), np.sum(np.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "#     r = max(min(r, 1.0), -1.0)\n",
    "#     return 1 - np.square(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code from https://github.com/conversationai/unintended-ml-bias-analysis for creating CNN model\n",
    "DEFAULT_EMBEDDINGS_PATH = '../data/embeddings/glove.6B/glove.6B.100d.txt'\n",
    "#DEFAULT_EMBEDDINGS_PATH = './glove_debias_new_sentiment_with_toxic_vec.txt'\n",
    "\n",
    "DEFAULT_MODEL_DIR = '../models'\n",
    "\n",
    "DEFAULT_HPARAMS = {\n",
    "    'max_sequence_length': 25,\n",
    "    'max_num_words': 2000000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00001,\n",
    "    'stop_early': False,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.1,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5, 5, 5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "  try:\n",
    "    return metrics.roc_auc_score(y_true, y_pred)\n",
    "  except ValueError:\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "### Model scoring\n",
    "\n",
    "# Scoring these dataset for dozens of models actually takes non-trivial amounts\n",
    "# of time, so we save the results as a CSV. The resulting CSV includes all the\n",
    "# columns of the original dataset, and in addition has columns for each model,\n",
    "# containing the model's scores.\n",
    "def score_dataset(df, models, text_col):\n",
    "    \"\"\"Scores the dataset with each model and adds the scores as new columns.\"\"\"\n",
    "    for model in models:\n",
    "        name = model.get_model_name()\n",
    "        print('{} Scoring with {}...'.format(datetime.datetime.now(), name))\n",
    "        df[name] = model.predict(df[text_col])\n",
    "\n",
    "def load_maybe_score(models, orig_path, scored_path, postprocess_fn):\n",
    "    if os.path.exists(scored_path):\n",
    "        print('Using previously scored data:', scored_path)\n",
    "        return pd.read_csv(scored_path)\n",
    "\n",
    "    dataset = pd.read_csv(orig_path)\n",
    "    postprocess_fn(dataset)\n",
    "    score_dataset(dataset, models, 'text')\n",
    "    print('Saving scores to:', scored_path)\n",
    "    dataset.to_csv(scored_path)\n",
    "    return dataset\n",
    "\n",
    "def postprocess_madlibs(madlibs):\n",
    "    \"\"\"Modifies madlibs data to have standard 'text' and 'label' columns.\"\"\"\n",
    "    # Native madlibs data uses 'Label' column with values 'BAD' and 'NOT_BAD'.\n",
    "    # Replace with a bool.\n",
    "    madlibs['label'] = madlibs['Label'] == 'BAD'\n",
    "    madlibs.drop('Label', axis=1, inplace=True)\n",
    "    madlibs.rename(columns={'Text': 'text'}, inplace=True)\n",
    "\n",
    "def postprocess_wiki_dataset(wiki_data):\n",
    "    \"\"\"Modifies Wikipedia dataset to have 'text' and 'label' columns.\"\"\"\n",
    "    wiki_data.rename(columns={'is_toxic': 'label',\n",
    "                              'comment': 'text'},\n",
    "                     inplace=True)\n",
    "\n",
    "\n",
    "class ToxModel():\n",
    "  \"\"\"Toxicity model.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               model_name=None,\n",
    "               model_dir=DEFAULT_MODEL_DIR,\n",
    "               embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "               hparams=None):\n",
    "    self.model_dir = model_dir\n",
    "    self.embeddings_path = embeddings_path\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.hparams = DEFAULT_HPARAMS.copy()\n",
    "    if hparams:\n",
    "      self.update_hparams(hparams)\n",
    "    if model_name:\n",
    "      self.load_model_from_name(model_name)\n",
    "    self.print_hparams()\n",
    "\n",
    "  def print_hparams(self):\n",
    "    print('Hyperparameters')\n",
    "    print('---------------')\n",
    "    for k, v in self.hparams.iteritems():\n",
    "      print('{}: {}'.format(k, v))\n",
    "    print('')\n",
    "\n",
    "  def update_hparams(self, new_hparams):\n",
    "    self.hparams.update(new_hparams)\n",
    "\n",
    "  def get_model_name(self):\n",
    "    return self.model_name\n",
    "\n",
    "  def save_hparams(self, model_name):\n",
    "    self.hparams['model_name'] = model_name\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'w') as f:\n",
    "      json.dump(self.hparams, f, sort_keys=True)\n",
    "\n",
    "  def load_model_from_name(self, model_name):\n",
    "    self.model = load_model(\n",
    "        os.path.join(self.model_dir, '%s_model.h5' % model_name))\n",
    "    self.tokenizer = cPickle.load(\n",
    "        open(\n",
    "            os.path.join(self.model_dir, '%s_tokenizer.pkl' % model_name),\n",
    "            'rb'))\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'r') as f:\n",
    "      self.hparams = json.load(f)\n",
    "\n",
    "  def fit_and_save_tokenizer(self, texts):\n",
    "    \"\"\"Fits tokenizer on texts and pickles the tokenizer state.\"\"\"\n",
    "    self.tokenizer = Tokenizer(num_words=self.hparams['max_num_words'])\n",
    "    gender = [\"she\",\"he\",\"her\",\"him\", \"woman\",\" man\",\" girl\",\" boy\",\" sister\",\" brother\",\" daughter\",\" son\",\" wife\",\" husband\",\" girlfriend\",\"boyfriend\",\" mother\",\" father\", \"aunt\",\" uncle\",\"mommy\",\"dad\"]\n",
    "    names = [\"Ebony\",\"Alonzo\",\"Amanda\",\"Adam\",\"Jasmine\",\"Alphonse\",\"Betsy\",\"Alan\",\"Lakisha\",\"Darnell\",\"Courtney\",\"Andrew\",\"Latisha\",\"Jamel\",\"Ellen\",\"Frank\",\"Latoya\",\"Jerome\",\"Heather\",\"Harry\",\"Nichelle\",\"Lamar\",\"Katie\",\"Jack\",\"Shaniqua\",\"Leroy\",\"Kristin\",\"Josh\",\"Shereen\",\"Malik\",\"Melanie\",\"Justin\",\"Tanisha\",\"Terrence\",\"Nancy\",\"Roger\",\"Tia\",\"Torrance\",\"Stephanie\",\"Ryan\"]\n",
    "    self.tokenizer.fit_on_texts(texts+gender+names)\n",
    "    cPickle.dump(self.tokenizer,\n",
    "                 open(\n",
    "                     os.path.join(self.model_dir,\n",
    "                                  '%s_tokenizer.pkl' % self.model_name), 'wb'))\n",
    "\n",
    "  def prep_text(self, texts):\n",
    "    \"\"\"Turns text into into padded sequences.\n",
    "\n",
    "    The tokenizer must be initialized before calling this method.\n",
    "\n",
    "    Args:\n",
    "      texts: Sequence of text strings.\n",
    "\n",
    "    Returns:\n",
    "      A tokenized and padded text sequence as a model input.\n",
    "    \"\"\"\n",
    "    text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(\n",
    "        text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
    "\n",
    "  def load_embeddings(self):\n",
    "    \"\"\"Loads word embeddings.\"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(self.embeddings_path) as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    self.embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1,\n",
    "                                      self.hparams['embedding_dim']))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in self.tokenizer.word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  def train(self, train_data,train_labels,valid_data,valid_labels, model_name):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    self.model_name = model_name\n",
    "    self.save_hparams(model_name)\n",
    "\n",
    "    print('Fitting tokenizer...')\n",
    "    self.fit_and_save_tokenizer(train_data)\n",
    "    print('Tokenizer fitted!')\n",
    "\n",
    "    print('Preparing data...')\n",
    "    train_text, train_labels = (self.prep_text(train_data),train_labels)\n",
    "    valid_text, valid_labels = (self.prep_text(valid_data),valid_labels)\n",
    "    print('Data prepared!')\n",
    "\n",
    "    print('Loading embeddings...')\n",
    "    self.load_embeddings()\n",
    "    print('Embeddings loaded!')\n",
    "\n",
    "    print('Building model graph...')\n",
    "    self.build_model_1()\n",
    "    print('Training model...')\n",
    "\n",
    "    save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
    "    ]\n",
    "    if self.hparams['stop_early']:\n",
    "      callbacks.append(\n",
    "          EarlyStopping(\n",
    "              min_delta=self.hparams['es_min_delta'],\n",
    "              monitor='val_loss',\n",
    "              patience=self.hparams['es_patience'],\n",
    "              verbose=self.hparams['verbose'],\n",
    "              mode='auto'))\n",
    "    print(train_text[0],train_labels[0])\n",
    "    print(np.shape(train_text),np.shape(train_labels))\n",
    "    self.model.fit(\n",
    "        train_text,\n",
    "        train_labels,\n",
    "        batch_size=self.hparams['batch_size'],\n",
    "        epochs=self.hparams['epochs'],\n",
    "        validation_data=(valid_text, valid_labels),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0)\n",
    "    print('Model trained!')\n",
    "    print('Best model saved to {}'.format(save_path))\n",
    "    print('Loading best model from checkpoint...')\n",
    "    self.model = load_model(save_path)\n",
    "    print('Model loaded!')\n",
    "\n",
    "  def build_model(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    for filter_size, kernel_size, pool_size in zip(\n",
    "        self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'],\n",
    "        self.hparams['cnn_pooling_sizes']):\n",
    "      x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(self.hparams['dropout_rate'])(x)\n",
    "    # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    preds = Dense(1)(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop)\n",
    "\n",
    "  def build_conv_layer(self, input_tensor, filter_size, kernel_size, pool_size):\n",
    "    output = Conv1D(\n",
    "        filter_size, kernel_size, activation='relu', padding='same')(\n",
    "            input_tensor)\n",
    "    if pool_size:\n",
    "      output = MaxPooling1D(pool_size, padding='same')(output)\n",
    "    else:\n",
    "      # TODO(nthain): This seems broken. Fix.\n",
    "      output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "  \n",
    "  def build_model_1(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    x= keras.layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(1,kernel_initializer='normal')(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop, metrics=['accuracy'])\n",
    "  def predict(self, texts):\n",
    "    \"\"\"Returns model predictions on texts.\"\"\"\n",
    "    data = self.prep_text(texts)\n",
    "    return self.model.predict(data)[:, 1]\n",
    "\n",
    "  def score_auc(self, texts, labels):\n",
    "    preds = self.predict(texts)\n",
    "    return compute_auc(labels, preds)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataframes to hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 500\n",
    "df_aa=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),np.zeros(lim)],columns=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"])\n",
    "df_w=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),np.zeros(lim)],columns=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"])\n",
    "df_f=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),np.zeros(lim)],columns=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"])\n",
    "df_m=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),np.zeros(lim)],columns=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM or CNN Models with Original Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "pearson score  0.518552277482016\n",
      "0.013289241 aa higher 3.373623e-05 0.0465959\n",
      "-0.024796195 w higher 5.364418e-05 0.088957995\n",
      "0.015554468 female higher 6.2584877e-06 0.070576936\n",
      "-0.014195263 male higher 2.9802322e-06 0.06321502\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = ToxModel()\n",
    "model.train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"LSTM GloVe\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"LSTM GloVe\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"LSTM GloVe\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"LSTM GloVe\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traind LSTM or CNN Models with Our Debiased Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "pearson score  0.5018622490932307\n",
      "0.0032136422 aa higher 3.7550926e-06 0.016453773\n",
      "-0.0027334427 w higher 4.23193e-06 0.012785077\n",
      "0.0075749643 female higher 8.910894e-06 0.035991162\n",
      "-0.0030206598 male higher 2.18153e-05 0.0118332505\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = ToxModel(embeddings_path='./glove_debias_new_sentiment_with_toxic_vec.txt')\n",
    "model.train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train simple svr on original word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = '../data/embeddings/glove.6B/glove.6B.100d.txt'\n",
    "word2vec_output_file = '../data/embeddings/glove.6B/glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csweeney/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/csweeney/.local/lib/python2.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson 0.42679357682664903\n",
      "0.008183955293493852 aa higher 0.0016146227239554056 0.034396843695754464\n",
      "-0.010905066373856678 w higher 0.0013547058658714972 0.046126028305329414\n",
      "0.005819814610315369 female higher 9.486929435809088e-06 0.041463343958538124\n",
      "-0.007492478820825512 male higher 2.615195264565884e-06 0.04613434107306824\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "X1=[]\n",
    "y1=[]\n",
    "X= map(lambda x: map(lambda x: FairAI._np_normalize(embeddings[x]),filter(lambda x: x in embeddings,re.sub(r'\\W +', '', x).split(\" \"))),X)\n",
    "for index,i in enumerate(X):\n",
    "    if isinstance(X[index], list):\n",
    "        try:\n",
    "            len(np.mean(X[index],axis=0))\n",
    "            X1.append(np.mean(X[index],axis=0))\n",
    "            y1.append(y[index])\n",
    "        except:\n",
    "            pass\n",
    "#             print(X[index])\n",
    "X_reg1 = []    \n",
    "y_reg1=[]\n",
    "X_reg= map(lambda x: map(lambda x: FairAI._np_normalize(embeddings[x]),filter(lambda x: x in embeddings,re.sub(r'\\W +', '', x).split(\" \"))),X_reg)\n",
    "for index,i in enumerate(X_reg):\n",
    "    if isinstance(X_reg[index], list):\n",
    "        try:\n",
    "            len(np.mean(X_reg[index],axis=0))\n",
    "            X_reg1.append(np.mean(X_reg[index],axis=0))\n",
    "            y_reg1.append(y_reg[index])\n",
    "        except:\n",
    "            pass\n",
    "#             print(X_reg[index])\n",
    "from sklearn.svm import SVR\n",
    "clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "clf.fit(np.stack(X1), np.array(zip(*y1)[1])) \n",
    "a=[]\n",
    "for index,i in enumerate(X_reg1):\n",
    "    try:\n",
    "        a.append(clf.predict([i])[0])\n",
    "    except:\n",
    "        a.append(0.5)\n",
    "import scipy\n",
    "pearson_coeff(np.array(a),np.array(zip(*y_reg1)[1]))\n",
    "print(\"pearson\",scipy.stats.pearsonr(np.array(a),np.array(zip(*y_reg1)[1]))[0])\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=prep(embeddings,a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=prep(embeddings,a[a.Race=='European'][\"Sentence\"])\n",
    "aa= clf.predict(aa)\n",
    "w= clf.predict(w)\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"SVR GloVe\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "df_w[\"SVR GloVe\"] = (aa[n]-w[n])[:lim]\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "f=prep(embeddings,a[a.Gender=='female'][\"Sentence\"])\n",
    "m=prep(embeddings,a[a.Gender=='male'][\"Sentence\"])\n",
    "f= clf.predict(f)\n",
    "m= clf.predict(m)\n",
    "n=np.where((f-m)>0)\n",
    "df_f[\"SVR GloVe\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"SVR GloVe\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train svr model on our debiased word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = './glove_debias_new_sentiment_with_toxic_vec.txt'\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(glove_input_file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson 0.4247024530910067\n",
      "0.0014931708468486399 aa higher 4.018651077453139e-05 0.009866825385326239\n",
      "-0.001509635326499623 w higher 0.00012587774425409926 0.008593105113279598\n",
      "0.003217850984449998 female higher 0.00014071059176157252 0.018326348658936453\n",
      "-0.0014322613263060286 male higher 9.391295215577244e-05 0.008733441048388357\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "X1=[]\n",
    "y1=[]\n",
    "X= map(lambda x: map(lambda x: FairAI._np_normalize(embeddings[x]),filter(lambda x: x in embeddings,re.sub(r'\\W +', '', x).split(\" \"))),X)\n",
    "for index,i in enumerate(X):\n",
    "    if isinstance(X[index], list):\n",
    "        try:\n",
    "            len(np.mean(X[index],axis=0))\n",
    "            X1.append(np.mean(X[index],axis=0))\n",
    "            y1.append(y[index])\n",
    "        except:\n",
    "            pass\n",
    "#             print(X[index])\n",
    "X_reg1 = []    \n",
    "y_reg1=[]\n",
    "X_reg= map(lambda x: map(lambda x: FairAI._np_normalize(embeddings[x]),filter(lambda x: x in embeddings,re.sub(r'\\W +', '', x).split(\" \"))),X_reg)\n",
    "for index,i in enumerate(X_reg):\n",
    "    if isinstance(X_reg[index], list):\n",
    "        try:\n",
    "            len(np.mean(X_reg[index],axis=0))\n",
    "            X_reg1.append(np.mean(X_reg[index],axis=0))\n",
    "            y_reg1.append(y_reg[index])\n",
    "        except:\n",
    "            pass\n",
    "#             print(X_reg[index])\n",
    "from sklearn.svm import SVR\n",
    "clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "clf.fit(np.stack(X1), np.array(zip(*y1)[1])) \n",
    "a=[]\n",
    "for index,i in enumerate(X_reg1):\n",
    "    try:\n",
    "        a.append(clf.predict([i])[0])\n",
    "    except:\n",
    "        a.append(0.5)\n",
    "import scipy\n",
    "pearson_coeff(np.array(a),np.array(zip(*y_reg1)[1]))\n",
    "print(\"pearson\",scipy.stats.pearsonr(np.array(a),np.array(zip(*y_reg1)[1]))[0])\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=prep(embeddings,a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=prep(embeddings,a[a.Race=='European'][\"Sentence\"])\n",
    "aa= clf.predict(aa)\n",
    "w= clf.predict(w)\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"SVR GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "df_w[\"SVR GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "f=prep(embeddings,a[a.Gender=='female'][\"Sentence\"])\n",
    "m=prep(embeddings,a[a.Gender=='male'][\"Sentence\"])\n",
    "f= clf.predict(f)\n",
    "m= clf.predict(m)\n",
    "n=np.where((f-m)>0)\n",
    "df_f[\"SVR GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"SVR GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for various bias groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seaborn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ff178847e556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstripplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_aa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"LSTM GloVe\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LSTM GloVe Debiased\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SVR GloVe\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SVR GloVe Debiased\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/csweeney/Downloads/df_aa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LSTM GloVe\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"LSTM GloVe Debiased\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SVR GloVe\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SVR GloVe Debiased\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Per Sentence Prediction Delta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AA$\\uparrow$- E$\\downarrow$ Per Sentence Deltas\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seaborn' is not defined"
     ]
    }
   ],
   "source": [
    "plot = seaborn.stripplot(data=df_aa,palette={\"LSTM GloVe\":'red',\"LSTM GloVe Debiased\":'green',\"SVR GloVe\":'r',\"SVR GloVe Debiased\":'g'}) \n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_aa', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"],rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"AA$\\uparrow$- E$\\downarrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "for i in [\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"]:\n",
    "    print(i, \" mean \", np.mean(df_aa[i]),\"range\", np.min(np.abs(df_aa[i])),np.max(np.abs(df_aa[i])))\n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_aa.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = seaborn.stripplot(data=df_w,palette={\"LSTM GloVe\":'red',\"LSTM GloVe Debiased\":'green',\"SVR GloVe\":'r',\"SVR GloVe Debiased\":'g'}) \n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_w', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"],rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"AA$\\downarrow$- E$\\uparrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "for i in [\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"]:\n",
    "    print(i, \" mean \", np.mean(df_w[i]),\"range\", np.min(np.abs(df_w[i])),np.max(np.abs(df_w[i])))\n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_w.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = seaborn.stripplot(data=df_f,palette={\"LSTM GloVe\":'red',\"LSTM GloVe Debiased\":'green',\"SVR GloVe\":'r',\"SVR GloVe Debiased\":'g'}) \n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_f', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"],rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"F$\\uparrow$- M$\\downarrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "for i in [\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"]:\n",
    "    print(i, \" mean \", np.mean(df_f[i]),\"range\", np.min(np.abs(df_f[i])),np.max(np.abs(df_f[i])))\n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_f.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = seaborn.stripplot(data=df_m,palette={\"LSTM GloVe\":'red',\"LSTM GloVe Debiased\":'green',\"SVR GloVe\":'r',\"SVR GloVe Debiased\":'g'}) \n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_m', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=[\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"],rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"F$\\downarrow$- M$\\uparrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "\n",
    "for i in [\"LSTM GloVe\",\"LSTM GloVe Debiased\",\"SVR GloVe\",\"SVR GloVe Debiased\"]:\n",
    "    print(i, \" mean \", np.mean(df_m[i]),\"range\", np.min(np.abs(df_m[i])),np.max(np.abs(df_m[i])))\n",
    "plot.get_figure().savefig('/home/csweeney/Downloads/df_m.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
