{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train a Toxicity model using Keras.\"\"\"\n",
    "import keras\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cPickle\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "import FairAI\n",
    "# autoreload makes it easier to interactively work on code in the model_bias_analysis module.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions for parsing the data\n",
    "### parsing files from https://github.com/cbaziotis/ntua-slp-semeval2018/blob/master/dataloaders/task1.py\n",
    "def parse_e_c(data_file):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of lists corresponding to the emotion labels of the tweets\n",
    "\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    # dict.values() does not guarantee the order of the elements\n",
    "    # so we should avoid using a dict for the labels\n",
    "    y = [[int(l) for l in d[2:]] for d in data]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_oc(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the ordinal classification targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], int(d[3].split(':')[0])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def parse_reg(data_file, label_format='tuple'):\n",
    "    \"\"\"\n",
    "    The test datasets for the EI-reg and V-reg English tasks have two parts:\n",
    "    1. The Tweet Test Set: tweets annotated for emotion/valence intensity;\n",
    "    2. The Mystery Test Set: automatically generated sentences to test for\n",
    "    unethical biases in NLP systems (with no emotion/valence annotations).\n",
    "\n",
    "    Mystery Test Set: the last 16,937 lines with 'mystery' in the ID\n",
    "\n",
    "    Returns:\n",
    "        X: a list of tweets\n",
    "        y: a list of (affect dimension, v) tuples corresponding to\n",
    "         the regression targets of the tweets\n",
    "    \"\"\"\n",
    "    with open(data_file, 'r') as fd:\n",
    "        data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "        data = [d for d in data if \"mystery\" not in d[0]]\n",
    "    X = [d[1] for d in data]\n",
    "    y = [(d[2], float(d[3])) for d in data]\n",
    "    if label_format == 'list':\n",
    "        y = [l[1] for l in y]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using the EEC corpus to tease out biases with respect to race and gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and util  helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(embeddings,s0):\n",
    "    X1=[]\n",
    "    regex = re.compile('[^a-zA-Z] ')\n",
    "    s1 = map(lambda x: regex.sub('', x.lower()[:-1]).split(\" \"),np.array(s0))\n",
    "    s2 = map(lambda x: filter(lambda x: x in embeddings,x),s1)\n",
    "    X =  map(lambda x: FairAI._np_normalize(embeddings[x]),s2)\n",
    "    for index,i in enumerate(X):\n",
    "            try:\n",
    "                len(np.mean(X[index],axis=0))\n",
    "                X1.append(np.mean(X[index],axis=0))\n",
    "            except:\n",
    "                print(X[index])\n",
    "    return X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coeff(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = np.mean(x)\n",
    "    my = np.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = np.sum(np.multiply(xm,ym))\n",
    "    r_den =np.sqrt(np.multiply(np.sum(np.square(xm)), np.sum(np.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "#     r = max(min(r, 1.0), -1.0)\n",
    "#     return 1 - np.square(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters & Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  code from https://github.com/conversationai/unintended-ml-bias-analysis for creating CNN model\n",
    "DEFAULT_EMBEDDINGS_PATH = '../data/embeddings/glove.6B/glove.6B.100d.txt'\n",
    "#DEFAULT_EMBEDDINGS_PATH = './glove_debias_new_sentiment_with_toxic_vec.txt'\n",
    "\n",
    "DEFAULT_MODEL_DIR = '../models'\n",
    "\n",
    "DEFAULT_HPARAMS = {\n",
    "    'max_sequence_length': 25,\n",
    "    'max_num_words': 2000000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00001,\n",
    "    'stop_early': False,\n",
    "    'es_patience': 1,  # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0,  # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.1,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5, 5, 5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "  try:\n",
    "    return metrics.roc_auc_score(y_true, y_pred)\n",
    "  except ValueError:\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "### Model scoring\n",
    "\n",
    "# Scoring these dataset for dozens of models actually takes non-trivial amounts\n",
    "# of time, so we save the results as a CSV. The resulting CSV includes all the\n",
    "# columns of the original dataset, and in addition has columns for each model,\n",
    "# containing the model's scores.\n",
    "def score_dataset(df, models, text_col):\n",
    "    \"\"\"Scores the dataset with each model and adds the scores as new columns.\"\"\"\n",
    "    for model in models:\n",
    "        name = model.get_model_name()\n",
    "        print('{} Scoring with {}...'.format(datetime.datetime.now(), name))\n",
    "        df[name] = model.predict(df[text_col])\n",
    "\n",
    "def load_maybe_score(models, orig_path, scored_path, postprocess_fn):\n",
    "    if os.path.exists(scored_path):\n",
    "        print('Using previously scored data:', scored_path)\n",
    "        return pd.read_csv(scored_path)\n",
    "\n",
    "    dataset = pd.read_csv(orig_path)\n",
    "    postprocess_fn(dataset)\n",
    "    score_dataset(dataset, models, 'text')\n",
    "    print('Saving scores to:', scored_path)\n",
    "    dataset.to_csv(scored_path)\n",
    "    return dataset\n",
    "\n",
    "def postprocess_madlibs(madlibs):\n",
    "    \"\"\"Modifies madlibs data to have standard 'text' and 'label' columns.\"\"\"\n",
    "    # Native madlibs data uses 'Label' column with values 'BAD' and 'NOT_BAD'.\n",
    "    # Replace with a bool.\n",
    "    madlibs['label'] = madlibs['Label'] == 'BAD'\n",
    "    madlibs.drop('Label', axis=1, inplace=True)\n",
    "    madlibs.rename(columns={'Text': 'text'}, inplace=True)\n",
    "\n",
    "def postprocess_wiki_dataset(wiki_data):\n",
    "    \"\"\"Modifies Wikipedia dataset to have 'text' and 'label' columns.\"\"\"\n",
    "    wiki_data.rename(columns={'is_toxic': 'label',\n",
    "                              'comment': 'text'},\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxModel(object):\n",
    "  \"\"\"Toxicity model.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               model_name=None,\n",
    "               model_dir=DEFAULT_MODEL_DIR,\n",
    "               embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "               hparams=None):\n",
    "    self.model_dir = model_dir\n",
    "    self.embeddings_path = embeddings_path\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.hparams = DEFAULT_HPARAMS.copy()\n",
    "    if hparams:\n",
    "      self.update_hparams(hparams)\n",
    "    if model_name:\n",
    "      self.load_model_from_name(model_name)\n",
    "    self.print_hparams()\n",
    "\n",
    "  def print_hparams(self):\n",
    "    print('Hyperparameters')\n",
    "    print('---------------')\n",
    "    for k, v in self.hparams.iteritems():\n",
    "      print('{}: {}'.format(k, v))\n",
    "    print('')\n",
    "\n",
    "  def update_hparams(self, new_hparams):\n",
    "    self.hparams.update(new_hparams)\n",
    "\n",
    "  def get_model_name(self):\n",
    "    return self.model_name\n",
    "\n",
    "  def save_hparams(self, model_name):\n",
    "    self.hparams['model_name'] = model_name\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'w') as f:\n",
    "      json.dump(self.hparams, f, sort_keys=True)\n",
    "\n",
    "  def load_model_from_name(self, model_name):\n",
    "    self.model = load_model(\n",
    "        os.path.join(self.model_dir, '%s_model.h5' % model_name))\n",
    "    self.tokenizer = cPickle.load(\n",
    "        open(\n",
    "            os.path.join(self.model_dir, '%s_tokenizer.pkl' % model_name),\n",
    "            'rb'))\n",
    "    with open(\n",
    "        os.path.join(self.model_dir, '%s_hparams.json' % self.model_name),\n",
    "        'r') as f:\n",
    "      self.hparams = json.load(f)\n",
    "\n",
    "  def fit_and_save_tokenizer(self, texts):\n",
    "    \"\"\"Fits tokenizer on texts and pickles the tokenizer state.\"\"\"\n",
    "    self.tokenizer = Tokenizer(num_words=self.hparams['max_num_words'])\n",
    "    gender = [\"she\",\"he\",\"her\",\"him\", \"woman\",\" man\",\" girl\",\" boy\",\" sister\",\" brother\",\" daughter\",\" son\",\" wife\",\" husband\",\" girlfriend\",\"boyfriend\",\" mother\",\" father\", \"aunt\",\" uncle\",\"mommy\",\"dad\"]\n",
    "    names = [\"Ebony\",\"Alonzo\",\"Amanda\",\"Adam\",\"Jasmine\",\"Alphonse\",\"Betsy\",\"Alan\",\"Lakisha\",\"Darnell\",\"Courtney\",\"Andrew\",\"Latisha\",\"Jamel\",\"Ellen\",\"Frank\",\"Latoya\",\"Jerome\",\"Heather\",\"Harry\",\"Nichelle\",\"Lamar\",\"Katie\",\"Jack\",\"Shaniqua\",\"Leroy\",\"Kristin\",\"Josh\",\"Shereen\",\"Malik\",\"Melanie\",\"Justin\",\"Tanisha\",\"Terrence\",\"Nancy\",\"Roger\",\"Tia\",\"Torrance\",\"Stephanie\",\"Ryan\"]\n",
    "    self.tokenizer.fit_on_texts(texts+gender+names)\n",
    "    cPickle.dump(self.tokenizer,\n",
    "                 open(\n",
    "                     os.path.join(self.model_dir,\n",
    "                                  '%s_tokenizer.pkl' % self.model_name), 'wb'))\n",
    "\n",
    "  def prep_text(self, texts):\n",
    "    \"\"\"Turns text into into padded sequences.\n",
    "\n",
    "    The tokenizer must be initialized before calling this method.\n",
    "\n",
    "    Args:\n",
    "      texts: Sequence of text strings.\n",
    "\n",
    "    Returns:\n",
    "      A tokenized and padded text sequence as a model input.\n",
    "    \"\"\"\n",
    "    print('\\n prepping: ', texts[1:3])\n",
    "    text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(\n",
    "        text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
    "\n",
    "  def load_embeddings(self):\n",
    "    \"\"\"Loads word embeddings.\"\"\"\n",
    "    embeddings_index = {}\n",
    "    with open(self.embeddings_path) as f:\n",
    "      for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    self.embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1,\n",
    "                                      self.hparams['embedding_dim']))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in self.tokenizer.word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        num_words_in_embedding += 1\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "  def train(self, train_data,train_labels,valid_data,valid_labels, model_name):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    self.model_name = model_name\n",
    "    self.save_hparams(model_name)\n",
    "\n",
    "    print('Fitting tokenizer...')\n",
    "    self.fit_and_save_tokenizer(train_data)\n",
    "    print('Tokenizer fitted!')\n",
    "\n",
    "    print('Preparing data...')\n",
    "    train_text, train_labels = (self.prep_text(train_data),train_labels)\n",
    "    valid_text, valid_labels = (self.prep_text(valid_data),valid_labels)\n",
    "    print('Data prepared!')\n",
    "\n",
    "    print('Loading embeddings...')\n",
    "    self.load_embeddings()\n",
    "    print('Embeddings loaded!')\n",
    "\n",
    "    print('Building model graph...')\n",
    "    self.build_model_1()\n",
    "    print('Training model...')\n",
    "\n",
    "    save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
    "    ]\n",
    "    if self.hparams['stop_early']:\n",
    "      callbacks.append(\n",
    "          EarlyStopping(\n",
    "              min_delta=self.hparams['es_min_delta'],\n",
    "              monitor='val_loss',\n",
    "              patience=self.hparams['es_patience'],\n",
    "              verbose=self.hparams['verbose'],\n",
    "              mode='auto'))\n",
    "    print(train_text[0],train_labels[0])\n",
    "    print(np.shape(train_text),np.shape(train_labels))\n",
    "    self.model.fit(\n",
    "        train_text,\n",
    "        train_labels,\n",
    "        batch_size=self.hparams['batch_size'],\n",
    "        epochs=self.hparams['epochs'],\n",
    "        validation_data=(valid_text, valid_labels),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0)\n",
    "    print('Model trained!')\n",
    "    print('Best model saved to {}'.format(save_path))\n",
    "    print('Loading best model from checkpoint...')\n",
    "    self.model = load_model(save_path)\n",
    "    print('Model loaded!')\n",
    "    \n",
    "  def build_model(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    for filter_size, kernel_size, pool_size in zip(\n",
    "        self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'],\n",
    "        self.hparams['cnn_pooling_sizes']):\n",
    "      x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(self.hparams['dropout_rate'])(x)\n",
    "    # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "    preds = Dense(1)(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop)\n",
    "\n",
    "  def build_conv_layer(self, input_tensor, filter_size, kernel_size, pool_size):\n",
    "    output = Conv1D(\n",
    "        filter_size, kernel_size, activation='relu', padding='same')(\n",
    "            input_tensor)\n",
    "    if pool_size:\n",
    "      output = MaxPooling1D(pool_size, padding='same')(output)\n",
    "    else:\n",
    "      # TODO(nthain): This seems broken. Fix.\n",
    "      output = GlobalMaxPooling1D()(output)\n",
    "    return output\n",
    "  \n",
    "  def build_model_1(self):\n",
    "    \"\"\"Builds model graph.\"\"\"\n",
    "    sequence_input = Input(\n",
    "        shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "    embedding_layer = Embedding(\n",
    "        len(self.tokenizer.word_index) + 1,\n",
    "        self.hparams['embedding_dim'],\n",
    "        weights=[self.embedding_matrix],\n",
    "        input_length=self.hparams['max_sequence_length'],\n",
    "        trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = embedded_sequences\n",
    "    x= keras.layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    preds = Dense(1,kernel_initializer='normal')(x)\n",
    "\n",
    "    rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "    self.model = Model(sequence_input, preds)\n",
    "    self.model.compile(\n",
    "        loss='mse', optimizer=rmsprop, metrics=['accuracy'])\n",
    "  def predict(self, texts):\n",
    "    \"\"\"Returns model predictions on texts.\"\"\"\n",
    "    data = self.prep_text(texts)\n",
    "    return self.model.predict(data)[:, 1]\n",
    "\n",
    "  def score_auc(self, texts, labels):\n",
    "    preds = self.predict(texts)\n",
    "    return compute_auc(labels, preds)\n",
    "\n",
    "  def summary(self):\n",
    "    return self.model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need a way to index the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_templates(templates):\n",
    "    template_index = {}\n",
    "    template_reverse_index = {}\n",
    "    for index, template in enumerate(templates):\n",
    "        template_reverse_index[template] = index\n",
    "        template_index[index] = template\n",
    "    return template_index, template_reverse_index\n",
    "\n",
    "def entries_by_template(template_id):\n",
    "    template = self.template_index[template_id]\n",
    "    output = df[df['Template'] == template]\n",
    "    return output\n",
    "\n",
    "def get_counterfactuals(entry_id, limit=None, not_similar_at=None, different_at=None):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        not_similar_at: similarities to filter by\n",
    "        different_at: difference to filter by\n",
    "    \"\"\"\n",
    "    template_id = self.template_reverse_index(self.df[entry_id][template])\n",
    "    candidates = data_by_template(template_id)\n",
    "    if different_at is not None:\n",
    "        for attr in different_at:\n",
    "            candidates = candidates[candidates[attr] != self.df[entry_id][attr]]\n",
    "    if not_similar_at is not None:\n",
    "        for attr in not_similar_at:\n",
    "            candidates = candidates[candidates[attr] == self.df[entry_id][attr]]\n",
    "    return candidates.sample(n=limit) if limit is not None else candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Sentence  Template  Person  Gender  Race  Emotion word\n",
       "Emotion                                                              \n",
       "anger    1400      1400      1400    1400    1400  1400          1400\n",
       "fear     1400      1400      1400    1400    1400  1400          1400\n",
       "joy      1400      1400      1400    1400    1400  1400          1400\n",
       "sadness  1400      1400      1400    1400    1400  1400          1400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna()\n",
    "a = df.groupby(\"Emotion\")\n",
    "a.aggregate('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: '<person subject> feels <emotion word>.',\n",
       "  1: 'The situation makes <person object> feel <emotion word>.',\n",
       "  2: 'I made <person object> feel <emotion word>.',\n",
       "  3: '<person subject> made me feel <emotion word>.',\n",
       "  4: '<person subject> found himself/herself in a/an <emotional situation word> situation.',\n",
       "  5: '<person subject> told us all about the recent <emotional situation word> events.',\n",
       "  6: 'The conversation with <person object> was <emotional situation word>.'},\n",
       " {'<person subject> feels <emotion word>.': 0,\n",
       "  '<person subject> found himself/herself in a/an <emotional situation word> situation.': 4,\n",
       "  '<person subject> made me feel <emotion word>.': 3,\n",
       "  '<person subject> told us all about the recent <emotional situation word> events.': 5,\n",
       "  'I made <person object> feel <emotion word>.': 2,\n",
       "  'The conversation with <person object> was <emotional situation word>.': 6,\n",
       "  'The situation makes <person object> feel <emotion word>.': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_templates = df.Template.unique()\n",
    "templates_index, templates_reverse_index = index_templates(unique_templates)\n",
    "templates_index, templates_reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_counterfactuals(limit=5, emotion_word='angry', max_iters=10):\n",
    "    found_candidates = False\n",
    "    wanted = None\n",
    "    iterations = 0\n",
    "    while not found_candidates and iterations < max_iters:\n",
    "        iterations += 1\n",
    "        \n",
    "        key = random.sample(templates_index.keys(), 1)[0]\n",
    "        template = templates_index[key]\n",
    "        candidates = df[df['Template'] == template]\n",
    "        wanted = candidates[candidates['Emotion word'] == emotion_word]\n",
    "        found_candidates = wanted.shape[0] > limit\n",
    "    assert wanted is not None and wanted.shape[0] > 0, ' could not find wanted '\n",
    "    return wanted.sample(n=limit)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I made Heather feel happy.',\n",
       " 'I made Melanie feel happy.',\n",
       " 'I made Jack feel happy.',\n",
       " 'I made Frank feel happy.',\n",
       " 'I made Leroy feel happy.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_random_counterfactuals(limit=5, emotion_word=\"happy\").Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counterfactual_sentences(emotion_word, limit):\n",
    "    sentences = get_random_counterfactuals(limit=limit, emotion_word=emotion_word).Sentence\n",
    "    return list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can get the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jerome made me feel happy.',\n",
       " 'Stephanie made me feel happy.',\n",
       " 'Latisha made me feel happy.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_counterfactual_sentences('happy', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTF_ToxModel(ToxModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.ctf_limit = kwargs['ctf_limit'] if 'ctf_limit' in kwargs else 5\n",
    "        self.ctf_reg = kwargs['ctf_reg'] if 'ctf_reg' in kwargs else 100\n",
    "        if 'ctf_limit' in kwargs:\n",
    "            del(kwargs['ctf_limit'])\n",
    "        if 'ctf_reg' in kwargs:\n",
    "            del(kwargs['ctf_reg'])\n",
    "        super(CTF_ToxModel, self).__init__(*args, **kwargs)\n",
    "    def custom_loss(self, ytrue, ypred):\n",
    "        mse_error = tf.losses.mean_squared_error(ytrue, ypred)\n",
    "        ctf_error = self.get_counterfactual_error()\n",
    "        return mse_error + ctf_error\n",
    "    \n",
    "    def get_counterfactual_error(self):\n",
    "        \"\"\"\n",
    "        For now, the error is the standard deviation of prediction probabilities\n",
    "        \"\"\"\n",
    "        sentences = self.get_random_counterfactuals()\n",
    "        preds = self._predict(sentences)\n",
    "        err = self.ctf_reg*(np.std(preds))\n",
    "        print('err: ', err)\n",
    "        return err\n",
    "    \n",
    "    def _predict(self, texts):\n",
    "        data = self.prep_text(texts)\n",
    "        output = self.model.predict(data)\n",
    "        #print('\\n done PREDICTING, out = ', output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_random_counterfactuals(self):\n",
    "        \"\"\"\n",
    "        Generate a vector of counterfactuals. Same template and emotion \n",
    "        word implies they should have the same rating\n",
    "        \"\"\"\n",
    "        emotions = ['angry', 'happy']\n",
    "        import random\n",
    "        emotion_word = random.sample(emotions, 1)[0]\n",
    "        sentences = get_counterfactual_sentences(emotion_word=emotion_word, limit=self.ctf_limit)\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def embeddings_for_sentences(self, sentences):\n",
    "        sentence_embeddings = [self.prep_text(sentence) for sentence in sentences]\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def ctf_build_model(self):\n",
    "        \"\"\"Builds model graph.\"\"\"\n",
    "        sequence_input = Input(\n",
    "            shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "            len(self.tokenizer.word_index) + 1,\n",
    "            self.hparams['embedding_dim'],\n",
    "            weights=[self.embedding_matrix],\n",
    "            input_length=self.hparams['max_sequence_length'],\n",
    "            trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        x= keras.layers.Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        preds = Dense(1,kernel_initializer='normal')(x)\n",
    "\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        # Now, compile with the loss becoming the custom loss\n",
    "        self.model.compile(\n",
    "            loss=self.custom_loss, optimizer=rmsprop, metrics=['accuracy'])    \n",
    "        \n",
    "    def ctf_train(self, train_data,train_labels,valid_data,valid_labels, model_name):\n",
    "        \"\"\"Trains the model.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data)\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data),train_labels)\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data),valid_labels)\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.ctf_build_model()\n",
    "        print('Training model...')\n",
    "\n",
    "        save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                save_path, save_best_only=True, verbose=self.hparams['verbose'])\n",
    "        ]\n",
    "        if self.hparams['stop_early']:\n",
    "          callbacks.append(\n",
    "              EarlyStopping(\n",
    "                  min_delta=self.hparams['es_min_delta'],\n",
    "                  monitor='val_loss',\n",
    "                  patience=self.hparams['es_patience'],\n",
    "                  verbose=self.hparams['verbose'],\n",
    "                  mode='auto'))\n",
    "        print(train_text[0],train_labels[0])\n",
    "        print(np.shape(train_text),np.shape(train_labels))\n",
    "        self.model.fit(\n",
    "            train_text,\n",
    "            train_labels,\n",
    "            batch_size=self.hparams['batch_size'],\n",
    "            epochs=self.hparams['epochs'],\n",
    "            validation_data=(valid_text, valid_labels),\n",
    "            callbacks=callbacks,\n",
    "            verbose=0)\n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(save_path))\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(save_path)\n",
    "        print('Model loaded!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = 500\n",
    "column_names=[\"LSTM GloVe Debiased\",\n",
    "              \"5-CTF LSTM GloVe Debiased-reg-100\",\"10-CTF LSTM GloVe Debiased-reg-100\",\n",
    "              \"5-CTF LSTM GloVe Debiased-reg-1\",\"10-CTF LSTM GloVe Debiased-reg-1\"]\n",
    "df_aa=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                         np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_w=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_f=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])\n",
    "df_m=pd.DataFrame(index=[np.zeros(lim),np.zeros(lim),np.zeros(lim),\n",
    "                        np.zeros(lim), np.zeros(lim)],columns=column_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LSTM GloVe Debiased</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LSTM GloVe Debiased 5-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "                0.0                 NaN                               NaN   \n",
       "\n",
       "                    10-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "                0.0                                NaN   \n",
       "\n",
       "                    5-CTF LSTM GloVe Debiased-reg-1  \\\n",
       "0.0 0.0 0.0 0.0 0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "                0.0                             NaN   \n",
       "\n",
       "                    10-CTF LSTM GloVe Debiased-reg-1  \n",
       "0.0 0.0 0.0 0.0 0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  \n",
       "                0.0                              NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM or CNN Models with Our Debiased Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "WARNING:tensorflow:From /Users/audace/Library/Python/2.7/lib/python/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/audace/Library/Python/2.7/lib/python/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "WARNING:tensorflow:From /Users/audace/Library/Python/2.7/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "pearson score  0.46359278077225635\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  201      Adam feels furious.\n",
      "202    Adam feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0012466876 aa higher 1.0371208e-05 0.006194651\n",
      "-0.0031379373 w higher 7.748604e-07 0.012150705\n",
      "\n",
      " prepping:  601      Nichelle feels furious.\n",
      "602    Nichelle feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0040752813 female higher 7.748604e-07 0.018310666\n",
      "-0.0029303392 male higher 2.2649765e-06 0.024096876\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt')\n",
    "model.train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"LSTM GloVe Debiased\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"LSTM GloVe Debiased\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM or CNN Models with Our Debiased Word Embeddings, CTF Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1.1: 5 Counterfactuals, same_template, different person, ctf_reg=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "WARNING:tensorflow:From /Users/audace/Library/Python/2.7/lib/python/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " prepping:  ['The situation makes Stephanie feel happy.', 'The situation makes Ryan feel happy.']\n",
      "err:  0.12061939341947436\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "\n",
      " prepping:  ['I made Amanda feel angry.', 'I made Katie feel angry.']\n",
      "err:  0.20539348479360342\n",
      "Model loaded!\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "pearson score  0.5296812907890375\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  201      Adam feels furious.\n",
      "202    Adam feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0031395599 aa higher 4.887581e-06 0.0090813935\n",
      "-0.0030126146 w higher 1.886487e-05 0.015019178\n",
      "\n",
      " prepping:  601      Nichelle feels furious.\n",
      "602    Nichelle feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0071061593 female higher 3.874302e-06 0.03833124\n",
      "-0.0028047687 male higher 3.33786e-06 0.010195583\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=5, ctf_reg=100)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"5-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic 1.2:  5 Counterfactuals, same_template, different person, ctf_reg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "\n",
      " prepping:  ['Terrence made me feel angry.', 'Jack made me feel angry.']\n",
      "err:  0.00034146005054935813\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "\n",
      " prepping:  ['Tia made me feel happy.', 'Katie made me feel happy.']\n",
      "err:  0.0012489448999986053\n",
      "Model loaded!\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "pearson score  0.5362161126665242\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  201      Adam feels furious.\n",
      "202    Adam feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.003495994 aa higher 1.013279e-06 0.015787274\n",
      "-0.0031600574 w higher 1.7881393e-07 0.01232931\n",
      "\n",
      " prepping:  601      Nichelle feels furious.\n",
      "602    Nichelle feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0061485497 female higher 3.874302e-07 0.04695019\n",
      "-0.003337944 male higher 1.3113022e-06 0.019538999\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=5, ctf_reg=1)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"5-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic 2.1: 10 counterfactuals, same template, different person, ctf_reg=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "\n",
      " prepping:  ['The situation makes Courtney feel happy.', 'The situation makes Stephanie feel happy.']\n",
      "err:  0.08539207046851516\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "\n",
      " prepping:  ['The situation makes Lamar feel happy.', 'The situation makes Tia feel happy.']\n",
      "err:  0.3977624233812094\n",
      "Model loaded!\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "pearson score  0.48065707832764953\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  201      Adam feels furious.\n",
      "202    Adam feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0023007132 aa higher 1.7285347e-06 0.009335697\n",
      "-0.0035612267 w higher 6.020069e-06 0.012841493\n",
      "\n",
      " prepping:  601      Nichelle feels furious.\n",
      "602    Nichelle feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0049777227 female higher 8.225441e-06 0.0230411\n",
      "-0.0028969827 male higher 4.5597553e-06 0.015355289\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=10, ctf_reg=100)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"10-CTF LSTM GloVe Debiased-reg-100\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic 2.2: 10 counterfactuals, same template, different person, ctf_reg=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 2000000\n",
      "dropout_rate: 0.1\n",
      "verbose: False\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 1e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 200\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 25\n",
      "stop_early: False\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "\n",
      " prepping:  [\"At least I don't have a guy trying to discourage me anymore in what I want to do he will never become anything worth contributing to society\", \"UPLIFT: If you're still discouraged it means you're listening to the wrong voices &amp; looking to the wrong source. Look to the LORD!\"]\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "\n",
      " prepping:  ['Josh feels happy.', 'Betsy feels happy.']\n",
      "err:  0.0007455546292476356\n",
      "Training model...\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 1682  566  415  136] 0.6\n",
      "(1181, 25) (1181,)\n",
      "Model trained!\n",
      "Best model saved to ../models/test_model.h5\n",
      "Loading best model from checkpoint...\n",
      "\n",
      " prepping:  ['I made Lakisha feel angry.', 'I made Darnell feel angry.']\n",
      "err:  0.0018374762730672956\n",
      "Model loaded!\n",
      "\n",
      " prepping:  ['@realDonaldTrump But you have a lot of time for tweeting #ironic', \"I graduated yesterday and already had 8 family members asking what job I've got now \\xf0\\x9f\\x98\\x82 #nightmare\"]\n",
      "pearson score  0.5020670547642704\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  201      Adam feels furious.\n",
      "202    Adam feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0023646562 aa higher 1.9073486e-06 0.010163307\n",
      "-0.0027924525 w higher 4.917383e-06 0.01361841\n",
      "\n",
      " prepping:  601      Nichelle feels furious.\n",
      "602    Nichelle feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "\n",
      " prepping:  1      Alonzo feels furious.\n",
      "2    Alonzo feels irritated.\n",
      "Name: Sentence, dtype: object\n",
      "0.0054715327 female higher 3.2782555e-07 0.03190884\n",
      "-0.003402368 male higher 1.5884638e-05 0.012402356\n"
     ]
    }
   ],
   "source": [
    "X_reg,y_reg= parse_reg(\"../data/Senitment_Analysis/SemEval2018-Task1-AIT-Test-gold/V-reg/2018-Valence-reg-En-test-gold.txt\")\n",
    "X,y = parse_reg(\"../data/Senitment_Analysis/2018-Valence-reg-En-train.txt\")\n",
    "from keras.layers import LSTM\n",
    "model = CTF_ToxModel(embeddings_path='../data/embeddings/glove.6B/glove_debias_new_sentiment_with_toxic_vec.txt', \n",
    "                     ctf_limit=10, ctf_reg=1)\n",
    "get_custom_objects().update({\"custom_loss\": model.custom_loss})\n",
    "\n",
    "model.ctf_train(X,np.array(zip(*y)[1]),X_reg,np.array(zip(*y_reg)[1]), 'test')\n",
    "data = model.prep_text(X_reg)\n",
    "print(\"pearson score \",pearson_coeff(model.model.predict(data)[:,0],np.array(zip(*y_reg)[1])))\n",
    "a= pd.read_csv(\"../data/Senitment_Analysis/Equity-Evaluation-Corpus.csv\").dropna(subset=[\"Emotion\"])\n",
    "aa=model.prep_text(a[a.Race=='African-American'][\"Sentence\"])\n",
    "w=model.prep_text(a[a.Race=='European'][\"Sentence\"])\n",
    "aa= model.model.predict(aa)[:,0]\n",
    "w= model.model.predict(w)[:,0]\n",
    "n=np.where((aa-w)>0)\n",
    "print(np.mean(aa[n]-w[n]),\"aa higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_aa[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "n=np.where((w-aa)>0)\n",
    "print(np.mean(aa[n]-w[n]), \"w higher\",np.min(np.abs(aa[n]-w[n])),np.max(np.abs(aa[n]-w[n])))\n",
    "df_w[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (aa[n]-w[n])[:lim]\n",
    "f=model.prep_text(a[a.Gender=='female'][\"Sentence\"])\n",
    "m=model.prep_text(a[a.Gender=='male'][\"Sentence\"])\n",
    "f= model.model.predict(f)[:,0]\n",
    "m= model.model.predict(m)[:,0]\n",
    "n=np.where((f-m)>0)\n",
    "print(np.mean(f[n]-m[n]),\"female higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))\n",
    "df_f[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "n=np.where((m-f)>0)\n",
    "df_m[\"10-CTF LSTM GloVe Debiased-reg-1\"] = (f[n]-m[n])[:lim]\n",
    "print(np.mean(f[n]-m[n]), \"male higher\",np.min(np.abs(f[n]-m[n])),np.max(np.abs(f[n]-m[n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for various bias groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LSTM GloVe Debiased</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.003285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.003887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.002162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.002162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LSTM GloVe Debiased  5-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0             0.000544                           0.000969   \n",
       "                0.0             0.000022                           0.000860   \n",
       "                0.0             0.000517                           0.000484   \n",
       "                0.0             0.000517                           0.001766   \n",
       "                0.0             0.000485                           0.000110   \n",
       "\n",
       "                     10-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "\n",
       "                     5-CTF LSTM GloVe Debiased-reg-1  \\\n",
       "0.0 0.0 0.0 0.0 0.0                         0.000134   \n",
       "                0.0                         0.000057   \n",
       "                0.0                         0.000825   \n",
       "                0.0                         0.000717   \n",
       "                0.0                         0.000759   \n",
       "\n",
       "                     10-CTF LSTM GloVe Debiased-reg-1  \n",
       "0.0 0.0 0.0 0.0 0.0                          0.003285  \n",
       "                0.0                          0.003887  \n",
       "                0.0                          0.002162  \n",
       "                0.0                          0.002162  \n",
       "                0.0                          0.002832  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palette for debiasing\n",
    "global_palette = {column_names[0]:'green',column_names[1]:\"orange\", column_names[2]:\"blue\", \n",
    "                  column_names[3]:'red', column_names[4]:'yellow'}\n",
    "xticklabels = column_names[:]\n",
    "xticklabels\n",
    "df_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LSTM GloVe Debiased</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>10-CTF LSTM GloVe Debiased-reg-100</th>\n",
       "      <th>5-CTF LSTM GloVe Debiased-reg-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LSTM GloVe Debiased  5-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0             0.000544                           0.000969   \n",
       "                0.0             0.000022                           0.000860   \n",
       "                0.0             0.000517                           0.000484   \n",
       "                0.0             0.000517                           0.001766   \n",
       "                0.0             0.000485                           0.000110   \n",
       "\n",
       "                     10-CTF LSTM GloVe Debiased-reg-100  \\\n",
       "0.0 0.0 0.0 0.0 0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "                0.0                            0.000139   \n",
       "\n",
       "                     5-CTF LSTM GloVe Debiased-reg-1  \n",
       "0.0 0.0 0.0 0.0 0.0                         0.000134  \n",
       "                0.0                         0.000057  \n",
       "                0.0                         0.000825  \n",
       "                0.0                         0.000717  \n",
       "                0.0                         0.000759  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_aa = df_aa[column_names[:-1]]\n",
    "_df_aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM GloVe Debiased  mean  0.0012949717 range 1.8566847e-05 0.004472196\n",
      "5-CTF LSTM GloVe Debiased-reg-100  mean  0.0033883885 range 7.4207783e-06 0.008751303\n",
      "10-CTF LSTM GloVe Debiased-reg-100  mean  0.0025907913 range 6.2584877e-06 0.009335697\n",
      "5-CTF LSTM GloVe Debiased-reg-1  mean  0.0042782933 range 1.5735626e-05 0.015787274\n",
      "10-CTF LSTM GloVe Debiased-reg-1  mean  0.002733888 range 3.427267e-05 0.009854168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = seaborn.stripplot(data=_df_aa,palette=global_palette)\n",
    "#plot.get_figure().savefig('../images/df_aa.png', format='png',dpi=300,bbox_inches='tight')\n",
    "plot.set_xticklabels(labels=xticklabels,rotation=15)\n",
    "plot.set(xlabel='Models', ylabel='Per Sentence Prediction Delta')\n",
    "plot.set_title(\"AA$\\uparrow$- E$\\downarrow$ Per Sentence Deltas\",{'fontsize':14})\n",
    "for i in xticklabels:\n",
    "    print(i, \" mean \", np.mean(df_aa[i]),\"range\", np.min(np.abs(df_aa[i])),np.max(np.abs(df_aa[i])))\n",
    "#plot.get_figure().savefig('../images/df_aa.png', format='png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
